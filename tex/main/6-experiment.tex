\section{Experiments}
\label{sec:experiments}

This section provides controlled simulations that validate two core design claims of REINFORCE Pro Max: (i)~the leave-one-out baseline and adaptive asymmetric normalization stabilize advantage statistics without flipping advantage sign; and (ii)~prefix-level masking behaves as predicted under the deviation patterns analyzed in \Cref{thm:prefix-tighter}. Large-scale LLM benchmark results (e.g., AIME/MATH with a specific backbone) are orthogonal to these structural properties and are left as future work.

\subsection{Experimental Setup}

We simulate prompt groups with $n=8$ samples. Rewards are drawn from a skewed mixture that mimics sparse-success reasoning settings: with probability $p=0.15$ a sample is drawn from a ``success'' cluster centered near $1$, otherwise from a ``failure'' cluster centered near $0$, both with Gaussian noise $\sigma_{\mathrm{rew}}=0.10$. To reflect token-level weighting effects after expansion (\Cref{sec:token-expand}), we assign longer sequences to lower-quality samples (failure trajectories), setting lengths to
\[
    T_i = \begin{cases}
        256 + u_i, & \text{success sample},\\
        512 + u_i, & \text{failure sample},
    \end{cases}
    \qquad u_i \sim \mathrm{Unif}\{-32,\ldots,32\}.
\]

For IS masking, we use the acceptance interval $[1-\epslow,1+\epshigh]=[0.9,1.1]$ unless otherwise stated (i.e., $\epslow=\epshigh=0.1$).
We also define the corresponding log-thresholds
\[
    \tau_+ := \log(1+\epshigh),
    \qquad
    \tau_- := -\log(1-\epslow),
\]
so that $\rho_t \in [1-\epslow,1+\epshigh]$ is equivalent to $\log\rho_t \in [-\tau_-,\tau_+]$.
Finally, we construct synthetic log-ratio patterns with $T=32$ to directly instantiate the three regimes in \Cref{thm:prefix-tighter}.

\subsection{Baselines}

\textbf{Baselines and normalization.} We compare the mean baseline (REINFORCE++/GRPO-style) to the leave-one-out (RLOO) baseline (\Cref{def:rloo}), and we compare global normalization $(A-\mu)/\sigma$ to adaptive asymmetric normalization (\Cref{def:adaptive-norm}).

\textbf{IS masking.} We compare token-level masking (IcePop), sequence-level masking (GSPO-style, based on $\rhoseq$), and prefix-level
masking based on the prefix proxy in \Cref{def:prefix-is}.

\subsection{Metrics}

\textbf{Baseline self-correlation.} We measure $\mathrm{corr}(r_i, b_i)$ between each sample reward $r_i$ and its baseline $b_i$. A
correlated baseline can couple a sample with its own control variate, complicating the clean second-moment analysis enabled by RLOO (see
\Cref{app:rloo-proof}).

\textbf{Sign flip rate under normalization.} After token expansion, we compute the token-weighted fraction of non-zero tokens whose normalized advantage changes sign, i.e., $\mathbb{I}[\mathrm{sign}(\hat{A}_{i,t}) \neq \mathrm{sign}(A_{i,t})]$. Sign flips are undesirable because they can turn punishment into reinforcement (or vice versa).

\textbf{Masked-token coverage.} For each synthetic log-ratio pattern, we report the number of masked tokens and the first/last masked
positions for IcePop, sequence-level IS masking, and prefix masking.

\subsection{Main Results}

\begin{table}[ht]
\centering
\caption{Controlled simulation of baseline correlation and sign flips under token weighting. Numbers are estimated over $20{,}000$ prompt groups with $n=8$.}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Mean / global} & \textbf{RLOO / adaptive} \\
\midrule
$\mathrm{corr}(r_i, b_i)$ & $0.350$ & $-0.008$ \\
Sign flip rate (token-weighted) & $6.6\%$ & $0.0\%$ \\
\bottomrule
\end{tabular}
\end{table}

\noindent The mean baseline exhibits substantial self-correlation, while RLOO makes the baseline effectively independent of the current sample. Under token-length weighting, global normalization introduces non-negligible sign flips, whereas adaptive normalization preserves sign by construction (\Cref{par:adv-sign-preservation}).

\begin{table}[ht]
\centering
\caption{Masking behavior under synthetic deviation patterns. We use $T=32$ and $[1-\epslow,1+\epshigh]=[0.9,1.1]$. We report the number of masked tokens and the masked position range.}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lccc}
\toprule
\textbf{Pattern} & \textbf{IcePop (token)} & \textbf{Seq (GSPO-style)} & \textbf{Prefix (ours)} \\
\midrule
Early ($\log\rho_1{=}8\tau_+$; else $0$) & $1$ ($[1,1]$) & $0$ (---) & $7$ ($[1,7]$) \\
Late ($\log\rho_{24}{=}32\tau_+$; else $0$) & $1$ ($[24,24]$) & $0$ (---) & $8$ ($[24,31]$) \\
Drift ($\log\rho_t{=}2\tau_+,\ \forall t$) & $32$ ($[1,32]$) & $32$ ($[1,32]$) & $32$ ($[1,32]$) \\
\bottomrule
\end{tabular}
\end{table}

\noindent The early and late patterns instantiate \Cref{thm:prefix-tighter}(a,b) in the simplest form with $\varepsilon=0$ (all non-spike
tokens satisfy $\rho_t=1$ and hence lie within the trust-region interval).
\par
\noindent\textbf{Early deviation.} IcePop reacts only to the local outlier at $t=1$ and therefore masks a single token. Sequence-level
masking aggregates over the full horizon and dilutes the violation. Prefix masking, in contrast, propagates the early mismatch forward by
masking a short prefix:
\[
    \log\rhoprefix{t} = \frac{1}{t}\log\rho_1 = \frac{8}{t}\tau_+,
    \qquad
    \log\rhoseq = \frac{1}{T}\log\rho_1 = \frac{1}{4}\tau_+,
    \qquad (T=32).
\]
Thus $\log\rhoprefix{t}>\tau_+$ for $t\in\{1,\ldots,7\}$, while $\log\rhoseq$ remains in-bound.
\par
\noindent\textbf{Late deviation.} For a single spike at $t^*=24$, prefix masking is strictly suffix-local (it can only start at $t\ge t^*$),
while sequence-level masking remains all-or-nothing. With $\log\rho_{24}=32\tau_+$ and $\log\rho_t=0$ otherwise, the sequence statistic
satisfies $\log\rhoseq=\tau_+$ and is accepted, but the prefix average exceeds $\tau_+$ for $t\in\{24,\ldots,31\}$, yielding a localized
masked suffix. These behaviors align with the regimes in \Cref{thm:prefix-tighter}.

\subsection{Ablation Studies}

\textbf{Without adaptive normalization.} Global normalization can flip the sign of token-expanded advantages when token-length weighting makes the token-level mean non-zero, while adaptive normalization preserves sign by using separate positive/negative scalings (\Cref{par:adv-sign-preservation}).

\textbf{Without prefix masking.} Token-level masking only reacts to per-token outliers and may leave future tokens unfiltered even when the context has already drifted; prefix masking filters based on the prefix statistic, which is aligned with the causal accumulation structure (\Cref{lem:coupling}, \Cref{thm:prefix-tighter}).

\subsection{Sensitivity to Thresholds}

Masking behavior depends on $[1-\epslow,1+\epshigh]$. Stricter thresholds can cause sequence-level IS masking to reject an entire trajectory,
while prefix masking remains localized in position. For example, in the late-deviation pattern above, the sequence statistic satisfies
$\rhoseq=1+\epshigh=1.1$ under the default thresholds. If we instead use the stricter symmetric interval $[1-\epslow,1+\epshigh]=[0.95,1.05]$
(i.e., $\epslow=\epshigh=0.05$), then $\rhoseq>1.05$ and the sequence-level mask rejects all $32$ tokens, while prefix masking still rejects only
suffix positions starting at $t=24$.

\subsection{Reproducibility Checklist}

\begin{itemize}
    \item Random seed: $0$.
    \item Monte Carlo trials: $20{,}000$ prompt groups; $n=8$ samples per group.
    \item Reward distribution: mixture with $p=0.15$ and Gaussian noise $\sigma_{\mathrm{rew}}=0.10$.
    \item Token lengths: success $T_i = 256 + u_i$, failure $T_i = 512 + u_i$, $u_i \sim \mathrm{Unif}\{-32,\ldots,32\}$.
    \item Thresholds: default $(\epslow,\epshigh)=(0.1,0.1)$ (i.e., $[1-\epslow,1+\epshigh]=[0.9,1.1]$); sensitivity example $(\epslow,\epshigh)=(0.05,0.05)$ (i.e., $[0.95,1.05]$).
    \item Code version: git commit \texttt{9a3879e}.
\end{itemize}
