\section{Experiments}
\label{sec:experiments}

This section provides controlled simulations that validate two core design claims of REINFORCE Pro Max: (i)~the leave-one-out baseline and adaptive asymmetric normalization stabilize advantage statistics without flipping advantage sign; and (ii)~prefix-level masking behaves as predicted under the deviation patterns analyzed in \Cref{thm:prefix-tighter}. Large-scale LLM benchmark results (e.g., AIME/MATH with a specific backbone) are orthogonal to these structural properties and are left as future work.

\subsection{Setup}

We simulate prompt groups with $n=8$ samples. Rewards are drawn from a skewed mixture that mimics sparse-success reasoning settings: with probability $p=0.15$ a sample is drawn from a ``success'' cluster centered near $1$, otherwise from a ``failure'' cluster centered near $0$, both with Gaussian noise $\sigma_{\mathrm{rew}}=0.10$. To reflect token-level weighting effects after expansion (\Cref{sec:token-expand}), we assign longer sequences to lower-quality samples (failure trajectories), setting lengths to
\[
    T_i = \begin{cases}
        256 + u_i, & \text{success sample},\\
        512 + u_i, & \text{failure sample},
    \end{cases}
    \qquad u_i \sim \mathrm{Unif}\{-32,\ldots,32\}.
\]

For IS masking, we use thresholds $[\lambda,\Lambda]=[0.5,5.0]$ unless otherwise stated, and synthetic log-ratio patterns with $T=32$ to directly instantiate the three regimes in \Cref{thm:prefix-tighter}.

\subsection{Baselines}

\textbf{Baselines and normalization.} We compare the mean baseline (REINFORCE++/GRPO-style) to the leave-one-out (RLOO) baseline (\Cref{def:rloo}), and we compare global normalization $(A-\mu)/\sigma$ to adaptive asymmetric normalization (\Cref{def:adaptive-norm}).

\textbf{IS masking.} We compare token-level masking (ICEPOP), sequence-level masking (GSPO-style, based on $\rhoseq$), and prefix-level
masking based on the prefix proxy in \Cref{def:prefix-is}.

\subsection{Metrics}

\textbf{Baseline self-correlation.} We measure $\mathrm{corr}(r_i, b_i)$ between each sample reward $r_i$ and its baseline $b_i$. A
correlated baseline can couple a sample with its own control variate, complicating the clean second-moment analysis enabled by RLOO (see
\Cref{app:rloo-proof}).

\textbf{Sign flip rate under normalization.} After token expansion, we compute the token-weighted fraction of non-zero tokens whose normalized advantage changes sign, i.e., $\mathbb{I}[\mathrm{sign}(\hat{A}_{i,t}) \neq \mathrm{sign}(A_{i,t})]$. Sign flips are undesirable because they can turn punishment into reinforcement (or vice versa).

\textbf{Masked-token coverage.} For each synthetic log-ratio pattern, we report the number of masked tokens and the first/last masked
positions for ICEPOP, sequence-level IS masking, and prefix masking.

\subsection{Main Results}

\begin{table}[ht]
\centering
\caption{Controlled simulation of baseline correlation and sign flips under token weighting. Numbers are estimated over $20{,}000$ prompt groups with $n=8$.}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Mean / global} & \textbf{RLOO / adaptive} \\
\midrule
$\mathrm{corr}(r_i, b_i)$ & $0.350$ & $-0.008$ \\
Sign flip rate (token-weighted) & $6.6\%$ & $0.0\%$ \\
\bottomrule
\end{tabular}
\end{table}

\noindent The mean baseline exhibits substantial self-correlation, while RLOO makes the baseline effectively independent of the current sample. Under token-length weighting, global normalization introduces non-negligible sign flips, whereas adaptive normalization preserves sign by construction (\Cref{par:adv-sign-preservation}).

\begin{table}[ht]
\centering
\caption{Masking behavior under synthetic deviation patterns ($T=32$, $[\lambda,\Lambda]=[0.5,5.0]$). We report the number of masked tokens and the masked position range.}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lccc}
\toprule
\textbf{Pattern} & \textbf{ICEPOP (token)} & \textbf{Seq (GSPO-style)} & \textbf{Prefix (ours)} \\
\midrule
Early ($\log\rho_1{=}10$; else $0$) & $1$ ($[1,1]$) & $0$ (---) & $6$ ($[1,6]$) \\
Late ($\log\rho_{24}{=}50$; else $0$) & $1$ ($[24,24]$) & $0$ (---) & $8$ ($[24,31]$) \\
Drift ($\log\rho_t{=}2,\ \forall t$) & $32$ ($[1,32]$) & $32$ ($[1,32]$) & $32$ ($[1,32]$) \\
\bottomrule
\end{tabular}
\end{table}

\noindent In the analyzed deviation patterns (where the post-deviation log-ratios remain in-bound), prefix masking rejects a contiguous
block starting at the first threshold violation of the prefix statistic, preserving positional information that is lost by sequence-level
masking, and masking more prefix-drifted tokens than token-level masking in the early-deviation regime (as predicted by
\Cref{thm:prefix-tighter}).

\subsection{Ablations}

\textbf{Without adaptive normalization.} Global normalization can flip the sign of token-expanded advantages when token-length weighting makes the token-level mean non-zero, while adaptive normalization preserves sign by using separate positive/negative scalings (\Cref{par:adv-sign-preservation}).

\textbf{Without prefix masking.} Token-level masking only reacts to per-token outliers and may leave future tokens unfiltered even when the context has already drifted; prefix masking filters based on the prefix statistic, which is aligned with the causal accumulation structure (\Cref{lem:coupling}, \Cref{thm:prefix-tighter}).

\subsection{Sensitivity to Thresholds}

Masking behavior depends on $[\lambda,\Lambda]$. Stricter thresholds can cause sequence-level IS masking to reject an entire trajectory,
while prefix masking remains localized in position. For example, in the late-deviation pattern above, using $[\lambda,\Lambda]=[0.8,2.0]$
causes the sequence-level mask to reject all $32$ tokens, while prefix masking rejects only the suffix positions $t \in [24,32]$.

\subsection{Reproducibility Checklist}

\begin{itemize}
    \item Random seed: $0$.
    \item Monte Carlo trials: $20{,}000$ prompt groups; $n=8$ samples per group.
    \item Reward distribution: mixture with $p=0.15$ and Gaussian noise $\sigma_{\mathrm{rew}}=0.10$.
    \item Token lengths: success $T_i = 256 + u_i$, failure $T_i = 512 + u_i$, $u_i \sim \mathrm{Unif}\{-32,\ldots,32\}$.
    \item Thresholds: default $[\lambda,\Lambda]=[0.5,5.0]$; sensitivity example $[0.8,2.0]$.
    \item Code version: git commit \texttt{9a3879e}.
\end{itemize}
