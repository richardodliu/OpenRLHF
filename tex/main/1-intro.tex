\section{Introduction}
\label{sec:introduction}

Reinforcement Learning (RL) has become a widely adopted paradigm for aligning Large Language Models (LLMs) with human preferences and improving their reasoning capabilities~\citep{zeng2025simplerl, guo2025deepseek}. As tasks shift from short-form chat to long-horizon reasoning---mathematical problem solving, code generation, and multi-step planning---the computational cost of maintaining a critic (value) network becomes prohibitive. Critic-free methods such as REINFORCE++ Baseline~\citep{hu2025reinforce}, GRPO~\citep{shao2024deepseekmath}, and RLOO~\citep{ahmadian2024back} have emerged as practical alternatives, eliminating the critic and saving approximately 25\% of GPU memory in comparable settings~\citep{hu2025reinforce}.

Despite their efficiency, critic-free methods face two fundamental challenges that become increasingly severe as sequence lengths grow:

\textbf{Challenge 1: High-variance advantage estimation.} Without a learned value function, the advantage must be estimated from reward signals alone. The standard mean baseline $b = \frac{1}{n}\sum_j r_j$ introduces correlation between each sample's baseline and its own reward, which can inflate gradient variance. Global normalization $\hat{A} = (A - \mu)/\sigma$ applies a uniform scaling that distorts the relative magnitude of positive and negative advantages, particularly problematic in reasoning tasks where reward distributions are highly skewed (most samples fail, few succeed).

\textbf{Challenge 2: Off-policy mismatch.} Modern LLM-RL pipelines use separate systems for inference and training---high-throughput engines like vLLM~\citep{kwon2023efficient} for rollout and frameworks like PyTorch FSDP~\citep{zhao2023pytorch} for gradient updates. Backend discrepancies (different attention kernels, precision formats), MoE routing discontinuities~\citep{liu2024deepseek}, and distributed training staleness cause the rollout policy $\piroll$ to differ from the training policy $\piold$, introducing off-policy errors that compound autoregressively over long sequences~\citep{liu2025rlcollapse, yao2025offpolicy}.

These challenges are not independent: noisy advantage estimates amplify the effect of off-policy errors, and biased importance sampling weights distort the advantage signal, creating a compounding instability. Formally, the surrogate objective error decomposes into per-position terms involving both an advantage factor and a context distribution shift factor (see \Cref{eq:error-decomp}); high variance inflates the former while off-policy mismatch inflates the latter. Existing methods address each challenge in isolation---RLOO improves the baseline but ignores off-policy correction; TIS and ICEPOP correct for policy mismatch but ignore causal structure and use suboptimal advantage estimation.

We present \textbf{REINFORCE Pro Max}, a unified framework that addresses both challenges simultaneously. By combining variance-reduced advantage estimation (\Cref{sec:reinforce-max}) with causal off-policy correction (\Cref{sec:reinforce-pro}), both factors in the error decomposition are targeted---the advantage magnitude via normalization and the rollout/training mismatch via sample-level prefix filtering; the joint benefit is discussed in \Cref{sec:unified}. Our contributions are:

\begin{enumerate}
    \item \textbf{Leave-one-out baseline:} We adopt the leave-one-out (RLOO) baseline for its statistical independence between each sample and its baseline, avoiding correlated baseline artifacts. See \Cref{app:rloo-proof} for a detailed analysis.

    \item \textbf{Adaptive asymmetric normalization} (\Cref{sec:adaptive-norm}): We introduce a normalization scheme that applies
    different scaling factors $\alpha, \beta$ to positive and negative advantages. The scheme provably preserves gradient direction while
    enforcing zero empirical mean and unit empirical variance on non-zero tokens (the constraints in \Cref{def:adaptive-norm}).

    \item \textbf{Prefix cumulative IS with causal structure (\Cref{sec:prefix-is}):} We propose a prefix cumulative importance sampling correction that respects the autoregressive causal structure. We show that, under the explicit sufficient conditions stated in Theorem~\ref{thm:prefix-tighter}, prefix cumulative IS can provide tighter masking (i.e., mask at least as many off-policy tokens) than token-level methods (which ignore causal dependencies) and sequence-level methods (which lose positional information) in three analyzed deviation patterns (early deviation, late deviation, and monotone drift).

    \item \textbf{Connection to the Adaptive bound (\Cref{sec:adaptive-connection}):} We relate prefix cumulative IS to the per-position structure of the Adaptive bound from the trust region framework~\citep{li2025trm}: the KL chain rule yields an expectation identity for prefix log-ratios, and thresholding the sample-level prefix log-ratio provides a practical proxy for trust-region style filtering at each position.
\end{enumerate}

\textbf{Paper organization.} \Cref{sec:preliminaries} introduces notation, assumptions, and the surrogate objective error decomposition. \Cref{sec:reinforce-max} presents REINFORCE Max (leave-one-out baseline, token expansion, and adaptive normalization). \Cref{sec:reinforce-pro} develops REINFORCE Pro (prefix cumulative IS correction and its connection to the Adaptive bound). \Cref{sec:unified} unifies both components and presents the full algorithm. \Cref{sec:related} discusses related work, \Cref{sec:experiments} describes experimental evaluation, and \Cref{sec:conclusion} concludes.
