\section{Introduction}

Reinforcement Learning (RL) has become a widely adopted paradigm for aligning Large Language Models (LLMs) with human preferences and improving their reasoning capabilities~\citep{zeng2025simplerl, guo2025deepseek}. As tasks shift from short-form chat to long-horizon reasoning---mathematical problem solving, code generation, and multi-step planning---the computational cost of maintaining a critic (value) network becomes prohibitive. Critic-free methods such as REINFORCE++ Baseline~\citep{hu2025reinforce}, GRPO~\citep{shao2024deepseekmath}, and RLOO~\citep{ahmadian2024back} have emerged as practical alternatives, eliminating the critic and saving approximately 25\% of GPU memory in comparable settings~\citep{hu2025reinforce}.

Despite their efficiency, critic-free methods face two fundamental challenges that become increasingly severe as sequence lengths grow:

\textbf{Challenge 1: High-variance advantage estimation.} Without a learned value function, the advantage must be estimated from reward signals alone. The standard mean baseline $b = \frac{1}{n}\sum_j r_j$ introduces correlation between each sample's baseline and its own reward, which can inflate gradient variance. Global normalization $\hat{A} = (A - \mu)/\sigma$ applies a uniform scaling that distorts the relative magnitude of positive and negative advantages, particularly problematic in reasoning tasks where reward distributions are highly skewed (most samples fail, few succeed).

\textbf{Challenge 2: Causal off-policy mismatch.} PPO-style LLM-RL optimizes a policy $\pitheta$ using trajectories generated by a fixed
rollout policy $\piroll$ (often implemented as a frozen ``old'' snapshot). Distributed staleness, numerical effects, and architectural
discontinuities (e.g., MoE routing flips~\citep{liu2024deepseek}) can amplify the divergence between $\piroll$ and $\pitheta$, producing
large per-token importance ratios $\rho_t=\pitheta(y_t\mid c_t)/\piroll(y_t\mid c_t)$. Because generation is autoregressive, an early ratio
deviation shifts the
future context distribution, so token-independent ratio interventions may fail to respect the causal structure of error accumulation.

These challenges are not independent: noisy advantage estimates amplify the effect of off-policy errors, and biased importance sampling weights distort the advantage signal, creating a compounding instability. Formally, the surrogate objective error decomposes into per-position terms involving both an advantage factor and a context distribution shift factor (see \Cref{eq:error-decomp}); high variance inflates the former while off-policy mismatch inflates the latter. Existing methods address each challenge in isolation---RLOO improves the baseline but ignores off-policy correction; TIS and ICEPOP correct for policy mismatch but ignore causal structure and use suboptimal advantage estimation.

We present \textbf{REINFORCE Pro Max}, a unified framework that addresses both challenges simultaneously. By combining variance-reduced
advantage estimation (\Cref{sec:reinforce-max}) with prefix-level Causal Trust Region masking (\Cref{sec:reinforce-pro}), both factors in the error
decomposition are targeted---the advantage magnitude via normalization and the context-shift factor via sample-level prefix filtering; the
joint benefit is discussed in \Cref{sec:unified}. Our contributions are:

\begin{enumerate}
    \item \textbf{Leave-one-out baseline:} We adopt the leave-one-out (RLOO) baseline for its statistical independence between each sample and its baseline, avoiding correlated baseline artifacts. See \Cref{app:rloo-proof} for a detailed analysis.

    \item \textbf{Adaptive asymmetric normalization} (\Cref{sec:adaptive-norm}): We introduce a normalization scheme that applies
    different scaling factors $\alpha, \beta$ to positive and negative advantages. The scheme provably preserves gradient direction while
    enforcing zero empirical mean and unit empirical variance on non-zero tokens (the constraints in \Cref{def:adaptive-norm}).

    \item \textbf{Prefix Causal Trust Region masking with causal structure (\Cref{sec:prefix-is}):} We propose a prefix-cumulative masking
    strategy constructed from the per-token ratio $\rho_t=\pitheta/\piroll$. Motivated by long-horizon trust-region analyses for LLM-RL
    (e.g., TRM~\citep{li2025trm}), we apply a per-position prefix gate to the token-level loss, respecting the autoregressive causal
    structure. This differs from TRM, which uses a sequence-level acceptance gate $M(x,y)$ to discard entire trajectories.
    Under the explicit sufficient conditions in Theorem~\ref{thm:prefix-tighter}, prefix masking can provide tighter filtering (i.e., mask at
    least as many ratio-violating tokens) than token-level methods (which ignore causal dependencies) and sequence-level methods (which lose
    positional information) in three analyzed deviation patterns (early deviation, late deviation, and monotone drift).

    \item \textbf{Connection to the Adaptive bound (\Cref{sec:adaptive-connection}):} We relate prefix masking to the per-position
    structure of the Adaptive bound from the trust-region framework~\citep{li2025trm}: the KL chain rule yields an expectation identity for
    cumulative log-ratios, and thresholding the sample-level prefix log-ratio provides a practical proxy for Causal Trust Region filtering at
    each position.
\end{enumerate}

\textbf{Paper organization.} \Cref{sec:preliminaries} introduces notation, assumptions, and the surrogate objective error decomposition.
\Cref{sec:reinforce-max} presents REINFORCE Max (leave-one-out baseline, token expansion, and adaptive normalization).
\Cref{sec:reinforce-pro} develops REINFORCE Pro (prefix-level Causal Trust Region masking and its connection to the Adaptive bound).
\Cref{sec:unified} unifies both components and presents the full algorithm. \Cref{sec:related} discusses related work,
\Cref{sec:experiments} describes experimental evaluation, and \Cref{sec:conclusion} concludes.
