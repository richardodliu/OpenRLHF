\section{The Unified Framework}
\label{sec:unified}

\subsection{Why the Two Components Are Complementary}

The error decomposition (\Cref{eq:error-decomp}) has two factors at each position $t$:
\begin{equation}
    |\mathrm{Error}| \le \sum_{t=1}^T \underbrace{2\|g_t\|_\infty}_{\text{advantage factor}} \cdot \underbrace{\|d_t^{\pitheta} - d_t^{\piroll}\|_{\mathrm{TV}}}_{\text{context shift factor}}.
\end{equation}

\textbf{Component 1 (REINFORCE Max)} controls the advantage factor. The leave-one-out baseline provides statistical independence between
each sample and its baseline (\Cref{prop:rloo-variance}), and the adaptive normalization enforces zero empirical mean and unit empirical
variance on non-zero tokens (equivalently, the constraints in \Cref{def:adaptive-norm}), stabilizing the effective advantage scale and
mitigating the impact of outlier advantages on gradient magnitude.

\textbf{Component 2 (REINFORCE Pro)} targets the context-shift factor by filtering updates that exhibit large prefix drift between the
behavior policy $\piroll$ and the training policy $\pitheta$. The prefix-level mask rejects tokens whose prefix average log-ratio leaves the threshold
interval (\Cref{cor:per-position-trust}). The KL chain rule yields an expectation identity relating cumulative log-ratios to a joint KL
divergence between prefix distributions (\Cref{lem:cumsum-kl}); together with the threshold equivalence (\Cref{rem:prefix-proxy}), this
motivates viewing prefix masking as a sample-level proxy for Causal Trust Region filtering on the same policy pair $(\piroll,\pitheta)$ that
appears in the error decomposition.

\begin{remark}[Heuristic complementarity]
The discussion above should be interpreted as heuristic rather than as a formal inequality. It highlights that REINFORCE Max targets the
advantage factor in \eqref{eq:error-decomp}, while REINFORCE Pro targets policy drift between $\piroll$ and $\pitheta$ through sample-level
prefix filtering.
Together, they address two distinct sources of instability in the surrogate optimization pipeline.
\end{remark}

\begin{remark}[Variance Reduction]
Intuitively, the two components of REINFORCE Pro Max reduce gradient variance through complementary mechanisms. Component~1 (leave-one-out
baseline + adaptive normalization) reduces the variance of the advantage estimates themselves (\Cref{prop:rloo-variance},
\Cref{prop:alpha-beta}). Component~2 (prefix-level masking) attenuates high-variance gradient contributions from ratio-outlier tokens
(\Cref{thm:prefix-tighter}), where the importance ratios would otherwise amplify noise.
\end{remark}

\subsection{Algorithm}

Algorithm~\ref{alg:promax} presents the complete REINFORCE Pro Max training procedure. It unifies variance-reduced advantage estimation
(REINFORCE Max, \Cref{sec:reinforce-max}) and prefix-level Causal Trust Region masking (REINFORCE Pro, \Cref{sec:reinforce-pro}) into a single
training loop. The algorithm processes one prompt at a time: it generates $n$ rollout samples from the behavior policy, computes
advantages with the RLOO baseline and adaptive normalization, then applies the prefix-level mask during the loss computation. We
highlight the key design choices below.

\begin{algorithm}[t]
\caption{REINFORCE Pro Max (minimizing $\mathcal{L}^{\mathrm{ProMax}}$ in \eqref{eq:promax-loss})}
\label{alg:promax}
\begin{algorithmic}[1]
    \REQUIRE Prompt $x$; $n$ samples; thresholds $[1-\epslow, 1+\epshigh]$
    \STATE \textbf{Rollout:} Generate $\{y^{(1)}, \ldots, y^{(n)}\} \sim \piroll(\cdot|x)$; store $\log \piroll(y_t|c_t)$
    \STATE \textbf{Reward:} Compute rewards $\{r_1, \ldots, r_n\}$
    \STATE \textbf{Baseline:} $\tilde{r}_i \leftarrow r_i - \frac{1}{n-1}\sum_{j \neq i} r_j$ \hfill \COMMENT{leave-one-out}
    \STATE \textbf{Token Expansion:} $A_{i,t} \leftarrow \tilde{r}_i$ \hfill \COMMENT{sparse reward, $\gamma\!=\!1$}
    \STATE \textbf{Adaptive Normalization:} Compute $\alpha, \beta$ via Eq.~\eqref{eq:alpha-beta}; $\hat{A}_{i,t} \leftarrow \alpha A_{i,t}$ or $\beta A_{i,t}$
    \STATE \hspace{2.6em} (If $\mathcal{P} = \emptyset$ or $\mathcal{N} = \emptyset$: skip normalization)
    \STATE \textbf{Forward Pass:} Compute $\log \pitheta(y_t|c_t)$; use cached $\log \piroll(y_t|c_t)$ from rollout
    \STATE \textbf{Ratio:} $\rho_t \leftarrow \exp\!\big(\log \pitheta(y_t|c_t) - \log \piroll(y_t|c_t)\big)$
    \STATE \textbf{Prefix Proxy:} Compute $\log\rho_t$
    \STATE \hspace{2.6em} Compute $\rhoprefix{t}$ as in Eq.~\eqref{eq:prefix-is}
    \STATE \hspace{2.6em} Compute $M_t^{\mathrm{pre}}$ as in Eq.~\eqref{eq:prefix-mask}
    \STATE \textbf{Loss:} $\mathcal{L} \leftarrow \mathcal{L}^{\mathrm{ProMax}}$ (Eq.~\eqref{eq:promax-loss}) \hfill \COMMENT{negative surrogate}
    \STATE \textbf{Update:} $\theta \leftarrow \theta - \alpha_{\mathrm{lr}} \nabla_\theta \mathcal{L}$
\end{algorithmic}
\end{algorithm}

\textbf{Step-by-step walkthrough.}

\emph{Rollout (Line~1).} Samples are generated by a rollout engine (e.g., vLLM) running the behavior policy $\piroll$. The per-token
log-probabilities $\log\piroll(y_t|c_t)$ are stored alongside the generated tokens; these cached values anchor the importance ratio
$\rho_t=\pitheta/\piroll$ in \eqref{eq:ratio} during the subsequent gradient steps. This design follows the trust-region anchoring choice analyzed in
\citet{qi2026rethinkingtrustregionllm}; see Appendix~\ref{app:mismatch}.

\emph{Baseline and Token Expansion (Lines~3--4).} The leave-one-out baseline (\Cref{def:rloo}) ensures that each sample's baseline $b_i$ is statistically independent of its own reward $r_i$, avoiding the correlated baseline artifacts of the mean baseline (\Cref{app:rloo-proof}). Under sparse reward with $\gamma = 1$ (\Cref{app:gamma-one}), the token-level advantage reduces to the shaped reward: $A_{i,t} = \tilde{r}_i$ for all positions $t$, eliminating the need for per-token reward assignment or discounted cumulative sums.

\emph{Adaptive Normalization (Lines~5--6).} The scaling factors $\alpha$ and $\beta$ are computed in closed form from
Eq.~\eqref{eq:alpha-beta}, enforcing the empirical constraints in \Cref{def:adaptive-norm} on non-zero tokens while preserving the sign
and relative ordering of advantages (\Cref{par:adv-sign-preservation}). When all $n$ samples receive the same reward (i.e.,
$\mathcal{P} = \emptyset$ or $\mathcal{N} = \emptyset$), the RLOO baseline yields zero advantages; in this degenerate case,
normalization is skipped and the optional uniform scale mode (\Cref{app:uniform-scale}) can be used to recover gradient signal.

\emph{Forward Pass (Line~7).} The training framework computes log-probabilities under the current policy $\pitheta$. The rollout-policy
log-probabilities $\log\piroll(y_t|c_t)$ are reused from the rollout cache, as in standard PPO implementations.

\emph{Prefix Causal Trust Region Masking (Lines~9--11).} The per-token log-ratio $\log\rho_t$ captures local policy drift between $\pitheta$ and
$\piroll$. The prefix cumulative statistic $\rhoprefix{t}$ in \eqref{eq:prefix-is} aggregates this drift causally: it depends only
on positions $s\le t$, respecting the autoregressive order (\Cref{def:prefix-is}). The corresponding binary mask
$M_t^{\mathrm{pre}}$ in \eqref{eq:prefix-mask} filters out positions where the prefix average log-ratio leaves
$[\log(1-\epslow),\log(1+\epshigh)]$, enforcing a per-position, sample-level constraint (\Cref{cor:per-position-trust}).

\emph{Loss Computation (Line~12).} The loss \eqref{eq:promax-loss} multiplies the per-token importance-weighted surrogate term
$\rho_t \hat{A}_t$ by a detached prefix gate. Concretely, $M_t^{\mathrm{pre}}$ is a binary mask (detached) and the normalized
advantage $\hat{A}_t$ is detached, so gradients propagate only through $\rho_t$. The negative sign ensures that minimizing
$\mathcal{L}$ maximizes the surrogate objective.

\emph{Computational overhead.} Compared to standard PPO implementations that cache rollout-time log-probabilities, REINFORCE Pro Max adds
only lightweight arithmetic overhead. The RLOO baseline requires $O(n)$ arithmetic operations per prompt group. Adaptive normalization
computes four partial sums ($S^+, S^-, Q^+, Q^-$) in a single pass. The prefix-level mask requires one cumulative sum over the sequence,
which is $O(T)$ per sample. All components are computed from cached rollout log-probabilities and current-policy log-probabilities,
requiring no additional model evaluations beyond those already used to compute $\log\pitheta$.

\subsection{Comparison with Existing Methods}

\Cref{tab:comparison} compares REINFORCE Pro Max with existing critic-free LLM-RL methods along four dimensions: baseline type, advantage normalization strategy, IS correction granularity, and whether the IS correction preserves the causal structure of autoregressive generation. These dimensions correspond to the key design choices that affect the two factors in the error decomposition (\Cref{eq:error-decomp}): the advantage factor (targeted by baseline and normalization) and the context shift factor (targeted by IS correction).

\begin{table}[ht]
\centering
\caption{Comparison of critic-free LLM-RL methods.}
\label{tab:comparison}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Baseline} & \textbf{Normalization} & \textbf{IS Correction} & \textbf{Causal IS} \\
\midrule
REINFORCE++ & Mean & Global $\mu/\sigma$ & None & --- \\
GRPO & Mean & Global $\mu/\sigma$ & None & --- \\
RLOO & Leave-one-out & None & None & --- \\
DAPO\footnote{DAPO: Decoupled Alignment via Policy Optimization~\citep{yu2025dapo}.} + TIS & Mean & Global $\mu/\sigma$ & Token clip ($\rho_t$) & No \\
DAPO + IcePop & Mean & Global $\mu/\sigma$ & Token mask ($\rho_t$) & No \\
GSPO & Mean & Global $\mu/\sigma$ & Seq clip ($\rhoseq$) & No \\
GRPO + TRM & Mean & Global $\mu/\sigma$ & Seq mask (max-KL) & No \\
\midrule
\textbf{Pro Max} & \textbf{RLOO} & \textbf{Adaptive $\alpha/\beta$} & \textbf{Prefix mask ($\rhoprefix{t}$)} & \textbf{Yes} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Baseline and normalization.} REINFORCE++~\citep{hu2025reinforce} and GRPO~\citep{shao2024deepseekmath} use the batch mean reward as a baseline, which introduces correlation between each sample and its own baseline (\Cref{app:rloo-proof}). Both apply global $\mu/\sigma$ normalization, which uses a single scaling factor for all advantages regardless of sign. In reasoning tasks where reward distributions are highly skewed (most samples fail, few succeed), this uniform scaling distorts the relative balance between positive and negative advantages. RLOO~\citep{ahmadian2024back} improves the baseline by using the leave-one-out estimator, which eliminates the self-correlation artifact, but applies no normalization and no ratio-based filtering, leaving both the advantage magnitude and policy-ratio drift uncontrolled.

\textbf{IS correction methods.} Token-level corrections (TIS, IcePop) operate directly on the per-token ratio $\rho_t$. TIS clamps $\rho_t$
to a fixed interval $[1-\epslow, 1+\epshigh]$, retaining a biased but bounded weight at every position. IcePop instead masks out tokens whose
ratio falls outside the interval, eliminating extreme-weight bias at the cost of discarding gradient signal. Both treat positions
independently and therefore do not account for the downstream context shift caused by an early deviation (\Cref{prop:token-is-fails}).

Sequence-level corrections aggregate ratios over the whole trajectory. GSPO~\citep{gspo2025} clips the length-normalized sequence
likelihood ratio $\rhoseq$ (a geometric mean of token ratios) and applies a single coefficient to all tokens, which aligns with
sequence-level rewards but loses positional information and can dilute isolated deviations at long horizons (\Cref{prop:seq-is-fails}).
TRM~\citep{li2025trm} applies a binary sequence gate (e.g., a max-KL trust-region criterion) and rejects entire trajectories, which is
theoretically principled but sacrifices per-position granularity.

\textbf{REINFORCE Pro Max.} REINFORCE Pro Max targets all four dimensions in \Cref{tab:comparison}. The RLOO
baseline provides statistical independence (\Cref{prop:rloo-variance}); adaptive normalization preserves advantage sign while enforcing
zero empirical mean and unit empirical variance on non-zero tokens (\Cref{prop:alpha-beta}, \Cref{par:adv-sign-preservation}); and prefix
cumulative IS provides causal, per-position masking that detects both early deviations propagating forward and cumulative drift from many
small mismatches (\Cref{thm:prefix-tighter}). Each design choice targets a specific factor in the error decomposition, and the two
components are complementary (\Cref{sec:unified}): REINFORCE Max targets the advantage factor while REINFORCE Pro targets the context shift
factor.
