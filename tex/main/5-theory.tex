\section{The Unified Framework}
\label{sec:unified}

\subsection{Why the Two Components Are Complementary}

The error decomposition (\Cref{eq:error-decomp}) has two factors at each position $t$:
\begin{equation}
    |\mathrm{Error}| \le \sum_{t=1}^T \underbrace{2\|g_t\|_\infty}_{\text{advantage factor}} \cdot \underbrace{\|d_t^{\pitheta} - d_t^{\piroll}\|_{\mathrm{TV}}}_{\text{context shift factor}}.
\end{equation}

\textbf{Component 1 (REINFORCE Max)} controls the advantage factor. The leave-one-out baseline provides statistical independence between
each sample and its baseline (\Cref{prop:rloo-variance}), and the adaptive normalization enforces zero empirical mean and unit empirical
variance on non-zero tokens (equivalently, the constraints in \Cref{def:adaptive-norm}), stabilizing the effective advantage scale and
mitigating the impact of outlier advantages on gradient magnitude.

\textbf{Component 2 (REINFORCE Pro)} targets the rollout/training mismatch. Prefix IS masks tokens whose prefix average log-ratio falls
outside the threshold interval, i.e., it enforces a per-position, sample-level constraint on the cumulative $\piold/\piroll$ log-ratio
(\Cref{cor:per-position-trust}). The KL chain rule yields an expectation identity relating the cumulative log-ratio to a joint KL
divergence between prefix distributions (\Cref{lem:cumsum-kl}); together with the threshold equivalence
(\Cref{rem:prefix-proxy}), this motivates viewing prefix masking as a trust-region style proxy. Note that the error decomposition involves
$\|d_t^{\pitheta} - d_t^{\piroll}\|_{\mathrm{TV}}$, while prefix masking directly concerns the $\piold/\piroll$ mismatch; relating $\piold$
to $\pitheta$ requires additional assumptions on the PPO update (e.g., small policy drift enforced by clipping), so we keep the formal scope
of the proxy at the $\piold$ level.

Together, both factors are addressed simultaneously, intuitively yielding a tighter effective bound than addressing either factor alone. This is an intuitive summary rather than a formal inequality: the joint benefit arises because both terms in the error decomposition (\Cref{eq:error-decomp}) are targeted---the advantage magnitude by normalization, and the rollout/training mismatch by sample-level prefix filtering.

\begin{remark}[Variance Reduction]
\label{rem:variance-decomp}
Intuitively, the two components of REINFORCE Pro Max reduce gradient variance through complementary mechanisms. Component~1 (leave-one-out baseline + adaptive normalization) reduces the variance of the advantage estimates themselves (\Cref{prop:rloo-variance}, \Cref{prop:alpha-beta}). Component~2 (prefix IS masking) attenuates high-variance gradient contributions from off-policy tokens (\Cref{thm:prefix-tighter}), where the importance weights would otherwise amplify noise.
\end{remark}

\subsection{Algorithm}

Algorithm~\ref{alg:promax} presents the complete REINFORCE Pro Max training procedure. It unifies variance-reduced advantage estimation (REINFORCE Max, \Cref{sec:reinforce-max}) and causal off-policy correction (REINFORCE Pro, \Cref{sec:reinforce-pro}) into a single training loop. The algorithm processes one prompt at a time: it generates $n$ rollout samples, computes advantages with the RLOO baseline and adaptive normalization, then applies prefix cumulative IS masking during the loss computation. We highlight the key design choices below.

\begin{algorithm}[t]
\caption{REINFORCE Pro Max (minimizing $\mathcal{L} = -L_{\piroll}(\pitheta)$)}
\label{alg:promax}
\begin{algorithmic}[1]
    \REQUIRE Prompt $x$; $n$ samples; thresholds $[\lambda, \Lambda]$
    \STATE \textbf{Rollout:} Generate $\{y^{(1)}, \ldots, y^{(n)}\} \sim \piroll(\cdot|x)$ using vLLM; store $\log \piroll(y_t|c_t)$
    \STATE \textbf{Reward:} Compute rewards $\{r_1, \ldots, r_n\}$
    \STATE \textbf{Baseline:} $\tilde{r}_i \leftarrow r_i - \frac{1}{n-1}\sum_{j \neq i} r_j$ \hfill \COMMENT{leave-one-out}
    \STATE \textbf{Token Expansion:} $A_{i,t} \leftarrow \tilde{r}_i$ \hfill \COMMENT{sparse reward, $\gamma\!=\!1$}
    \STATE \textbf{Adaptive Normalization:} Compute $\alpha, \beta$ via Eq.~\eqref{eq:alpha-beta}; $\hat{A}_{i,t} \leftarrow \alpha A_{i,t}$ or $\beta A_{i,t}$
    \STATE \hspace{2.6em} (If $\mathcal{P} = \emptyset$ or $\mathcal{N} = \emptyset$: skip normalization)
    \STATE \textbf{Forward Pass:} Compute $\log \pitheta(y_t|c_t)$ and $\log \piold(y_t|c_t)$
    \STATE \textbf{Prefix IS:} $\ell_t \leftarrow \log \piold(y_t|c_t) - \log \piroll(y_t|c_t)$
    \STATE \hspace{2.6em} $\mathrm{prefix\_is}(t) \leftarrow \exp\!\big(\sum_{s \le t} \ell_s m_s \,/\, \sum_{s \le t} m_s\big)$
    \STATE \hspace{2.6em} $M_t \leftarrow \mathbb{I}[\mathrm{prefix\_is}(t) \in [\lambda, \Lambda]]$
    \STATE \textbf{Loss:} $\mathcal{L} \leftarrow -\sum_t M_t \cdot \exp(\ell_t) \cdot \mathrm{PPOClip}(\rho_t^{\mathrm{PPO}}, \hat{A}_t)$ \hfill \COMMENT{negative surrogate}
    \STATE \textbf{Update:} $\theta \leftarrow \theta - \alpha_{\mathrm{lr}} \nabla_\theta \mathcal{L}$
\end{algorithmic}
\end{algorithm}

\textbf{Step-by-step walkthrough.}

\emph{Rollout (Line~1).} Samples are generated by a high-throughput inference engine (e.g., vLLM) running the rollout policy $\piroll$. The per-token log-probabilities $\log\piroll(y_t|c_t)$ are stored alongside the generated tokens; these are needed later to compute the IS log-ratio $\ell_t$ without an additional forward pass through the rollout model.

\emph{Baseline and Token Expansion (Lines~3--4).} The leave-one-out baseline (\Cref{def:rloo}) ensures that each sample's baseline $b_i$ is statistically independent of its own reward $r_i$, avoiding the correlated baseline artifacts of the mean baseline (\Cref{app:rloo-proof}). Under sparse reward with $\gamma = 1$ (\Cref{app:gamma-one}), the token-level advantage reduces to the shaped reward: $A_{i,t} = \tilde{r}_i$ for all positions $t$, eliminating the need for per-token reward assignment or discounted cumulative sums.

\emph{Adaptive Normalization (Lines~5--6).} The scaling factors $\alpha$ and $\beta$ are computed in closed form from
Eq.~\eqref{eq:alpha-beta}, enforcing the empirical constraints in \Cref{def:adaptive-norm} on non-zero tokens while preserving the sign
and relative ordering of advantages (\Cref{prop:gradient-direction}). When all $n$ samples receive the same reward (i.e.,
$\mathcal{P} = \emptyset$ or $\mathcal{N} = \emptyset$), the RLOO baseline yields zero advantages; in this degenerate case,
normalization is skipped and the optional uniform scale mode (\Cref{app:uniform-scale}) can be used to recover gradient signal.

\emph{Forward Pass (Line~7).} The training framework computes log-probabilities under both the current policy $\pitheta$ and the old policy $\piold$. The old policy corresponds to the actor parameters at the start of the current training step (before mini-batch updates). In standard PPO implementations, $\log\piold$ is already computed and cached, so this step introduces no additional forward pass.

\emph{Prefix IS Masking (Lines~8--10).} The IS log-ratio $\ell_t = \log\piold(y_t|c_t) - \log\piroll(y_t|c_t)$ captures the per-token mismatch between the training actor and the rollout engine. The prefix cumulative geometric mean $\mathrm{prefix\_is}(t) = \exp(L_t/P_t)$ aggregates this mismatch causally: it depends only on positions $s \le t$, respecting the autoregressive generation order (\Cref{def:prefix-is}). The binary mask $M_t$ filters out positions where the prefix average log-ratio leaves the threshold interval $[\log\lambda, \log\Lambda]$, restricting gradient updates to tokens that satisfy the sample-level prefix constraint (\Cref{cor:per-position-trust}).

\emph{Loss Computation (Line~11).} The loss at each position is the product of three factors: (i)~the prefix mask $M_t$ (binary gate, detached), which excludes high-divergence tokens; (ii)~the token-level IS weight $\exp(\ell_t)$ (detached), which corrects for the remaining $\piroll \neq \piold$ mismatch on accepted tokens; and (iii)~the PPO clipped surrogate $\mathrm{PPOClip}(\rho_t^{\mathrm{PPO}}, \hat{A}_t)$, which is the only term carrying gradients. The negative sign ensures that minimizing $\mathcal{L}$ maximizes the surrogate objective (\Cref{eq:promax-loss}).

\emph{Computational overhead.} Compared to standard PPO, REINFORCE Pro Max adds negligible overhead. The RLOO baseline requires $O(n)$ arithmetic operations per prompt group. Adaptive normalization computes four partial sums ($S^+, S^-, Q^+, Q^-$) in a single pass. The prefix IS mask requires one cumulative sum over the sequence, which is $O(T)$ per sample. No additional forward passes or model evaluations are needed beyond what standard PPO already requires.

\subsection{Comparison with Existing Methods}

\Cref{tab:comparison} compares REINFORCE Pro Max with existing critic-free LLM-RL methods along four dimensions: baseline type, advantage normalization strategy, IS correction granularity, and whether the IS correction preserves the causal structure of autoregressive generation. These dimensions correspond to the key design choices that affect the two factors in the error decomposition (\Cref{eq:error-decomp}): the advantage factor (targeted by baseline and normalization) and the context shift factor (targeted by IS correction).

\begin{table}[h]
\centering
\caption{Comparison of critic-free LLM-RL methods.}
\label{tab:comparison}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Baseline} & \textbf{Normalization} & \textbf{IS Correction} & \textbf{Causal IS} \\
\midrule
REINFORCE++ & Mean & Global $\mu/\sigma$ & None & --- \\
GRPO & Mean & Global $\mu/\sigma$ & None & --- \\
RLOO & Leave-one-out & None & None & --- \\
DAPO\footnote{DAPO: Decoupled Alignment via Policy Optimization~\citep{yu2025dapo}.} + TIS & Mean & Global $(\mu,\sigma)$ & Token clamp & No \\
DAPO + ICEPOP & Mean & Global $(\mu,\sigma)$ & Token mask & No \\
\midrule
\textbf{Pro Max} & \textbf{RLOO} & \textbf{Adaptive $\alpha/\beta$} & \textbf{Prefix cumulative} & \textbf{Yes} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Baseline and normalization.} REINFORCE++~\citep{hu2025reinforce} and GRPO~\citep{shao2024deepseekmath} use the batch mean reward as a baseline, which introduces correlation between each sample and its own baseline (\Cref{app:rloo-proof}). Both apply global $\mu/\sigma$ normalization, which uses a single scaling factor for all advantages regardless of sign. In reasoning tasks where reward distributions are highly skewed (most samples fail, few succeed), this uniform scaling distorts the relative balance between positive and negative advantages. RLOO~\citep{ahmadian2024back} improves the baseline by using the leave-one-out estimator, which eliminates the self-correlation artifact, but applies no normalization and no IS correction, leaving both the advantage magnitude and the off-policy mismatch uncontrolled.

\textbf{IS correction methods.} DAPO~\citep{yu2025dapo} combined with TIS clamps the per-token IS weight to a fixed interval $[\lambda, \Lambda]$, retaining a biased but bounded correction at every position. DAPO combined with ICEPOP takes a harder approach: tokens whose IS weight falls outside the interval are zeroed out entirely, which eliminates bias from extreme weights but discards potentially useful gradient signal. Both methods operate at the token level and treat each position independently. As shown in \Cref{prop:token-is-fails}, this means that an early deviation ($w_1 \gg 1$) is corrected only at position~1, while all subsequent positions---whose contexts are sampled from a mismatched distribution---remain uncorrected.

\textbf{REINFORCE Pro Max.} Our method is the only one in this comparison that simultaneously addresses all four dimensions. The RLOO
baseline provides statistical independence (\Cref{prop:rloo-variance}); adaptive normalization preserves gradient direction while enforcing
zero empirical mean and unit empirical variance on non-zero tokens (\Cref{prop:alpha-beta}, \Cref{prop:gradient-direction}); and prefix
cumulative IS provides causal, per-position masking that detects both early deviations propagating forward and cumulative drift from many
small mismatches (\Cref{thm:prefix-tighter}). Each design choice targets a specific factor in the error decomposition, and the two
components are complementary (\Cref{sec:unified}): REINFORCE Max targets the advantage factor while REINFORCE Pro targets the context shift
factor.
