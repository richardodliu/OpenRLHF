\section{The Unified Framework}
\label{sec:unified}

\subsection{Why the Two Components Are Complementary}

The error decomposition (\Cref{eq:error-decomp}) has two factors at each position $t$:
\begin{equation}
    |\mathrm{Error}| \le \sum_{t=1}^T \underbrace{2\|g_t\|_\infty}_{\text{advantage factor}} \cdot \underbrace{\|d_t^{\pitheta} - d_t^{\piroll}\|_{\mathrm{TV}}}_{\text{context shift factor}}.
\end{equation}

\textbf{Component 1 (REINFORCE Max)} controls the advantage factor. The leave-one-out baseline provides statistical independence between
each sample and its baseline (\Cref{prop:rloo-variance}), and the adaptive normalization enforces zero empirical mean and unit empirical
variance on non-zero tokens (equivalently, the constraints in \Cref{def:adaptive-norm}), stabilizing the effective advantage scale and
mitigating the impact of outlier advantages on gradient magnitude.

\textbf{Component 2 (REINFORCE Pro)} targets the context-shift factor by filtering updates that exhibit large prefix drift between the
behavior policy $\piroll$ and the training policy $\pitheta$. The prefix-level mask rejects tokens whose prefix average log-ratio leaves the threshold
interval (\Cref{cor:per-position-trust}). The KL chain rule yields an expectation identity relating cumulative log-ratios to a joint KL
divergence between prefix distributions (\Cref{lem:cumsum-kl}); together with the threshold equivalence (\Cref{rem:prefix-proxy}), this
motivates viewing prefix masking as a sample-level proxy for Causal Trust Region filtering on the same policy pair $(\piroll,\pitheta)$ that
appears in the error decomposition.

\begin{remark}[Heuristic complementarity]
The discussion above should be interpreted as heuristic rather than as a formal inequality. It highlights that REINFORCE Max targets the
advantage factor in \eqref{eq:error-decomp}, while REINFORCE Pro targets policy drift between $\piroll$ and $\pitheta$ through sample-level
prefix filtering.
Together, they address two distinct sources of instability in the surrogate optimization pipeline.
\end{remark}

\begin{remark}[Variance Reduction]
Intuitively, the two components of REINFORCE Pro Max reduce gradient variance through complementary mechanisms. Component~1 (leave-one-out
baseline + adaptive normalization) reduces the variance of the advantage estimates themselves (\Cref{prop:rloo-variance},
\Cref{eq:alpha-beta}). Component~2 (prefix-level masking) attenuates high-variance gradient contributions from ratio-outlier tokens
(\Cref{thm:prefix-tighter}), where the importance ratios would otherwise amplify noise.
\end{remark}

\subsection{Algorithm}

Algorithm~\ref{alg:promax} presents the complete REINFORCE Pro Max training procedure. It unifies variance-reduced advantage estimation
(REINFORCE Max, \Cref{sec:reinforce-max}) and prefix-level Causal Trust Region masking (REINFORCE Pro, \Cref{sec:reinforce-pro}) into a single
training loop. The algorithm processes one prompt at a time: it generates $n$ rollout samples from the behavior policy, computes
advantages with the RLOO baseline and adaptive normalization, then applies the prefix-level mask during the loss computation. We
highlight the key design choices below.

\begin{algorithm}[t]
\caption{REINFORCE Pro Max: minimizing $\mathcal{L}^{\mathrm{ProMax}}$ in Eq.~\ref{eq:promax-loss}.}
\label{alg:promax}
\begin{algorithmic}[1]
    \REQUIRE Prompt $x$; $n$ samples; thresholds $[1-\epslow, 1+\epshigh]$
    \STATE \textbf{Rollout:} Generate $\{y^{(1)}, \ldots, y^{(n)}\} \sim \piroll(\cdot|x)$; store $\log \piroll(y_t|c_t)$
    \STATE \textbf{Reward:} Compute rewards $\{r_1, \ldots, r_n\}$
    \STATE \textbf{Baseline:} $\tilde{r}_i \leftarrow r_i - \frac{1}{n-1}\sum_{j \neq i} r_j$ \hfill \COMMENT{leave-one-out}
    \STATE \textbf{Token Expansion:} $A_{i,t} \leftarrow \tilde{r}_i$ \hfill \COMMENT{sparse reward, $\gamma\!=\!1$}
    \STATE \textbf{Adaptive Normalization:} Compute $\alpha, \beta$ via Eq.~\eqref{eq:alpha-beta}; $\hat{A}_{i,t} \leftarrow \alpha A_{i,t}$ or $\beta A_{i,t}$
    \STATE \hspace{2.6em} (If $\mathcal{P} = \emptyset$ or $\mathcal{N} = \emptyset$: skip normalization)
    \STATE \textbf{Forward Pass:} Compute $\log \pitheta(y_t|c_t)$; use cached $\log \piroll(y_t|c_t)$ from rollout
    \STATE \textbf{Ratio:} $\rho_t \leftarrow \exp\!\big(\log \pitheta(y_t|c_t) - \log \piroll(y_t|c_t)\big)$
    \STATE \textbf{Prefix Proxy:} Compute $\log\rho_t$
    \STATE \hspace{2.6em} Compute $\rhoprefix{t}$ as in Eq.~\eqref{eq:prefix-is}
    \STATE \hspace{2.6em} Compute $M_t^{\mathrm{pre}}$ as in Eq.~\eqref{eq:prefix-mask}
    \STATE \textbf{Loss:} $\mathcal{L} \leftarrow \mathcal{L}^{\mathrm{ProMax}}$ (Eq.~\eqref{eq:promax-loss}) \hfill \COMMENT{negative surrogate}
    \STATE \textbf{Update:} $\theta \leftarrow \theta - \alpha_{\mathrm{lr}} \nabla_\theta \mathcal{L}$
\end{algorithmic}
\end{algorithm}

\textbf{Step-by-step walkthrough.}

\emph{Rollout (Line~1).} Samples are generated by a rollout engine (e.g., vLLM) running the behavior policy $\piroll$. The per-token
log-probabilities $\log\piroll(y_t|c_t)$ are stored alongside the generated tokens; these cached values anchor the importance ratio
$\rho_t=\pitheta/\piroll$ in \eqref{eq:ratio} during the subsequent gradient steps. This design follows the trust-region anchoring choice analyzed in
\citet{qi2026rethinkingtrustregionllm}; see Appendix~\ref{app:mismatch}.

\emph{Baseline and Token Expansion (Lines~3--4).} The leave-one-out baseline (\Cref{def:rloo}) ensures that each sample's baseline $b_i$ is statistically independent of its own reward $r_i$, avoiding the correlated baseline artifacts of the mean baseline (\Cref{app:rloo-proof}). Under sparse reward with $\gamma = 1$ (\Cref{app:gamma-one}), the token-level advantage reduces to the shaped reward: $A_{i,t} = \tilde{r}_i$ for all positions $t$, eliminating the need for per-token reward assignment or discounted cumulative sums.

\emph{Adaptive Normalization (Lines~5--6).} The scaling factors $\alpha$ and $\beta$ are computed in closed form from
Eq.~\eqref{eq:alpha-beta}, enforcing the empirical constraints in \Cref{def:adaptive-norm} on non-zero tokens while preserving the sign
and relative ordering of advantages (\Cref{par:adv-sign-preservation}). When all $n$ samples receive the same reward (i.e.,
$\mathcal{P} = \emptyset$ or $\mathcal{N} = \emptyset$), the RLOO baseline yields zero advantages; in this degenerate case,
normalization is skipped and the optional uniform scale mode (\Cref{app:uniform-scale}) can be used to recover gradient signal.

\emph{Forward Pass (Line~7).} The training framework computes log-probabilities under the current policy $\pitheta$. The rollout-policy
log-probabilities $\log\piroll(y_t|c_t)$ are reused from the rollout cache, as in standard PPO implementations.

\emph{Prefix Causal Trust Region Masking (Lines~9--11).} The per-token log-ratio $\log\rho_t$ captures local policy drift between $\pitheta$ and
$\piroll$. The prefix cumulative statistic $\rhoprefix{t}$ in \eqref{eq:prefix-is} aggregates this drift causally: it depends only
on positions $s\le t$, respecting the autoregressive order (\Cref{def:prefix-is}). The corresponding binary mask
$M_t^{\mathrm{pre}}$ in \eqref{eq:prefix-mask} filters out positions where the prefix average log-ratio leaves
$[\log(1-\epslow),\log(1+\epshigh)]$, enforcing a per-position, sample-level constraint (\Cref{cor:per-position-trust}).

\emph{Loss Computation (Line~12).} The loss \eqref{eq:promax-loss} multiplies the per-token importance-weighted surrogate term
$\rho_t \hat{A}_t$ by a detached prefix gate. Concretely, $M_t^{\mathrm{pre}}$ is a binary mask (detached) and the normalized
advantage $\hat{A}_t$ is detached, so gradients propagate only through $\rho_t$. The negative sign ensures that minimizing
$\mathcal{L}$ maximizes the surrogate objective.

\emph{Computational overhead.} Compared to standard PPO implementations that cache rollout-time log-probabilities, REINFORCE Pro Max adds
only lightweight arithmetic overhead. The RLOO baseline requires $O(n)$ arithmetic operations per prompt group. Adaptive normalization
computes the required first- and second-moment aggregates over positive and negative advantage tokens in a single pass. The prefix-level
mask requires one cumulative sum over the sequence,
which is $O(T)$ per sample. All components are computed from cached rollout log-probabilities and current-policy log-probabilities,
requiring no additional model evaluations beyond those already used to compute $\log\pitheta$.

\subsection{Comparison with Other Advantage Estimation Methods}

We compare REINFORCE Pro Max with existing critic-free LLM-RL methods along four dimensions: baseline type, advantage normalization
strategy, IS correction granularity, and whether the IS correction preserves the causal structure of autoregressive generation. We
organize the comparison into two parts: (i) baseline and normalization choices (this subsection) and (ii) IS correction mechanisms (the
next subsection). These dimensions correspond to the key design choices that affect the two factors in the error decomposition
(\Cref{eq:error-decomp}): the advantage factor (targeted by baseline and normalization) and the context shift factor (targeted by IS
correction).

\textbf{Baseline and normalization.} REINFORCE++~\citep{hu2025reinforce} and GRPO~\citep{shao2024deepseekmath} use the batch mean reward as a baseline, which introduces correlation between each sample and its own baseline (\Cref{app:rloo-proof}). Both apply global $\mu/\sigma$ normalization, which uses a single scaling factor for all advantages regardless of sign. In reasoning tasks where reward distributions are highly skewed (most samples fail, few succeed), this uniform scaling distorts the relative balance between positive and negative advantages. RLOO~\citep{ahmadian2024back} improves the baseline by using the leave-one-out estimator, which eliminates the self-correlation artifact, but applies no normalization and no ratio-based filtering, leaving both the advantage magnitude and policy-ratio drift uncontrolled.

\subsection{Comparison with Other IS Correction Methods}

We adopt a trust-region masking viewpoint: when mismatch between $\piroll$ and $\pitheta$ is large, one should attenuate or reject
unreliable gradient contributions. Existing methods differ mainly in the granularity of this attenuation (see \Cref{sec:existing-is} and
\Cref{tab:is-methods} for a unified notation).

\textbf{Token-level truncation and masking.} Under this view, TIS and IcePop are token-level analogues of trust-region masking: they operate
directly on the per-token ratio $\rho_t$. TIS clamps $\rho_t$ to a fixed interval $[1-\epslow, 1+\epshigh]$, retaining a biased but bounded
weight at every position. IcePop instead masks out tokens whose ratio falls outside the interval, eliminating extreme-weight bias at the
cost of discarding gradient signal. Both treat positions independently and therefore do not account for the downstream context shift caused
by an early deviation (\Cref{prop:token-is-fails}).

\textbf{Sequence-level aggregation and gating.} Sequence-level corrections aggregate ratios over the whole trajectory. GSPO~\citep{gspo2025}
clips the length-normalized sequence likelihood ratio $\rhoseq$ (a geometric mean of token ratios) and applies a single coefficient to all
tokens, which aligns with sequence-level rewards but loses positional information and can dilute isolated deviations at long horizons
(\Cref{prop:seq-is-fails}). TRM~\citep{li2025trm} applies a binary sequence gate (e.g., a max-KL trust-region criterion) and rejects entire
trajectories, which is theoretically principled but sacrifices per-position granularity.

\textbf{Prefix-level (causal) filtering.} Token- and sequence-level mechanisms leave a gap for long-horizon generation: token-level methods
ignore how an early deviation propagates to future contexts (\Cref{prop:token-is-fails}), while sequence-level gates share a single
decision across all positions. Our prefix mask $M_t^{\mathrm{pre}}$ (defined in \Cref{sec:prefix-is}) provides a causal, per-position gate
constructed from prefix ratio statistics. Compared with TRM's sequence-level gate $M^{\mathrm{TRM}}(x,y)$, it preserves positional
granularity by masking only the token-level loss terms whose prefixes violate the threshold, while still respecting the autoregressive
order. The resulting filtering can detect both early deviations that propagate forward and cumulative drift from many small mismatches
(\Cref{thm:prefix-tighter}).

\textbf{REINFORCE Pro Max.} Taken together with the advantage-estimation choices discussed above, REINFORCE Pro Max targets all four
dimensions discussed above. The RLOO
baseline provides statistical independence (\Cref{prop:rloo-variance}); adaptive normalization preserves advantage sign while enforcing
zero empirical mean and unit empirical variance on non-zero tokens (\Cref{eq:alpha-beta}, \Cref{par:adv-sign-preservation}); and prefix
cumulative IS provides causal, per-position masking that detects both early deviations propagating forward and cumulative drift from many
small mismatches (\Cref{thm:prefix-tighter}). Each design choice targets a specific factor in the error decomposition, and the two
components are complementary (\Cref{sec:unified}): REINFORCE Max targets the advantage factor while REINFORCE Pro targets the context shift
factor.
