\section{Preliminaries}
\label{sec:preliminaries}

\subsection{Autoregressive Generation and Objective}

A (stochastic) policy $\pi$ generates a response $y = (y_1, \ldots, y_T)$ conditioned on a prompt $x$. The induced trajectory
probability admits the autoregressive factorization
\begin{equation}
    P^{\pi}(y \mid x) = \prod_{t=1}^T \pi(y_t \mid x, y_{<t}).
\end{equation}
For each position $t$, we define the context
\begin{equation}
    c_t := (x, y_{<t}) = (x, \ylt).
\end{equation}
The induced \emph{context visitation distribution} (the marginal distribution of length-$t$ prefixes) is
\begin{equation}
    d_t^{\pi}(c_t) = P(x)\, P^{\pi}(y_{<t} \mid x) = P(x) \prod_{s=1}^{t-1} \pi(y_s \mid c_s).
\end{equation}
Given a scalar reward $R(x, y)$, the objective of the current policy $\pitheta$ is
\begin{equation}
    J(\pitheta) = \E_{x \sim P(x),\, y \sim \pitheta(\cdot|x)}\big[R(x, y)\big].
\end{equation}

\subsection{Notation}
\label{sec:notation}

\textbf{Two policies.} We consider two policies:
\begin{itemize}
    \item $\piroll$: the \emph{behavior} (rollout) policy used to generate samples; it is fixed during each policy update;
    \item $\pitheta$: the \emph{training} policy being optimized by gradient steps.
\end{itemize}
This is the same two-policy setup as in long-horizon trust-region analyses for LLM-RL (e.g., \citet{li2025trm}).

\textbf{Importance ratios.} The per-token importance ratio and its log-ratio are
\begin{align}
    \rho_t
    &:= \frac{\pitheta(y_t \mid c_t)}{\piroll(y_t \mid c_t)},
    \label{eq:ratio}
    \\
    r_t
    &:= \log \rho_t
    = \log \pitheta(y_t \mid c_t) - \log \piroll(y_t \mid c_t).
    \label{eq:log-ratio}
\end{align}

\begin{remark}[From three distributions to two]
\label{rem:three-to-two}
Practical LLM-RL pipelines can exhibit an apparent three-distribution structure: (i) a rollout engine generates tokens according to an
inference-time distribution, (ii) a training framework computes gradients under $\pitheta$, and (iii) some implementations \emph{recompute}
the behavior log-probabilities inside the training stack, yielding a recomputed distribution that can differ from the true rollout
distribution even when parameters are identical. Following~\citet{qi2026rethinkingtrustregionllm}, the trust region (and hence the IS
ratio) must be anchored to the original behavior distribution that generated the data. Accordingly, we define $\rho_t$ in
\eqref{eq:ratio} using $\piroll$ and assume access to $\log \piroll(y_t\mid c_t)$ via rollout-time caching. The theoretical development
therefore involves only the policy pair $(\piroll,\pitheta)$, as in~\citet{li2025trm}.
\par
\noindent\textbf{Notation mapping.} In PPO-style implementations, the behavior snapshot used for rollouts is often denoted $\piold$.
Within a single policy update, this snapshot is precisely the data-generating policy $\piroll$ in our theory; we do not introduce a third
distribution.
\end{remark}

\textbf{Action mask.} We use an action mask $m_t \in \{0,1\}$ to select active action-token positions (excluding padding, and excluding
prompt-only positions if a unified tokenization is used). When we index only over action tokens without padding, one can take $m_t \equiv 1$.
Sums such as $\sum_t f_t m_t$ should be read as summing over active positions.

\subsection{Divergence Measures}
\label{sec:divergence}

We use token-level divergence measures between conditional token distributions, following the trust-region literature for long-horizon
LLM-RL~\citep{li2025trm}. For two policies $\pi$ and $\pi'$, define the token-level total variation and KL divergences at context $c_t$:

\begin{definition}[Token-level divergences]
\label{def:token-div}
For a context $c_t = (x, y_{<t})$ and two policies $\pi$ and $\pi'$,
\begin{align}
    \Dtvtok(c_t;\pi,\pi')
    &:= \DTV\big(\pi(\cdot|c_t)\,\|\,\pi'(\cdot|c_t)\big)
    = \tfrac{1}{2}\sum_{v} \big|\pi(v|c_t) - \pi'(v|c_t)\big|, \\
    \Dkltok(c_t;\pi,\pi')
    &:= \DKL\big(\pi(\cdot|c_t)\,\|\,\pi'(\cdot|c_t)\big)
    = \sum_{v} \pi(v|c_t)\log\frac{\pi(v|c_t)}{\pi'(v|c_t)}.
\end{align}
When the policy pair is clear from context, we write $\Dtvtok(c_t)$ and $\Dkltok(c_t)$.
\end{definition}

\begin{definition}[Max token-level divergences and expected per-position TV]
\label{def:max-div}
For the policy pair $(\piroll,\pitheta)$, define:
\begin{align}
    \epsilon
    &:= \Dtvtokmax := \max_{t,\,c_t} \Dtvtok(c_t),
    \\
    \delta
    &:= \Dkltokmax := \max_{t,\,c_t} \Dkltok(c_t),
    \\
    \Dbar_t
    &:= \E_{c_t \sim d_t^{\piroll}}\!\big[\Dtvtok(c_t)\big].
\end{align}
\end{definition}

\subsection{Assumptions}
\label{sec:assumptions}

The following assumptions are used throughout the paper:

\begin{assumption}[Policy Support Overlap]
\label{asm:support}
For all contexts $c_t$ and tokens $y_t$ in the vocabulary, $\piroll(y_t|c_t) > 0 \implies \pitheta(y_t|c_t) > 0$. This ensures the
IS ratio $\IStoken{t}$ in \eqref{eq:ratio} is well-defined, and that KL terms with $\piroll$ in the first argument are finite.
\end{assumption}

\begin{assumption}[Bounded Rewards]
\label{asm:bounded-reward}
There exists $R_{\max} < \infty$ such that $|R(x, y)| \le R_{\max}$ for all prompt-response pairs $(x, y)$.
\end{assumption}

\begin{assumption}[Finite Sequence Length]
\label{asm:finite-length}
The response length satisfies $T < \infty$ for all generated sequences.
\end{assumption}

\subsection{The Surrogate Objective and Error Decomposition}

Samples are generated from a behavior policy $\piroll$ that generally differs from $\pitheta$. Following~\citet{kakade2002approximately},
the standard surrogate objective uses the per-step advantage under $\piroll$,
\begin{equation}
    A_t^{\piroll}(c_t, y_t)
    := \E_{\piroll}\!\big[R(x, y)\mid c_t, y_t\big] - \E_{\piroll}\!\big[R(x, y)\mid c_t\big].
\end{equation}
This yields the per-step surrogate
\begin{equation}
    L_{\piroll}(\pitheta) = \sum_{t=1}^T \E_{c_t \sim d_t^{\piroll}}\!\Big[\E_{y_t \sim \piroll(\cdot|c_t)}\!\big[\IStoken{t} \, A_t^{\piroll}(c_t, y_t)\big]\Big],
    \label{eq:surrogate}
\end{equation}
where $\IStoken{t}$ is defined in \eqref{eq:ratio}.

\begin{definition}[Trajectory-level surrogate (implementation)]
\label{def:impl-surrogate}
Let $b$ be a baseline that is independent of the sampled trajectory $y$ (e.g., a prompt-level baseline computed from other samples in the
same prompt group). Define the trajectory-level advantage
\begin{equation}
    A := R(x, y) - b.
\end{equation}
The trajectory-level surrogate used in critic-free implementations is
\begin{equation}
    \tilde{L}_{\piroll}(\pitheta) := \E_{\piroll}\!\Big[A \cdot \sum_{t=1}^T \IStoken{t}\Big].
\end{equation}
\end{definition}

\begin{remark}[Per-step surrogate vs.\ implementation surrogate]
\label{rem:surrogate-approx}
The per-step advantage $A_t^{\piroll}(c_t,y_t)$ depends on the full future trajectory conditioned on $(c_t,y_t)$, and is generally
intractable without a learned value function. The surrogate $\tilde{L}_{\piroll}$ in \Cref{def:impl-surrogate} replaces the future
conditional advantage with a trajectory-wide scalar. Therefore $\tilde{L}_{\piroll}$ should be viewed as an \emph{approximation} rather
than an equivalent reformulation of $L_{\piroll}$ in \eqref{eq:surrogate}.
\end{remark}

\begin{remark}[On-policy unbiasedness vs.\ surrogate equivalence]
\label{rem:onpolicy-unbiasedness}
When $\piroll=\pitheta$ (on-policy) and $b$ is independent of the sampled trajectory, the REINFORCE identity gives
\begin{equation}
    \E_{\pitheta}\!\Big[A \cdot \sum_{t=1}^T \nabla_\theta \log \pitheta(y_t \mid c_t)\Big] = \nabla_\theta J(\pitheta),
\end{equation}
which provides an unbiased estimator of the policy gradient $\nabla_\theta J(\pitheta)$. This does not imply that the trajectory-level
surrogate $\tilde{L}_{\piroll}$ is equivalent to the per-step surrogate $L_{\piroll}$ in \eqref{eq:surrogate}. Accordingly, the error
decomposition below is stated in terms of $L_{\piroll}$, which yields a clean advantage/context-shift factorization, while the implemented
loss uses $\tilde{L}_{\piroll}$ as a practical approximation.
\end{remark}

The approximation error of the per-step surrogate is
\begin{equation}
    \mathrm{Error}(\pitheta) := J(\pitheta) - J(\piroll) - L_{\piroll}(\pitheta).
    \label{eq:error-def}
\end{equation}
Via the Performance Difference Identity~\citep{kakade2002approximately}, this error decomposes as:
\begin{equation}
    |\mathrm{Error}(\pitheta)| \le \sum_{t=1}^T 2\|g_t\|_\infty \cdot \|d_t^{\pitheta} - d_t^{\piroll}\|_{\mathrm{TV}},
    \label{eq:error-decomp}
\end{equation}
where
\begin{equation}
    g_t(c_t) := \E_{y_t \sim \pitheta(\cdot\mid c_t)}\!\big[A_t^{\piroll}(c_t, y_t)\big]
\end{equation}
is the expected advantage shift. Classical bounds on \eqref{eq:error-decomp} scale as $O(T^2)$~\citep{kakade2002approximately,
achiam2017constrained}, which is vacuous for long-horizon LLM tasks ($T>4000$). Recent work~\citep{li2025trm} derives tighter bounds,
including the Pinsker-Marginal bound ($O(T^{3/2})$), the Mixed bound ($O(T)$), and the data-dependent Adaptive bound.

The Adaptive bound is particularly relevant to our work:
\begin{equation}
    B_{\mathrm{Adap}}^{*} = 4 \sum_{t=1}^{T} \Dbar_t \cdot \min\!\Big(1,\;(T\!-\!t)\,\epsilon,\;\sqrt{\tfrac{(T-t)\,\delta}{2}}\Big),
    \label{eq:adaptive-bound}
\end{equation}
where $\Dbar_t$ is the expected per-position TV divergence, and $\epsilon,\delta$ are the max token-level divergences from
\Cref{def:max-div}. This bound operates at per-position granularity and preserves the causal structure of error accumulation, which
motivates our prefix-level trust-region masking component.

\subsection{Existing IS Correction Methods}
\label{sec:existing-is}

Several methods have been proposed to stabilize policy-gradient updates by constraining the per-token IS ratio $\IStoken{t}$ between the
training policy $\pitheta$ and the behavior policy $\piroll$. We use the per-token log-ratio $r_t = \log \IStoken{t}$ defined in
\eqref{eq:log-ratio}.
\par
\noindent\textbf{A unified view by granularity.} We adopt a trust-region masking viewpoint: when mismatch between $\piroll$ and
$\pitheta$ is large, one should attenuate or reject unreliable gradient contributions. Existing methods differ mainly in the
\emph{granularity} of this attenuation: token-level truncation/masking (TIS, ICEPOP), sequence-level masking/aggregation (TRM, GSPO), and
prefix-level (causal) filtering (our method in \Cref{sec:prefix-is}).
\par
\noindent\textbf{Interpretation.} Under this view, TIS and ICEPOP can be seen as token-level analogues of trust-region masking, while GSPO
and TRM apply sequence-level decisions (a single ratio statistic or a binary rejection gate) shared across all positions.
\par
\noindent\textbf{Prefix-level TRM-style masking.} Our prefix mask $M_t^{\mathrm{prefix}}$ can be viewed as a per-position refinement of the
sequence-level rejection mechanism in TRM: it applies a trust-region \emph{proxy} at each prefix and masks only the token-level loss terms
whose prefixes violate the threshold.

\begin{table}[ht]
\centering
\caption{Existing trust-region and IS stabilization methods in critic-free LLM-RL.}
\label{tab:existing-is}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{llll}
\toprule
\textbf{Method} & \textbf{Weight} & \textbf{Granularity} & \textbf{Causal} \\
\midrule
TIS & $\mathrm{clip}(\rho_t,\lambda,\Lambda)$ & Token & No \\
ICEPOP & $\rho_t \cdot \mathbb{I}[\rho_t \in [\lambda, \Lambda]]$ & Token & No \\
TRM & $M(x,y) \cdot \rho_t$ & Sequence & No \\
GSPO & $\rhoseq$ & Sequence & No \\
\bottomrule
\end{tabular}
\end{table}

\noindent Here $\rhoseq$ is the length-normalized sequence likelihood ratio (a geometric mean of token ratios),
\begin{equation}
    \rhoseq
    := \left(\frac{\pitheta(y \mid x)}{\piroll(y \mid x)}\right)^{\frac{1}{|y|}}
    = \exp(\bar{r}),
    \qquad
    \bar{r} := \frac{\sum_t r_t m_t}{\sum_t m_t},
\end{equation}
and $m_t$ is the action mask. This definition matches the sequence-level ratio used in GSPO~\citep{gspo2025} (denoted $s_i(\theta)$ in the
GSPO paper) up to notation.

TRM (Trust Region Masking, \citet{li2025trm}) uses a binary \emph{sequence-level} mask $M(x,y)$ to discard trajectories that violate the
trust region. A standard (max-based) criterion is
\begin{equation}
    M(x,y)
    := \mathbb{I}\!\Big[\max_{t}\,\Dkltok(c_t;\piroll,\pitheta) \le \delta_{\mathrm{TRM}}\Big].
    \label{eq:trm-mask}
\end{equation}
Most existing methods in the critic-free LLM-RL setting share a common limitation: they either operate at token granularity (treating each
position independently) or at sequence granularity (collapsing all positions into a single decision), neither of which explicitly respects
the prefix-causal structure of autoregressive generation.

\begin{lemma}[Context Shift Propagation]
\label{lem:coupling}
Let $s$ be the \emph{first} position at which two policies $\pi$ and $\pi'$ differ: $\pi(\cdot|c_{s'}) = \pi'(\cdot|c_{s'})$ for all reachable $c_{s'}$ with $s' < s$, and $\Dtvtok(c_s) > 0$ for some context $c_s$ satisfying $d_s^{\pi}(c_s) > 0$. If for all subsequent positions $t > s$ the policies assign positive probability to all tokens reachable under either policy (i.e., $\pi(y_t|c_t) > 0 \iff \pi'(y_t|c_t) > 0$ for all reachable $c_t$), then for all $t > s$, the context visitation distributions satisfy $\|d_t^{\pi} - d_t^{\pi'}\|_{\mathrm{TV}} > 0$. That is, a token-level divergence at the first point of disagreement propagates to all future context distributions.
\end{lemma}

\begin{proof}
By definition, $d_t^{\pi}(c_t) = P(x)\prod_{s'=1}^{t-1}\pi(y_{s'}|c_{s'})$. We proceed by explicit construction.

\emph{Base case ($t = s+1$).} Since $\Dtvtok(c_s) > 0$, there exists a token $v^*$ such that $\pi(v^*|c_s) \neq \pi'(v^*|c_s)$. Consider the context $c_{s+1} = (c_s, v^*)$. By the product structure of the visitation distribution:
\begin{equation*}
    d_{s+1}^{\pi}(c_s, v^*) = d_s^{\pi}(c_s) \cdot \pi(v^*|c_s), \qquad d_{s+1}^{\pi'}(c_s, v^*) = d_s^{\pi}(c_s) \cdot \pi'(v^*|c_s),
\end{equation*}
where $d_s^{\pi}(c_s) = d_s^{\pi'}(c_s)$ holds because $s$ is the first position of disagreement, so $\pi(\cdot|c_{s'}) = \pi'(\cdot|c_{s'})$ for all reachable $c_{s'}$ with $s' < s$, and hence the two policies induce identical prefix distributions up to position $s$. Since $d_s^{\pi}(c_s) > 0$ and $\pi(v^*|c_s) \neq \pi'(v^*|c_s)$, we have $d_{s+1}^{\pi}(c_s, v^*) \neq d_{s+1}^{\pi'}(c_s, v^*)$, and therefore $\|d_{s+1}^{\pi} - d_{s+1}^{\pi'}\|_{\mathrm{TV}} > 0$.

\emph{Inductive step ($t > s+1$).} Assume that for some $t \ge s+1$ there exists a context $\tilde{c}_t$ such that $d_t^{\pi}(\tilde{c}_t) \neq d_t^{\pi'}(\tilde{c}_t)$. Consider its one-step extensions $\tilde{c}_{t+1} = (\tilde{c}_t, v)$ for tokens $v$ in the vocabulary. If $d_{t+1}^{\pi}(\tilde{c}_t, v) = d_{t+1}^{\pi'}(\tilde{c}_t, v)$ held for all $v$, then summing over $v$ would imply:
\begin{equation*}
    d_t^{\pi}(\tilde{c}_t) = \sum_{v} d_{t+1}^{\pi}(\tilde{c}_t, v) = \sum_{v} d_{t+1}^{\pi'}(\tilde{c}_t, v) = d_t^{\pi'}(\tilde{c}_t),
\end{equation*}
contradicting the induction hypothesis. Therefore there exists a token $v_t$ such that $d_{t+1}^{\pi}(\tilde{c}_t, v_t) \neq d_{t+1}^{\pi'}(\tilde{c}_t, v_t)$, and hence $\|d_{t+1}^{\pi} - d_{t+1}^{\pi'}\|_{\mathrm{TV}} > 0$.

Starting from the base case at $t=s+1$, this inductive step implies that for all $t > s$, there exists at least one context at which the visitation distributions differ. Quantitative bounds on context shift are discussed in \Cref{sec:adaptive-connection}.
\end{proof}
