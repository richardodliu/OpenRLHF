\section{Preliminaries}
\label{sec:preliminaries}

\subsection{Autoregressive Generation and Objective}

A (stochastic) policy $\pi$ generates a response $y = (y_1, \ldots, y_T)$ conditioned on a prompt $x$. The induced trajectory
probability admits the autoregressive factorization
\begin{equation}
    P^{\pi}(y \mid x) = \prod_{t=1}^T \pi(y_t \mid x, y_{<t}).
\end{equation}
For each position $t$, we define the context
\begin{equation}
    c_t := (x, y_{<t}) = (x, \ylt).
\end{equation}
The induced \emph{context visitation distribution} (the marginal distribution of length-$t$ prefixes) is
\begin{equation}
    d_t^{\pi}(c_t) = P(x)\, P^{\pi}(y_{<t} \mid x) = P(x) \prod_{s=1}^{t-1} \pi(y_s \mid c_s).
\end{equation}
Given a scalar reward $R(x, y)$, the objective of the current policy $\pitheta$ is
\begin{equation}
    J(\pitheta) = \E_{x \sim P(x),\, y \sim \pitheta(\cdot|x)}\big[R(x, y)\big].
\end{equation}

\subsection{Notation}

\textbf{Policies.} We consider:
\begin{itemize}
    \item $\piroll$: the \emph{behavior} (rollout) policy used to generate samples; it is fixed during each policy update;
    \item $\pitheta$: the \emph{training} policy being optimized by gradient steps.
\end{itemize}
This notation follows long-horizon trust-region analyses for LLM-RL (e.g., \citet{li2025trm}).

\textbf{Importance ratios.} The per-token importance ratio and its log-ratio are
\begin{align}
    \rho_t
    &:= \frac{\pitheta(y_t \mid c_t)}{\piroll(y_t \mid c_t)},
    \label{eq:ratio}
    \\
    \log\rho_t
    &= \log \pitheta(y_t \mid c_t) - \log \piroll(y_t \mid c_t).
    \label{eq:log-ratio}
\end{align}

\textbf{Rollout-time anchoring.} In practice, inference and training stacks can differ. We compute $\rho_t$ using rollout-time cached
log-probabilities $\log \piroll(y_t\mid c_t)$; see Appendix~\ref{app:mismatch} for practical mismatch modes and implementation mappings.

\subsection{Divergence Measures}

We use token-level divergence measures between conditional token distributions, following the trust-region literature for long-horizon
LLM-RL~\citep{li2025trm}. For two policies $\pi$ and $\pi'$, define the token-level total variation and KL divergences at context $c_t$:

\begin{definition}[Token-level divergences]
For a context $c_t = (x, y_{<t})$ and two policies $\pi$ and $\pi'$,
\begin{align}
    \Dtvtok(c_t;\pi,\pi')
    &:= \DTV\big(\pi(\cdot|c_t)\,\|\,\pi'(\cdot|c_t)\big)
    = \tfrac{1}{2}\sum_{v} \big|\pi(v|c_t) - \pi'(v|c_t)\big|, \\
    \Dkltok(c_t;\pi,\pi')
    &:= \DKL\big(\pi(\cdot|c_t)\,\|\,\pi'(\cdot|c_t)\big)
    = \sum_{v} \pi(v|c_t)\log\frac{\pi(v|c_t)}{\pi'(v|c_t)}.
\end{align}
When the policy pair is clear from context, we write $\Dtvtok(c_t)$ and $\Dkltok(c_t)$.
\end{definition}

\begin{definition}[Max token-level divergences and expected per-position TV]
\label{def:max-div}
For the policy pair $(\piroll,\pitheta)$, define:
\begin{align}
    \epsilon
    &:= \Dtvtokmax := \max_{t,\,c_t} \Dtvtok(c_t),
    \\
    \delta
    &:= \Dkltokmax := \max_{t,\,c_t} \Dkltok(c_t),
    \\
    \Dbar_t
    &:= \E_{c_t \sim d_t^{\piroll}}\!\big[\Dtvtok(c_t)\big].
\end{align}
\end{definition}

\subsection{Assumptions}
\label{sec:assumptions}

The following assumptions are used throughout the paper:

\begin{assumption}[Policy Support Overlap]
\label{asm:support}
For all contexts $c_t$ and tokens $y_t$ in the vocabulary, $\piroll(y_t|c_t) > 0 \implies \pitheta(y_t|c_t) > 0$. This ensures the
per-token importance ratio $\rho_t$ in \eqref{eq:ratio} is well-defined, and that KL terms with $\piroll$ in the first argument are finite.
\end{assumption}

\begin{assumption}[Bounded Rewards]
There exists $R_{\max} < \infty$ such that $|R(x, y)| \le R_{\max}$ for all prompt-response pairs $(x, y)$.
\end{assumption}

\begin{assumption}[Finite Sequence Length]
\label{asm:finite-length}
The response length satisfies $T < \infty$ for all generated sequences.
\end{assumption}

\subsection{The Surrogate Objective and Error Decomposition}

Samples are generated from a behavior policy $\piroll$ that generally differs from $\pitheta$. Following~\citet{kakade2002approximately},
the standard surrogate objective uses the per-step advantage under $\piroll$,
\begin{equation}
    A_t^{\piroll}(c_t, y_t)
    := \E_{\piroll}\!\big[R(x, y)\mid c_t, y_t\big] - \E_{\piroll}\!\big[R(x, y)\mid c_t\big].
\end{equation}
This yields the per-step surrogate
\begin{equation}
    \mathcal{L}_{\piroll}(\pitheta) = \sum_{t=1}^T \E_{c_t \sim d_t^{\piroll}}\!\Big[\E_{y_t \sim \piroll(\cdot|c_t)}\!\big[\rho_t \, A_t^{\piroll}(c_t, y_t)\big]\Big],
    \label{eq:surrogate}
\end{equation}
where $\rho_t$ is defined in \eqref{eq:ratio}.

\begin{definition}[Trajectory-level surrogate (implementation)]
\label{def:impl-surrogate}
Let $b$ be a baseline that is independent of the sampled trajectory $y$ (e.g., a prompt-level baseline computed from other samples in the
same prompt group). Define the trajectory-level advantage
\begin{equation}
    A := R(x, y) - b.
\end{equation}
The trajectory-level surrogate used in critic-free implementations is
\begin{equation}
    \tilde{\mathcal{L}}_{\piroll}(\pitheta) := \E_{\piroll}\!\Big[A \cdot \sum_{t=1}^T \rho_t\Big].
\end{equation}
\end{definition}

\begin{remark}[Per-step surrogate vs.\ implementation surrogate]
The per-step advantage $A_t^{\piroll}(c_t,y_t)$ depends on the full future trajectory conditioned on $(c_t,y_t)$, and is generally
intractable without a learned value function. The surrogate $\tilde{\mathcal{L}}_{\piroll}$ in \Cref{def:impl-surrogate} replaces the
future conditional advantage with a trajectory-wide scalar. Therefore $\tilde{\mathcal{L}}_{\piroll}$ should be viewed as an
\emph{approximation} rather than an equivalent reformulation of $\mathcal{L}_{\piroll}$ in \eqref{eq:surrogate}.
\end{remark}

\begin{remark}[On-policy unbiasedness vs.\ surrogate equivalence]
When $\piroll=\pitheta$ (on-policy) and $b$ is independent of the sampled trajectory, the REINFORCE identity gives
\begin{equation}
    \E_{\pitheta}\!\Big[A \cdot \sum_{t=1}^T \nabla_\theta \log \pitheta(y_t \mid c_t)\Big] = \nabla_\theta J(\pitheta),
\end{equation}
which provides an unbiased estimator of the policy gradient $\nabla_\theta J(\pitheta)$. This does not imply that the trajectory-level
surrogate $\tilde{\mathcal{L}}_{\piroll}$ is equivalent to the per-step surrogate $\mathcal{L}_{\piroll}$ in \eqref{eq:surrogate}.
Accordingly, the error decomposition below is stated in terms of $\mathcal{L}_{\piroll}$, which yields a clean advantage/context-shift
factorization, while the implemented loss uses $\tilde{\mathcal{L}}_{\piroll}$ as a practical approximation.
\end{remark}

The approximation error of the per-step surrogate is
\begin{equation}
    \mathrm{Error}(\pitheta) := J(\pitheta) - J(\piroll) - \mathcal{L}_{\piroll}(\pitheta).
\end{equation}
Via the Performance Difference Identity~\citep{kakade2002approximately}, this error decomposes as:
\begin{equation}
    |\mathrm{Error}(\pitheta)| \le \sum_{t=1}^T 2\|g_t\|_\infty \cdot \|d_t^{\pitheta} - d_t^{\piroll}\|_{\mathrm{TV}},
    \label{eq:error-decomp}
\end{equation}
where
\begin{equation}
    g_t(c_t) := \E_{y_t \sim \pitheta(\cdot\mid c_t)}\!\big[A_t^{\piroll}(c_t, y_t)\big]
\end{equation}
is the expected advantage shift. Classical bounds on \eqref{eq:error-decomp} scale as $O(T^2)$~\citep{kakade2002approximately,
achiam2017constrained}, rendering them theoretically vacuous for long-horizon LLM tasks with responses of thousands of tokens. Recent
work~\citep{li2025trm} derives tighter bounds,
including the Pinsker-Marginal bound ($O(T^{3/2})$), the Mixed bound ($O(T)$), and the data-dependent Adaptive bound.

The Adaptive bound is particularly relevant to our work:
\begin{equation}
    B_{\mathrm{Adap}}^{*} = 4 \sum_{t=1}^{T} \Dbar_t \cdot \min\!\Big(1,\;(T\!-\!t)\,\epsilon,\;\sqrt{\tfrac{(T-t)\,\delta}{2}}\Big),
\end{equation}
where $\Dbar_t$ is the expected per-position TV divergence, and $\epsilon,\delta$ are the max token-level divergences from
\Cref{def:max-div}. This bound operates at per-position granularity and preserves the causal structure of error accumulation, which
motivates our prefix-level Causal Trust Region masking component.

\subsection{Existing IS Correction Methods}
\label{sec:existing-is}

Several methods have been proposed to stabilize policy-gradient updates by constraining the per-token importance ratio $\rho_t$ between the
training policy $\pitheta$ and the behavior policy $\piroll$. We use the per-token log-ratio $\log\rho_t$ defined in
\eqref{eq:log-ratio}.
\par
\noindent\textbf{A unified view by granularity.} We adopt a trust-region masking viewpoint: when mismatch between $\piroll$ and
$\pitheta$ is large, one should attenuate or reject unreliable gradient contributions. Existing methods differ mainly in the
\emph{granularity} of this attenuation: token-level truncation/masking (TIS, IcePop), sequence-level masking/aggregation (TRM, GSPO), and
prefix-level (causal) filtering (our method in \Cref{sec:prefix-is}).
\par
\noindent\textbf{Interpretation.} Under this view, TIS and IcePop can be seen as token-level analogues of trust-region masking, while GSPO
and TRM apply sequence-level decisions (a single ratio statistic or a binary rejection gate) shared across all positions.
\par
\noindent\textbf{Prefix-level Causal Trust Region masking.} Our prefix mask $M_t^{\mathrm{pre}}$ implements a causal, per-position gate
constructed from prefix ratio statistics. Compared with TRM's sequence-level gate $M^{\mathrm{TRM}}(x,y)$, it preserves positional granularity by masking
only the token-level loss terms whose prefixes violate the threshold.

\noindent Table~\ref{tab:is-methods} summarizes these stabilization mechanisms under a unified notation.
\begin{table}[ht]
\centering
\small
\caption{Existing trust-region and IS stabilization methods in critic-free LLM-RL.}
\label{tab:is-methods}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{llll}
\toprule
\textbf{Method} & \textbf{IS Statistic} & \textbf{Truncation} & \textbf{Granularity} \\
\midrule
TIS & $\rho_t$ & $w_t^{\mathrm{tok}}=\mathrm{clip}(\rho_t,1-\epslow,1+\epshigh)$ & Token \\
IcePop & $\rho_t$ & $M_t^{\mathrm{tok}}=\mathbb{I}[\rho_t \in [1-\epslow,1+\epshigh]]$ & Token \\
TRM & $\rho_t$ & $M^{\mathrm{TRM}}(x,y)=\mathbb{I}\!\big[\max_t\,\Dkltok(c_t;\piroll,\pitheta)\le \delta_{\mathrm{TRM}}\big]$ & Sequence \\
GSPO & $\rhoseq$ & $w^{\mathrm{seq}}=\mathrm{clip}(\rhoseq,1-\epslow,1+\epshigh)$ & Sequence \\
\bottomrule
\end{tabular}
\end{table}

\noindent Here $\rhoseq$ is the length-normalized sequence likelihood ratio (a geometric mean of token ratios),
\begin{equation}
    \begin{aligned}
        \rhoseq
        &:= \left(\frac{P^{\pitheta}(y \mid x)}{P^{\piroll}(y \mid x)}\right)^{\frac{1}{T}}
        \\
        &= \left(\prod_{t=1}^{T}\rho_t\right)^{\frac{1}{T}}
        = \exp\!\left(\frac{1}{T}\sum_{t=1}^{T}\log\rho_t\right),
    \end{aligned}
\end{equation}
where $T=|y|$ is the response length. This definition matches the sequence-level ratio used in GSPO~\citep{gspo2025} (denoted $s_i(\theta)$
in the GSPO paper) up to notation.

TRM (Trust Region Masking, \citet{li2025trm}) uses a binary \emph{sequence-level} mask $M^{\mathrm{TRM}}(x,y)$ to discard trajectories that violate the
trust region. A standard (max-based) criterion is
\begin{equation}
    M^{\mathrm{TRM}}(x,y)
    := \mathbb{I}\!\Big[\max_{t}\,\Dkltok(c_t;\piroll,\pitheta) \le \delta_{\mathrm{TRM}}\Big].
\end{equation}
Most existing methods in the critic-free LLM-RL setting share a common limitation: they either operate at token granularity (treating each
position independently) or at sequence granularity (collapsing all positions into a single decision), neither of which explicitly respects
the prefix-causal structure of autoregressive generation.

\begin{remark}[Context Shift Propagation]
\label{lem:coupling}
Let $s$ be the \emph{first} position at which two policies $\pi$ and $\pi'$ differ: $\pi(\cdot|c_{s'}) = \pi'(\cdot|c_{s'})$ for all reachable $c_{s'}$ with $s' < s$, and $\Dtvtok(c_s) > 0$ for some context $c_s$ satisfying $d_s^{\pi}(c_s) > 0$. If for all subsequent positions $t > s$ the policies assign positive probability to all tokens reachable under either policy (i.e., $\pi(y_t|c_t) > 0 \iff \pi'(y_t|c_t) > 0$ for all reachable $c_t$), then for all $t > s$, the context visitation distributions satisfy $\|d_t^{\pi} - d_t^{\pi'}\|_{\mathrm{TV}} > 0$. That is, a token-level divergence at the first point of disagreement propagates to all future context distributions.
This propagation property is standard in autoregressive generation; see, e.g., \citet{li2025trm}.
\end{remark}
