\section{Preliminaries}
\label{sec:preliminaries}

\subsection{Autoregressive Generation and Objective}

A (stochastic) policy $\pi$ generates a response $y = (y_1, \ldots, y_T)$ given a prompt $x$, with trajectory probability
\begin{equation}
    P^{\pi}(y \mid x) = \prod_{t=1}^T \pi(y_t \mid x, y_{<t}).
\end{equation}
We define the context at position $t$ as $c_t = (x, y_{<t})$ and the induced context visitation distribution (the marginal distribution over length-$t$ prefixes):
\begin{equation}
    d_t^{\pi}(c_t) = P(x)\, P^{\pi}(y_{<t} \mid x) = P(x) \prod_{s=1}^{t-1} \pi(y_s \mid c_s).
\end{equation}
Given a scalar reward $R(x, y)$, the objective of the current policy $\pitheta$ is
\begin{equation}
    J(\pitheta) = \E_{x \sim P(x),\, y \sim \pitheta(\cdot|x)}[R(x, y)].
\end{equation}

\subsection{Notation}
\label{sec:notation}

\textbf{Three policies.} Modern LLM-RL pipelines involve three distinct policies:
\begin{itemize}
    \item $\piroll$: the rollout policy used by the inference engine to generate samples;
    \item $\piold$: the actor policy at the start of each training step (the ``old'' policy in PPO);
    \item $\pitheta$: the current actor policy being updated during gradient steps.
\end{itemize}

\textbf{Two importance ratios.} We distinguish two ratios that arise in practice:
\begin{itemize}
    \item \emph{PPO ratio} (carries gradients): $\rho_t^{\mathrm{PPO}} = \pitheta(y_t|c_t)/\piold(y_t|c_t)$;
    \item \emph{rollout mismatch ratio} (detached coefficient): $w_t = \piold(y_t|c_t)/\piroll(y_t|c_t)$, with log-ratio $\ell_t = \log w_t$.
\end{itemize}

\textbf{Action mask.} We use an action mask $m_t \in \{0,1\}$ to select active action-token positions (excluding padding, and excluding
prompt-only positions if a unified tokenization is used). When we index only over action tokens without padding, one can take $m_t \equiv 1$.
Sums such as $\sum_t f_t m_t$ should be read as summing over active positions.

\subsection{Assumptions}
\label{sec:assumptions}

The following assumptions are used throughout the paper:

\begin{assumption}[Policy Support Overlap]
\label{asm:support}
For all contexts $c_t$ and tokens $y_t$ in the vocabulary, $\piroll(y_t|c_t) > 0 \implies \piold(y_t|c_t) > 0$. This ensures that the importance sampling ratio $w_t = \piold(y_t|c_t)/\piroll(y_t|c_t)$ is well-defined.
\end{assumption}

\begin{assumption}[Bounded Rewards]
\label{asm:bounded-reward}
There exists $R_{\max} < \infty$ such that $|R(x, y)| \le R_{\max}$ for all prompt-response pairs $(x, y)$.
\end{assumption}

\begin{assumption}[Finite Sequence Length]
\label{asm:finite-length}
The response length satisfies $T < \infty$ for all generated sequences.
\end{assumption}

\subsection{The Surrogate Objective and Error Decomposition}

Samples are generated from a rollout policy $\piroll$ that generally differs from $\pitheta$. Following~\citet{kakade2002approximately}, the standard surrogate objective uses per-step advantages $A_t^{\piroll}(c_t, y_t) := \E_{\piroll}[R \mid c_t, y_t] - \E_{\piroll}[R \mid c_t]$:
\begin{equation}
    L_{\piroll}(\pitheta) = \sum_{t=1}^T \E_{c_t \sim d_t^{\piroll}}\!\Big[\E_{y_t \sim \piroll(\cdot|c_t)}\!\big[\rho_t \, A_t^{\piroll}(c_t, y_t)\big]\Big],
    \label{eq:surrogate}
\end{equation}
where $\rho_t = \pitheta(y_t \mid c_t) / \piroll(y_t \mid c_t)$ is the per-token importance ratio.

\textbf{From per-step to trajectory-level advantage (implementation surrogate).} The per-step advantage $A_t^{\piroll}(c_t, y_t) = \E_{\piroll}[R \mid c_t, y_t] - \E_{\piroll}[R \mid c_t]$ depends on the full future trajectory conditioned on $(c_t, y_t)$ and is generally intractable without a learned value function. In the critic-free setting, implementations therefore use a trajectory-level advantage $A = R(x,y) - b$, where the baseline $b$ is independent of the \emph{current} sampled trajectory (e.g., a prompt-level baseline computed from other samples). Using this scalar as a per-token weight yields a practical surrogate objective:
\begin{equation}
    \tilde{L}_{\piroll}(\pitheta) := \E_{\piroll}\!\Big[A \cdot \sum_{t=1}^T \rho_t\Big].
\end{equation}
This substitutes the future-conditional $A_t^{\piroll}(c_t, y_t)$ in Eq.~\eqref{eq:surrogate} with a trajectory-wide scalar, and should be viewed as an approximation rather than an equivalent reformulation of the PDI surrogate.

\textbf{Gradient estimator unbiasedness (on-policy).} It is important to distinguish two properties. When $\piroll = \pitheta$ (on-policy) and $b$ is independent of the sampled trajectory, the REINFORCE identity guarantees:
\begin{equation}
    \E_{\piroll}\!\Big[A \cdot \sum_{t=1}^T \nabla_\theta \log \pitheta(y_t|c_t)\Big] = \nabla_\theta J(\pitheta),
\end{equation}
providing an unbiased estimator of the \emph{policy gradient} $\nabla J(\pitheta)$. This does not imply that the trajectory-level surrogate $\tilde{L}_{\piroll}(\pitheta)$ is equivalent to the per-step surrogate $L_{\piroll}(\pitheta)$ in Eq.~\eqref{eq:surrogate}. For this reason, the error decomposition below is stated in terms of the per-step surrogate (which yields a clean advantage/context-shift factorization) and serves as theoretical motivation for off-policy correction, even though the implemented loss uses the trajectory-level approximation. The approximation error is:
\begin{equation}
    \mathrm{Error}(\pitheta) := J(\pitheta) - J(\piroll) - L_{\piroll}(\pitheta).
    \label{eq:error-def}
\end{equation}
Via the Performance Difference Identity~\citep{kakade2002approximately}, this error decomposes as:
\begin{equation}
    |\mathrm{Error}| \le \sum_{t=1}^T 2\|g_t\|_\infty \cdot \|d_t^{\pitheta} - d_t^{\piroll}\|_{\mathrm{TV}},
    \label{eq:error-decomp}
\end{equation}
where $g_t(c_t) = \E_{y_t \sim \pitheta}[A_t^{\piroll}(c_t, y_t)]$ is the expected advantage shift, with the per-step advantage defined as $A_t^{\piroll}(c_t, y_t) := \E_{\piroll}[R \mid c_t, y_t] - \E_{\piroll}[R \mid c_t]$. Classical bounds on this error scale as $O(T^2)$~\citep{kakade2002approximately, achiam2017constrained}, which is vacuous for long-horizon LLM tasks ($T > 4000$). Recent work~\citep{li2025trm} derives tighter bounds: the Pinsker-Marginal bound scales as $O(T^{3/2})$ using a global TV divergence; the Mixed bound achieves $O(T)$ by combining TV and KL terms; and the Adaptive bound is data-dependent, operating at per-position granularity. All three depend on the maximum token-level divergence $\Dkltokmax$, but their dependency forms differ: the Pinsker-Marginal bound uses a global aggregation, the Mixed bound combines TV and KL terms with different weighting, and the Adaptive bound uses per-position $\Dbar_t$ with a position-dependent minimum over three terms.

The Adaptive bound is particularly relevant to our work:
\begin{equation}
    B_{\mathrm{Adap}}^{*} = 4 \sum_{t=1}^{T} \Dbar_t \cdot \min\!\Big(1,\;(T\!-\!t)\,\epsilon,\;\sqrt{\tfrac{(T-t)\,\delta}{2}}\Big),
    \label{eq:adaptive-bound}
\end{equation}
where $\Dbar_t = \E_{c_t \sim d_t^{\piroll}}[\Dtvtok(c_t)]$ is the expected per-position TV divergence, $\epsilon = \Dtvtokmax$, and $\delta = \Dkltokmax$. This bound operates at per-position granularity and preserves the causal structure of the error accumulation---properties that motivate our prefix cumulative IS correction.

\subsection{Existing IS Correction Methods}
\label{sec:existing-is}

Several methods have been proposed to correct the $\piroll \neq \piold$ mismatch. Let $\ell_t = \log \piold(y_t|c_t) - \log \piroll(y_t|c_t)$ denote the per-token log IS ratio.

\begin{table}[h]
\centering
\caption{Existing IS correction methods for off-policy mismatch in LLM-RL.}
\label{tab:existing-is}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{llll}
\toprule
\textbf{Method} & \textbf{IS Weight} & \textbf{Granularity} & \textbf{Causal} \\
\midrule
TIS & $\exp(\ell_t)$ clamped to $[\lambda, \Lambda]$ & Token & No \\
ICEPOP & $\exp(\ell_t) \cdot \mathbb{I}[\exp(\ell_t) \in [\lambda, \Lambda]]$ & Token & No \\
seq-mask-tis & $\mathbb{I}[\exp(\bar{\ell}) \in [\lambda, \Lambda]] \cdot \exp(\ell_t)$ & Sequence & No \\
GSPO & $\exp\!\big(\sum_t \ell_t m_t / \sum_t m_t\big)$ & Sequence & No \\
\bottomrule
\end{tabular}
\end{table}

\noindent Here $\bar{\ell} = \sum_t \ell_t m_t / \sum_t m_t$ is the sequence-level geometric mean log-ratio and $m_t$ is the action mask. Most existing methods in the critic-free LLM-RL setting share a common limitation: they either operate at the token level (ignoring that early deviations affect the entire future trajectory) or at the sequence level (losing the ability to distinguish where divergence occurs within a sequence).

\begin{lemma}[Context Shift Propagation]
\label{lem:coupling}
Let $s$ be the \emph{first} position at which two policies $\pi$ and $\pi'$ differ: $\pi(\cdot|c_{s'}) = \pi'(\cdot|c_{s'})$ for all reachable $c_{s'}$ with $s' < s$, and $\Dtvtok(c_s) > 0$ for some context $c_s$ satisfying $d_s^{\pi}(c_s) > 0$. If for all subsequent positions $t > s$ the policies assign positive probability to all tokens reachable under either policy (i.e., $\pi(y_t|c_t) > 0 \iff \pi'(y_t|c_t) > 0$ for all reachable $c_t$), then for all $t > s$, the context visitation distributions satisfy $\|d_t^{\pi} - d_t^{\pi'}\|_{\mathrm{TV}} > 0$. That is, a token-level divergence at the first point of disagreement propagates to all future context distributions.
\end{lemma}

\begin{proof}
By definition, $d_t^{\pi}(c_t) = P(x)\prod_{s'=1}^{t-1}\pi(y_{s'}|c_{s'})$. We proceed by explicit construction.

\emph{Base case ($t = s+1$).} Since $\Dtvtok(c_s) > 0$, there exists a token $v^*$ such that $\pi(v^*|c_s) \neq \pi'(v^*|c_s)$. Consider the context $c_{s+1} = (c_s, v^*)$. By the product structure of the visitation distribution:
\begin{equation*}
    d_{s+1}^{\pi}(c_s, v^*) = d_s^{\pi}(c_s) \cdot \pi(v^*|c_s), \qquad d_{s+1}^{\pi'}(c_s, v^*) = d_s^{\pi}(c_s) \cdot \pi'(v^*|c_s),
\end{equation*}
where $d_s^{\pi}(c_s) = d_s^{\pi'}(c_s)$ holds because $s$ is the first position of disagreement, so $\pi(\cdot|c_{s'}) = \pi'(\cdot|c_{s'})$ for all reachable $c_{s'}$ with $s' < s$, and hence the two policies induce identical prefix distributions up to position $s$. Since $d_s^{\pi}(c_s) > 0$ and $\pi(v^*|c_s) \neq \pi'(v^*|c_s)$, we have $d_{s+1}^{\pi}(c_s, v^*) \neq d_{s+1}^{\pi'}(c_s, v^*)$, and therefore $\|d_{s+1}^{\pi} - d_{s+1}^{\pi'}\|_{\mathrm{TV}} > 0$.

\emph{Inductive step ($t > s+1$).} Assume that for some $t \ge s+1$ there exists a context $\tilde{c}_t$ such that $d_t^{\pi}(\tilde{c}_t) \neq d_t^{\pi'}(\tilde{c}_t)$. Consider its one-step extensions $\tilde{c}_{t+1} = (\tilde{c}_t, v)$ for tokens $v$ in the vocabulary. If $d_{t+1}^{\pi}(\tilde{c}_t, v) = d_{t+1}^{\pi'}(\tilde{c}_t, v)$ held for all $v$, then summing over $v$ would imply:
\begin{equation*}
    d_t^{\pi}(\tilde{c}_t) = \sum_{v} d_{t+1}^{\pi}(\tilde{c}_t, v) = \sum_{v} d_{t+1}^{\pi'}(\tilde{c}_t, v) = d_t^{\pi'}(\tilde{c}_t),
\end{equation*}
contradicting the induction hypothesis. Therefore there exists a token $v_t$ such that $d_{t+1}^{\pi}(\tilde{c}_t, v_t) \neq d_{t+1}^{\pi'}(\tilde{c}_t, v_t)$, and hence $\|d_{t+1}^{\pi} - d_{t+1}^{\pi'}\|_{\mathrm{TV}} > 0$.

Starting from the base case at $t=s+1$, this inductive step implies that for all $t > s$, there exists at least one context at which the visitation distributions differ. Quantitative bounds on context shift are discussed in \Cref{sec:adaptive-connection}.
\end{proof}
