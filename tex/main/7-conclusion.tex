\section{Conclusion}
\label{sec:conclusion}

We presented REINFORCE Pro Max, a unified framework for critic-free LLM reinforcement learning that addresses two fundamental challenges:
high-variance advantage estimation and causal off-policy mismatch between behavior and training policies.

The first component, REINFORCE Max, combines a leave-one-out baseline (statistically independent of each sample, enabling clean
second-moment analysis, \Cref{prop:rloo-variance}) with adaptive asymmetric normalization (preserving advantage sign while enforcing the
empirical constraints in \Cref{def:adaptive-norm}, \Cref{prop:alpha-beta}). The second component, REINFORCE Pro, introduces
prefix-level Causal Trust Region masking (a ratio-based prefix proxy) that preserves the causal structure of autoregressive generation. We showed that the
resulting prefix mask can provide tighter filtering than both token-level and sequence-level alternatives in the analyzed deviation
patterns (\Cref{thm:prefix-tighter}), subject to the conditions stated therein. The connection to the Adaptive bound from the trust region
framework highlights that prefix masking acts as a sample-level proxy for per-position Causal Trust Region filtering: the KL chain rule yields an
expectation identity for the cumulative log-ratio (\Cref{lem:cumsum-kl}), while the prefix thresholding rule enforces a per-position
sample constraint (\Cref{rem:prefix-proxy}).

\textbf{Limitations and future work.} The prefix-mask thresholds $[\lambda, \Lambda]$ are hyperparameters that require tuning. The
sample-level prefix constraint is only a proxy for a population-level trust-region condition, and the gap between the two is not formally
characterized. The theoretical analysis assumes bounded rewards and overlapping policy supports (\Cref{sec:assumptions}), which may not
hold in all practical settings. Empirical validation on large-scale benchmarks is ongoing; we plan to evaluate on mathematical reasoning
tasks (AIME, MATH) with ablations on each component.
