\section{REINFORCE Max: Variance-Reduced Advantage Estimation}
\label{sec:reinforce-max}

\subsection{Token-Level Expansion}
\label{sec:token-expand}

Given a prompt $x$, we sample $n$ responses $\{y^{(1)}, \ldots, y^{(n)}\}$ with rewards $\{r_1, \ldots, r_n\}$. Each response
$y^{(i)}$ consists of $T_i$ action tokens. REINFORCE Max combines three ingredients: a leave-one-out baseline
(\Cref{eq:rloo-baseline}), token-level expansion (\Cref{sec:token-expand}), and adaptive normalization (\Cref{sec:adaptive-norm}).

We adopt the leave-one-out (RLOO) baseline~\citep{ahmadian2024back} for its statistical independence, which avoids correlated baseline artifacts present in the mean baseline (see \Cref{app:rloo-proof} for a detailed analysis).

\begin{definition}[Leave-One-Out Baseline]
\label{def:rloo}
Assume $n \ge 2$. For sample $i$, the baseline and shaped reward are:
\begin{equation}
    b_i = \frac{1}{n-1}\sum_{j \neq i} r_j, \qquad \tilde{r}_i = r_i - b_i.
    \label{eq:rloo-baseline}
\end{equation}
\end{definition}

Since the reward is a scalar per response and we use $\gamma = 1.0$ (see \Cref{app:gamma-one} for a formal justification), the token-level advantage is simply the shaped reward itself:
\begin{equation}
    A_{i,t} = \tilde{r}_i = r_i - b_i, \quad \forall\, t \in \{1, \ldots, T_i\}.
    \label{eq:token-advantage}
\end{equation}

\subsection{Adaptive Asymmetric Normalization}
\label{sec:adaptive-norm}

Standard normalization $\hat{A} = (A - \mu)/\sigma$ applies a uniform scaling to all advantages. In reasoning tasks, the advantage distribution is typically highly skewed: most samples fail (negative advantages) while few succeed (positive advantages). Uniform scaling distorts the relative balance between reinforcement and punishment signals.

\begin{definition}[Adaptive Normalization]
\label{def:adaptive-norm}
Fix a prompt group with token-level advantages $\{A_{i,t}\}$, and exclude zero-advantage positions from the normalization statistics.
Define the index sets and summary statistics
\begin{align}
    \mathcal{P}
    &:= \{(i,t) : A_{i,t} > 0\},
    &
    \mathcal{N}
    &:= \{(i,t) : A_{i,t} < 0\},
    \\
    S^+
    &:= \sum_{(i,t) \in \mathcal{P}} A_{i,t},
    &
    S^-
    &:= \sum_{(i,t) \in \mathcal{N}} A_{i,t},
    \\
    Q^+
    &:= \sum_{(i,t) \in \mathcal{P}} A_{i,t}^2,
    &
    Q^-
    &:= \sum_{(i,t) \in \mathcal{N}} A_{i,t}^2,
    \\
    N
    &:= |\mathcal{P}| + |\mathcal{N}|.
\end{align}
The normalized advantage is defined by a sign-dependent rescaling:
\begin{equation}
    \hat{A}_{i,t} = \begin{cases} \alpha \cdot A_{i,t} & \text{if } A_{i,t} > 0, \\ \beta \cdot A_{i,t} & \text{if } A_{i,t} < 0, \\ 0 & \text{if } A_{i,t} = 0, \end{cases}
    \label{eq:adaptive-norm}
\end{equation}
where $\alpha,\beta>0$ are chosen to satisfy the following \emph{empirical} constraints over non-zero tokens:
\begin{align}
    \frac{1}{N}\sum_{(i,t)\in\mathcal{P}\cup\mathcal{N}}\hat{A}_{i,t} &= 0,
    \label{eq:adaptive-mean}
    \\
    \frac{1}{N}\sum_{(i,t)\in\mathcal{P}\cup\mathcal{N}}\hat{A}_{i,t}^2 &= 1.
    \label{eq:adaptive-var}
\end{align}
Equivalently, these constraints can be written as
\begin{equation}
    \alpha S^+ + \beta S^- = 0,
    \qquad
    \alpha^2 Q^+ + \beta^2 Q^- = N.
\end{equation}
\end{definition}

\begin{proposition}[Closed-Form Solution]
\label{prop:alpha-beta}
Assume $\mathcal{P} \neq \emptyset$ and $\mathcal{N} \neq \emptyset$. The unique solution satisfying $\alpha S^+ + \beta S^- = 0$ and $\alpha^2 Q^+ + \beta^2 Q^- = N$ is:
\begin{equation}
    \alpha = \sqrt{\frac{N}{Q^+ + \left(\frac{S^+}{S^-}\right)^{\!2} Q^-}}, \qquad \beta = -\alpha \cdot \frac{S^+}{S^-}.
    \label{eq:alpha-beta}
\end{equation}
\end{proposition}

\begin{proof}
We solve the two scalar constraints in \Cref{def:adaptive-norm} and then enforce $\alpha,\beta>0$.

\textbf{Step 1 (zero empirical mean).} The constraint $\alpha S^+ + \beta S^- = 0$ implies
\begin{equation}
    \beta = -\alpha \cdot \frac{S^+}{S^-}.
    \label{eq:beta-from-mean}
\end{equation}
Since $\mathcal{P}\neq\emptyset$ and $\mathcal{N}\neq\emptyset$, we have $S^+>0$ and $S^-<0$, so $S^-/S^+$ is well-defined.

\textbf{Step 2 (unit empirical second moment).} Substituting \eqref{eq:beta-from-mean} into $\alpha^2 Q^+ + \beta^2 Q^- = N$ yields
\begin{align}
    \alpha^2 Q^+ + \beta^2 Q^-
    &= N,
    \\
    \alpha^2 Q^+ + \alpha^2\Big(\frac{S^+}{S^-}\Big)^{\!2} Q^-
    &= N,
    \\
    \alpha^2
    &= \frac{N}{Q^+ + \left(\frac{S^+}{S^-}\right)^{2} Q^-}.
\end{align}
The denominator is strictly positive because $Q^+>0$, $Q^->0$, and $N>0$ under the same assumption. Taking the positive square root
gives the stated $\alpha>0$, and then \eqref{eq:beta-from-mean} yields the stated $\beta$.

\textbf{Uniqueness.} The mean constraint uniquely specifies the ratio $\beta/\alpha$, and the second-moment constraint uniquely
specifies $\alpha^2$. Enforcing $\alpha>0$ and $\beta>0$ therefore yields a unique solution.
\end{proof}

\paragraph{Advantage sign preservation.}
\label{prop:gradient-direction}
The adaptive normalization preserves the sign of the advantage at every token: $\sign(\hat{A}_{i,t}) = \sign(A_{i,t})$ for all $(i,t)$
with $A_{i,t} \neq 0$. Moreover, the relative ordering of magnitudes is preserved within the positive group $\mathcal{P}$ and within the
negative group $\mathcal{N}$. This follows immediately because the normalization scales $A_{i,t}$ by $\alpha>0$ on $\mathcal{P}$ and by
$\beta>0$ on $\mathcal{N}$ (\Cref{prop:alpha-beta}), and multiplication by a positive constant preserves both sign and ordering.

\begin{remark}[Connection to asymmetric clipping]
Adaptive normalization is a soft analog of PPO's asymmetric clipping. PPO clips the importance ratio differently depending on the advantage sign; adaptive normalization scales the advantage magnitude differently. Both control the effective step size asymmetrically, but normalization acts through the advantage rather than the ratio, avoiding the gradient leakage problem identified by~\citet{li2025trm}.
\end{remark}

\begin{remark}[Uniform Scale Mode]
When all $n$ samples share the same reward, the leave-one-out baseline yields zero advantages, eliminating gradient signal. An optional \emph{uniform scale} mode recovers this signal by setting $\tilde{r}_i = r_i/n$ and skipping normalization, improving sample utilization. See \Cref{app:uniform-scale} for details.
\end{remark}


\section{REINFORCE Pro: Prefix Causal Trust Region Masking}
\label{sec:reinforce-pro}

\subsection{Off-Policy Ratio Setup}
\label{sec:offpolicy-problem}

In PPO-style LLM-RL, samples are generated by a fixed rollout policy $\piroll$, and gradients are computed under a training policy
$\pitheta$ that is updated by gradient steps. This matches the trust-region setup in~\citet{li2025trm}.

The per-token importance ratio between these two policies is
\begin{equation}
    \rho_t := \frac{\pitheta(y_t \mid c_t)}{\piroll(y_t \mid c_t)},
\end{equation}
as defined in \eqref{eq:ratio}. Throughout this section, we restrict attention to masking strategies constructed solely from
$(\rho_t)_{t=1}^T$ (equivalently, the log-ratios $(\log\rho_t)_{t=1}^T$ as in \eqref{eq:log-ratio}), and we do not introduce additional
intermediate policies.

\paragraph{A Causal Trust Region viewpoint.}
Trust Region Masking (TRM; \citet{li2025trm}) introduces a \emph{sequence-level} rejection mechanism via a binary mask
$M(x,y)$, yielding a masked surrogate objective of the form
\begin{equation}
    \mathcal{L}_{\mathrm{masked}}^{\mathrm{TRM}}(\pitheta)
    := \E_{\piroll}\!\left[
        M(x,y)\cdot A(x,y)\cdot \sum_{t=1}^{T}\rho_t
    \right].
    \label{eq:trm-masked-surrogate}
\end{equation}
Motivated by the same trust-region perspective for long-horizon generation, we introduce a prefix-level, per-position gate
$M_t^{\mathrm{prefix}}$ computed from a ratio-based \emph{prefix} statistic and multiply it into the token-level loss. Unlike TRM, which
accepts or rejects an entire trajectory via a single gate $M(x,y)$, the prefix gate can mask only the positions whose prefixes violate the
threshold, thereby retaining positional granularity while respecting the autoregressive order. For comparison, a prefix-level masked
surrogate can be written as
\begin{equation}
    \mathcal{L}_{\mathrm{masked}}^{\mathrm{prefix}}(\pitheta)
    := \E_{\piroll}\!\left[
        A(x,y)\cdot \sum_{t=1}^{T} M_t^{\mathrm{prefix}}\rho_t
    \right],
    \label{eq:prefix-masked-surrogate}
\end{equation}
where the gate is applied at token granularity. Our implementation uses a PPO-style clipped surrogate with normalized advantages; see
\Cref{sec:combined-loss} for the exact objective. Importantly, our prefix criterion is a tractable sample-level \emph{proxy} for Causal
Trust Region filtering and is not the sequence-level max-divergence criterion used in TRM (based on $\max_t \DKL(c_t)$).

\subsection{Limitations of Existing Methods}
\label{sec:is-limitations}

\begin{proposition}[Token-Level IS Ignores Causal Structure]
\label{prop:token-is-fails}
Fix thresholds $0<\lambda\le 1\le \Lambda$. Consider a length-$T$ trajectory such that
\begin{equation}
    \rho_1 \notin [\lambda,\Lambda],
    \qquad
    \rho_t \in [\lambda,\Lambda]\ \text{ for all }\ t\in\{2,\ldots,T\}.
    \label{eq:token-early-deviation}
\end{equation}
Then the token-level ICEPOP mask,
\begin{equation}
    M_t^{\mathrm{token}} := \mathbb{I}\!\big[\rho_t \in [\lambda,\Lambda]\big],
\end{equation}
masks only the first position. In particular, it does not prevent later positions $t\ge 2$ from contributing gradients even though the
contexts $(c_t)_{t\ge2}$ are sampled under the behavior distribution induced by $\piroll$, which differs from the context distribution that
would be induced by the updated policy $\pitheta$ when $\rho_1 \neq 1$.
\end{proposition}

\begin{proof}
By definition, \eqref{eq:token-early-deviation} implies $M_1^{\mathrm{token}}=0$ and $M_t^{\mathrm{token}}=1$ for all $t\ge2$.

On the other hand, since $\rho_1 \neq 1$, we have $\pitheta(y_1\mid c_1) \neq \piroll(y_1\mid c_1)$, hence the conditional token
distributions $\pitheta(\cdot\mid c_1)$ and $\piroll(\cdot\mid c_1)$ are not equal. Therefore $\Dtvtok(c_1;\piroll,\pitheta)>0$.
Applying the context-shift propagation lemma (\Cref{lem:coupling}) with $(\pi,\pi')=(\piroll,\pitheta)$ and $s=1$ yields
\begin{equation}
    \|d_t^{\piroll} - d_t^{\pitheta}\|_{\mathrm{TV}} > 0
    \qquad
    \text{for all } t \ge 2.
\end{equation}
Therefore, even though ICEPOP masks the local deviation at $t=1$, the remaining accepted tokens $t\ge2$ are still evaluated under
mismatched context visitation distributions, which is precisely the causal mismatch that token-independent masking fails to address.
\end{proof}

\begin{proposition}[Sequence-Level IS Loses Positional Information]
\label{prop:seq-is-fails}
Let $\rhoseq$ denote the geometric-mean sequence likelihood ratio from \Cref{sec:existing-is}. Then its log is the sequence-average
log-ratio,
\begin{equation}
    \log\rhoseq = \frac{1}{T}\sum_{t=1}^T \log\rho_t.
\end{equation}
Assume there exists $t^*\in\{1,\ldots,T\}$ such that $|\log\rho_t|\le \varepsilon$ for all $t\neq t^*$. Then
\begin{equation}
    \Big|\log\rhoseq - \frac{1}{T}\log\rho_{t^*}\Big| \le \frac{T-1}{T}\,\varepsilon \le \varepsilon.
    \label{eq:seq-mean-dilution}
\end{equation}
In particular, in the pure single-spike case $\log\rho_t=0$ for $t\neq t^*$, one has $\log\rhoseq=\log\rho_{t^*}/T$, so
$\rhoseq = (\rho_{t^*})^{1/T}\to 1$ as $T\to\infty$ for any fixed $\rho_{t^*}>0$.
\end{proposition}

\begin{proof}
Write
\begin{equation}
    \log\rhoseq
    = \frac{1}{T}\log\rho_{t^*} + \frac{1}{T}\sum_{t\neq t^*}\log\rho_t.
\end{equation}
By the triangle inequality and the bound $|\log\rho_t|\le\varepsilon$ for $t\neq t^*$,
\begin{equation}
    \big|\log\rhoseq - \log\rho_{t^*}/T\big|
    = \frac{1}{T}\Big|\sum_{t\neq t^*}\log\rho_t\Big|
    \le \frac{T-1}{T}\,\varepsilon,
\end{equation}
which implies \eqref{eq:seq-mean-dilution}. The single-spike specialization follows by setting $\log\rho_t=0$ for $t\neq t^*$.
\end{proof}

\begin{example}[Numerical dilution at long horizons]
Consider the pure single-spike case with $T=4096$, $\log\rho_{t^*}=3$, and $\log\rho_t=0$ for $t\neq t^*$. Then
\begin{equation}
    \rhoseq = \exp(3/4096) \approx 1.00073,
\end{equation}
which can easily fall inside practical acceptance intervals $[\lambda,\Lambda]$ even though the local deviation at $t^*$ is large in
log-ratio magnitude.
\end{example}

\subsection{Prefix Causal Trust Region Proxy: Definition and Properties}
\label{sec:prefix-is}

\begin{definition}[Prefix Cumulative Ratio Proxy]
\label{def:prefix-is}
For a response trajectory $y=(y_1,\ldots,y_T)$ with per-token log-ratios $\log\rho_t$ defined in \eqref{eq:log-ratio}, define the prefix
cumulative ratio statistic
\begin{equation}
    \rhoprefix{t}
    := \exp\!\left(\frac{1}{t}\sum_{s=1}^{t} \log\rho_s\right)
    = \left(\prod_{s=1}^{t}\rho_s\right)^{\frac{1}{t}},
    \qquad
    t\in\{1,\ldots,T\}.
    \label{eq:prefix-is}
\end{equation}
The statistic $\rhoprefix{t}$ is the geometric mean of per-token importance ratios over the length-$t$ prefix.
\end{definition}

\begin{definition}[Prefix Causal Trust Region Mask]
\label{def:prefix-trm-proxy}
Given thresholds $[\lambda, \Lambda]$, define the per-position prefix mask
\begin{equation}
    M_t^{\mathrm{prefix}} := \mathbb{I}\!\big[\rhoprefix{t} \in [\lambda, \Lambda]\big].
    \label{eq:prefix-mask}
\end{equation}
\end{definition}

Three structural properties distinguish the prefix-level mask from existing methods:

\textbf{Causality.} $\rhoprefix{t}$ depends only on the prefix ratios $(\rho_s)_{s \le t}$, respecting the autoregressive order.

\textbf{Stability under in-bound increments.} The following lemma formalizes a basic stability property of prefix geometric means.

\begin{lemma}[Stability of prefix geometric means]
\label{lem:prefix-stability}
Fix thresholds $0<\lambda\le 1\le \Lambda$, and define $\rhoprefix{t}$ as in \eqref{eq:prefix-is}. If
\begin{equation}
    \rhoprefix{t}\in[\lambda,\Lambda]
    \qquad\text{and}\qquad
    \rho_{t+1}\in[\lambda,\Lambda],
\end{equation}
then $\rhoprefix{t+1}\in[\lambda,\Lambda]$.
\end{lemma}

\begin{proof}
Taking logarithms and using \eqref{eq:prefix-is}, we have
\begin{equation}
    \log\rhoprefix{t}
    = \frac{1}{t}\sum_{s=1}^{t} \log\rho_s.
\end{equation}
Therefore,
\begin{align}
    \log\rhoprefix{t+1}
    &= \frac{1}{t+1}\sum_{s=1}^{t+1} \log\rho_s
    \nonumber\\
    &= \frac{t}{t+1}\left(\frac{1}{t}\sum_{s=1}^{t} \log\rho_s\right) + \frac{1}{t+1}\log\rho_{t+1}
    \nonumber\\
    &= \frac{t}{t+1}\log\rhoprefix{t} + \frac{1}{t+1}\log\rho_{t+1}.
\end{align}
Since $\rhoprefix{t}\in[\lambda,\Lambda]$ and $\rho_{t+1}\in[\lambda,\Lambda]$, both
$\log\rhoprefix{t}$ and $\log\rho_{t+1}$ lie in $[\log\lambda,\log\Lambda]$. The last display is a convex combination, hence
$\log\rhoprefix{t+1}\in[\log\lambda,\log\Lambda]$, which implies $\rhoprefix{t+1}\in[\lambda,\Lambda]$.
\end{proof}

\textbf{Granularity.} The mask is computed at \emph{per-position} granularity: a single trajectory can contribute gradients at some
positions while being masked at others. This can be viewed as a prefix-level refinement of the sequence-level rejection mechanism in TRM.

\noindent\textbf{Notation.} To avoid overloading symbols, we reserve $(\epsilon,\delta)$ for the divergence quantities in
\Cref{sec:preliminaries}. In the deviation-pattern analysis below, we use $\varepsilon$ to denote a uniform bound on the per-token
log-ratios $\log\rho_t$.

\begin{theorem}[Causality-Aware Masking with Conditional Dominance]
\label{thm:prefix-tighter}
Let $M^{\mathrm{prefix}}_t$, $M^{\mathrm{token}}_t$, and $M^{\mathrm{seq}}_t$ denote the prefix-level mask
(\Cref{def:prefix-trm-proxy}), the token-level ICEPOP mask, and a sequence-level (GSPO-style) mask respectively, all with the same
threshold $[\lambda, \Lambda]$ where
$0 < \lambda \le 1 \le \Lambda$. Concretely, the sequence-level mask is defined by a single statistic
\begin{equation}
    \rhoseq
    := \exp\!\left(\frac{1}{T}\sum_{s=1}^{T}\log\rho_s\right)
    = \left(\prod_{s=1}^{T}\rho_s\right)^{\frac{1}{T}},
\end{equation}
and $M_t^{\mathrm{seq}} := \mathbb{I}[\rhoseq \in [\lambda,\Lambda]]$ for all $t$.
Assume policy support overlap (\Cref{asm:support}). Then:

\begin{enumerate}
    \item[(a)] \textbf{Early deviation: the prefix mask rejects a (possibly longer) prefix.} Assume $\rho_1 \notin [\lambda, \Lambda]$ and
    $|\log\rho_t| \le \varepsilon$ for all $t>1$, where $0 \le \varepsilon \le \min(\log\Lambda, |\log\lambda|)$. Then token-level ICEPOP masks
    only position $t=1$, while the prefix mask rejects at least position $t=1$. Moreover, if $\log\rho_1>\log\Lambda$ then for every position $t$
    satisfying
    \begin{equation}
        t < \frac{\log\rho_1+\varepsilon}{\log\Lambda+\varepsilon},
        \label{eq:early-upper-sufficient}
    \end{equation}
    we have $M_t^{\mathrm{prefix}}=0$. Symmetrically, if $\log\rho_1<\log\lambda$ then for every position $t$ satisfying
    \begin{equation}
        t < \frac{-\log\rho_1+\varepsilon}{|\log\lambda|+\varepsilon},
        \label{eq:early-lower-sufficient}
    \end{equation}
    we have $M_t^{\mathrm{prefix}}=0$. In particular, in the early-deviation regime the prefix mask rejects at least as many tokens as
    token-level ICEPOP.

    \item[(b)] \textbf{Late deviation: the prefix mask is more local than sequence masking.} Suppose there exists $t^*\in\{1,\ldots,T\}$ such that
    $|\log\rho_t| \le \varepsilon$ for all $t\neq t^*$ and $\rho_{t^*}\notin[\lambda,\Lambda]$, where $0 \le \varepsilon \le \min(\log\Lambda,
    |\log\lambda|)$. Then prefix masking can only occur at positions $t \ge t^*$, so
    \begin{equation}
    \big|\{t : M_t^{\mathrm{prefix}} = 0\}\big| \le T - t^* + 1.
    \end{equation}
    In contrast, sequence-level masking is all-or-nothing: $|\{t : M_t^{\mathrm{seq}} = 0\}| \in \{0, T\}$. Moreover, in the pure
    single-spike case where $\rho_t=1$ for $t\neq t^*$ and $\rho_{t^*}>1$, one has
    \[
        \log\rhoprefix{t^*} = \frac{1}{t^*}\log\rho_{t^*} \ge \frac{1}{T}\log\rho_{t^*} = \log\rhoseq,
    \]
    so there exist regimes where $M_{t^*}^{\mathrm{prefix}}=0$ while $M_t^{\mathrm{seq}}=1$ for all $t$ (sequence geometric mean diluted by
    large $T$). The lower-spike case $\rho_{t^*}<1$ is analogous with the lower threshold.

    \item[(c)] \textbf{Monotone violation: all methods reject.} If $\log\rho_s > \log\Lambda$ for all $s$ (upper violation), or
    $\log\rho_s < \log\lambda$ for all $s$ (lower violation), then $M_t^{\mathrm{prefix}} = 0$ and $M_t^{\mathrm{seq}} = 0$ for all $t$,
    so both methods reject all $T$ tokens.
\end{enumerate}
\end{theorem}

\begin{proof}[Proof sketch]
The full proof is in \Cref{app:prefix-proof}. We highlight the key steps.

\textbf{Part (a).} The $\varepsilon$-bounded condition implies $\log\rho_t \in [\log\lambda, \log\Lambda]$ for all $t>1$, hence ICEPOP masks
only $t=1$. For the prefix statistic, note that
\begin{equation}
    \log\rhoprefix{t}
    = \frac{1}{t}\sum_{s=1}^{t} \log\rho_s
    = \frac{1}{t}\left(\log\rho_1+\sum_{s=2}^{t}\log\rho_s\right).
\end{equation}
Using the bounds
\begin{align}
    \sum_{s=2}^{t}\log\rho_s &\ge -(t-1)\varepsilon,
    \\
    \sum_{s=2}^{t}\log\rho_s &\le (t-1)\varepsilon,
\end{align}
which yield the sufficient conditions \Cref{eq:early-upper-sufficient,eq:early-lower-sufficient}.

\textbf{Part (b).} For $t<t^*$, all active log-ratios in the prefix satisfy $\log\rho_s\in[\log\lambda,\log\Lambda]$, so repeated
application of \Cref{lem:prefix-stability} implies $\rhoprefix{t}\in[\lambda,\Lambda]$ (equivalently,
$\log\rhoprefix{t}\in[\log\lambda,\log\Lambda]$) and hence
$M_t^{\mathrm{prefix}}=1$. Therefore masking can only occur at positions $t\ge t^*$, yielding the stated bound. The single-spike
comparison follows by specializing to $\rho_t=1$ for $t\neq t^*$ and $\rho_{t^*}>1$, which gives $\log\rhoseq=\log\rho_{t^*}/T$ and
$\log\rhoprefix{t^*}=\log\rho_{t^*}/t^*$, so $\log\rhoprefix{t^*}\ge \log\rhoseq$ since $t^*\le T$.

\textbf{Part (c).} If each $\log\rho_s$ is strictly beyond the same threshold, then their running average is also strictly beyond the
threshold, so both prefix and sequence masks reject all tokens.
\end{proof}

\begin{remark}[Same-sign but in-bound deviations]
When all $\log\rho_s$ share the same sign but do not all exceed the corresponding threshold, the prefix log-statistic $\log\rhoprefix{t}$ need not be
monotone in $t$ and may return within bounds after crossing a threshold. In this regime there is no unconditional dominance ordering
between prefix masking and token-level masking: token-level methods detect large single-token outliers, while prefix masking detects
cumulative drift from many moderate deviations.
\end{remark}

\subsection{Connection to the Adaptive Bound}
\label{sec:adaptive-connection}

We relate prefix masking to the per-position, causal structure that appears in trust-region error bounds for long-horizon
LLM-RL~\citep{li2025trm}. In our notation, these bounds analyze the policy pair $(\piroll,\pitheta)$.

\begin{lemma}[KL chain rule for the cumulative log-ratio]
\label{lem:cumsum-kl}
Under Assumptions~\ref{asm:support}--\ref{asm:finite-length}, for any $t \le T$,
\begin{equation}
    \E_{\piroll}\!\Big[\sum_{s=1}^{t} \log\rho_s\Big]
    = -\sum_{s=1}^{t} \E_{c_s \sim d_s^{\piroll}}\!\big[\DKL(\piroll(\cdot|c_s) \| \pitheta(\cdot|c_s))\big]
    = -\DKL(d_{t+1}^{\piroll} \| d_{t+1}^{\pitheta}),
    \label{eq:cumsum-kl}
\end{equation}
where the final equality is the KL chain rule applied to the joint prefix distributions.
\end{lemma}

\begin{proof}
We start from the definition $\log\rho_s = \log \pitheta(y_s\mid c_s) - \log \piroll(y_s\mid c_s)$ in \eqref{eq:log-ratio}. Conditioning on
$c_s$ and taking expectation over $y_s\sim \piroll(\cdot\mid c_s)$ gives
\begin{align}
    \E_{\piroll}\!\big[\log\rho_s \mid c_s\big]
    &= \sum_{v} \piroll(v\mid c_s)\,\log\frac{\pitheta(v\mid c_s)}{\piroll(v\mid c_s)}
    \\
    &= - \sum_{v} \piroll(v\mid c_s)\,\log\frac{\piroll(v\mid c_s)}{\pitheta(v\mid c_s)}
    \\
    &= -\DKL\!\big(\piroll(\cdot\mid c_s)\,\|\,\pitheta(\cdot\mid c_s)\big).
\end{align}
Taking expectation over $c_s\sim d_s^{\piroll}$ and summing from $s=1$ to $t$ yields the first equality in \eqref{eq:cumsum-kl}.

For the second equality, apply the KL chain rule to the joint prefix distributions under the autoregressive factorizations:
\begin{equation}
    \DKL\!\big(d_{t+1}^{\piroll}\,\|\,d_{t+1}^{\pitheta}\big)
    = \sum_{s=1}^{t}\E_{c_s \sim d_s^{\piroll}}\!\Big[\DKL\!\big(\piroll(\cdot\mid c_s)\,\|\,\pitheta(\cdot\mid c_s)\big)\Big].
\end{equation}
Combining the two displays yields \eqref{eq:cumsum-kl}.
\end{proof}

\begin{remark}[Prefix filtering as a sample-level proxy]
\label{rem:prefix-proxy}
Thresholding $\rhoprefix{t}$ is equivalent to a per-position, sample-level constraint on the prefix average log-ratio:
\begin{equation}
    \rhoprefix{t} \in [\lambda, \Lambda]
    \iff \log\rhoprefix{t} \in [\log\lambda, \log\Lambda]
    \iff \frac{1}{t}\sum_{s=1}^{t}\log\rho_s \in [\log\lambda, \log\Lambda].
    \label{eq:threshold-kl}
\end{equation}
In contrast, \Cref{lem:cumsum-kl} is an \emph{expectation} identity over the behavior distribution. Therefore, the threshold condition
should be interpreted as a practical, sample-level proxy for Causal Trust Region filtering rather than a population-level guarantee
without additional assumptions. Separately, Pinsker's inequality relates the population-level KL in \eqref{eq:cumsum-kl} to a TV bound:
\begin{equation}
    \|d_{t+1}^{\pitheta} - d_{t+1}^{\piroll}\|_{\mathrm{TV}}
    \le \sqrt{\frac{\DKL(d_{t+1}^{\piroll} \| d_{t+1}^{\pitheta})}{2}}.
    \label{eq:context-shift-prefix}
\end{equation}
\end{remark}

\begin{corollary}[Per-Position Sample-Level Causal Trust Region Constraint]
\label{cor:per-position-trust}
Under prefix Causal Trust Region masking with threshold $[\lambda, \Lambda]$, for all accepted tokens ($M_t^{\mathrm{prefix}} = 1$), the sample-level cumulative log-ratio satisfies:
\begin{equation}
    \log\rhoprefix{t}
    = \frac{1}{t}\sum_{s=1}^{t} \log\rho_s
    \in [\log\lambda, \log\Lambda].
\end{equation}
This provides a per-position, sample-level trust region constraint analogous to the per-position structure in the Adaptive bound. The constraint operates on individual trajectories; its connection to the population-level KL is through the expectation identity in Eq.~\eqref{eq:cumsum-kl}.
\end{corollary}

\begin{proof}
By \Cref{def:prefix-trm-proxy}, $M_t^{\mathrm{prefix}} = 1$ implies $\rhoprefix{t}\in[\lambda,\Lambda]$. Taking logarithms yields
$\log\rhoprefix{t}\in[\log\lambda,\log\Lambda]$. The identity $\log\rhoprefix{t} = \frac{1}{t}\sum_{s=1}^{t}\log\rho_s$ follows
directly from \eqref{eq:prefix-is}.
\end{proof}

\subsection{The Combined Loss}
\label{sec:combined-loss}

We define the loss as the negative surrogate objective, $\mathcal{L} = -\mathcal{L}_{\piroll}(\pitheta)$, so that gradient descent on
$\mathcal{L}$ corresponds to maximizing the surrogate. The REINFORCE Pro Max loss applies the prefix mask to the standard per-token
importance-weighted surrogate:
\begin{equation}
    \mathcal{L}^{\mathrm{ProMax}}
    := -\sum_{t=1}^{T} M_t^{\mathrm{prefix}}\,\rho_t\,\hat{A}_t.
    \label{eq:promax-loss}
\end{equation}
Here $\hat{A}_t$ is the adaptively normalized advantage from \Cref{sec:adaptive-norm}, $M_t^{\mathrm{prefix}}$ is the prefix-level
Causal Trust Region mask from \Cref{def:prefix-trm-proxy},
$\rho_t$ is the per-token ratio defined in \eqref{eq:ratio}.

\textbf{Gradient flow.} Gradients propagate only through $\rho_t$. The prefix mask $M_t^{\mathrm{prefix}}$ and the normalized advantage
$\hat{A}_t$ are detached from the computation graph. When $M_t^{\mathrm{prefix}} = 0$, the loss at position $t$ is zero and contributes
no gradient.
