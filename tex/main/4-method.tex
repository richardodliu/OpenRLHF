\section{REINFORCE Max: Variance-Reduced Advantage Estimation}
\label{sec:reinforce-max}

\subsection{Token-Level Expansion}
\label{sec:token-expand}

Given a prompt $x$, we sample $n$ responses $\{y^{(1)}, \ldots, y^{(n)}\}$ with rewards $\{r_1, \ldots, r_n\}$. Each response
$y^{(i)}$ consists of $T_i$ action tokens. REINFORCE Max combines three ingredients: a leave-one-out baseline
(\Cref{eq:rloo-baseline}), token-level expansion (\Cref{sec:token-expand}), and adaptive normalization (\Cref{sec:adaptive-norm}).

We adopt the leave-one-out (RLOO) baseline~\citep{ahmadian2024back} for its statistical independence, which avoids correlated baseline artifacts present in the mean baseline (see \Cref{app:rloo-proof} for a brief summary and references).

\begin{definition}[Leave-One-Out Baseline]
\label{def:rloo}
Assume $n \ge 2$. For sample $i$, the baseline and shaped reward are:
\begin{equation}
    b_i = \frac{1}{n-1}\sum_{j \neq i} r_j, \qquad \tilde{r}_i = r_i - b_i.
    \label{eq:rloo-baseline}
\end{equation}
\end{definition}

Since the reward is a scalar per response and we use $\gamma = 1.0$ (see \Cref{app:gamma-one} for a brief note), the token-level advantage is simply the shaped reward itself:
\begin{equation*}
    A_{i,t} = \tilde{r}_i = r_i - b_i, \quad \forall\, t \in \{1, \ldots, T_i\}.
\end{equation*}

\subsection{Adaptive Asymmetric Normalization}
\label{sec:adaptive-norm}

Standard normalization $\hat{A} = (A - \mu)/\sigma$ applies a uniform scaling to all advantages. In reasoning tasks, the advantage distribution is typically highly skewed: most samples fail (negative advantages) while few succeed (positive advantages). Uniform scaling distorts the relative balance between reinforcement and punishment signals.

\begin{definition}[Adaptive Normalization]
\label{def:adaptive-norm}
Fix a prompt group with token-level advantages $\{A_{i,t}\}$, and exclude zero-advantage positions from the normalization statistics.
Define the index sets
\begin{equation*}
    \mathcal{P} := \{(i,t) : A_{i,t} > 0\},
    \qquad
    \mathcal{N} := \{(i,t) : A_{i,t} < 0\},
\end{equation*}
The normalized advantage is defined by a sign-dependent rescaling:
\begin{equation*}
    \hat{A}_{i,t}
    =
    \begin{cases}
        \alpha \cdot A_{i,t}, & \text{if } A_{i,t} > 0,
        \\
        \beta \cdot A_{i,t}, & \text{if } A_{i,t} < 0,
        \\
        0, & \text{if } A_{i,t} = 0.
    \end{cases}
\end{equation*}
where $\alpha,\beta>0$ are chosen to satisfy the following \emph{empirical} constraints over non-zero tokens:
\begin{align*}
    \frac{1}{|\mathcal{P}| + |\mathcal{N}|}\sum_{(i,t)\in\mathcal{P}\cup\mathcal{N}}\hat{A}_{i,t} &= 0,
    \\
    \frac{1}{|\mathcal{P}| + |\mathcal{N}|}\sum_{(i,t)\in\mathcal{P}\cup\mathcal{N}}\hat{A}_{i,t}^2 &= 1.
\end{align*}
When $\mathcal{P} \neq \emptyset$ and $\mathcal{N} \neq \emptyset$, define the ratio
\begin{equation*}
    \kappa := \frac{\sum_{(i,t)\in\mathcal{P}} A_{i,t}}{-\sum_{(i,t)\in\mathcal{N}} A_{i,t}}.
\end{equation*}
Equivalently, these constraints can be written as
\begin{equation*}
    \begin{aligned}
        \beta &= \alpha \kappa,
        \\
        \alpha^2 \sum_{(i,t)\in\mathcal{P}} A_{i,t}^2
        + \beta^2 \sum_{(i,t)\in\mathcal{N}} A_{i,t}^2
        &=
        |\mathcal{P}| + |\mathcal{N}|.
    \end{aligned}
\end{equation*}
\end{definition}

When $\mathcal{P} \neq \emptyset$ and $\mathcal{N} \neq \emptyset$, solving the constraints in \Cref{def:adaptive-norm} yields the
closed-form scaling factors:
\begin{equation}
    \label{eq:alpha-beta}
    \begin{aligned}
        \alpha
        &=
        \sqrt{
            \frac{|\mathcal{P}| + |\mathcal{N}|}{
                \sum_{(i,t)\in\mathcal{P}} A_{i,t}^2
                +
                \kappa^2 \sum_{(i,t)\in\mathcal{N}} A_{i,t}^2
            }
        },
        \\
        \beta
        &=
        \alpha \kappa.
    \end{aligned}
\end{equation}

\paragraph{Advantage Sign Preservation}
\label{par:adv-sign-preservation}
The adaptive normalization preserves the sign of the advantage at every token: $\sign(\hat{A}_{i,t}) = \sign(A_{i,t})$ for all $(i,t)$
with $A_{i,t} \neq 0$. Moreover, the relative ordering of magnitudes is preserved within the positive group $\mathcal{P}$ and within the
negative group $\mathcal{N}$. This follows immediately because the normalization scales $A_{i,t}$ by $\alpha>0$ on $\mathcal{P}$ and by
$\beta>0$ on $\mathcal{N}$ (\Cref{eq:alpha-beta}), and multiplication by a positive constant preserves both sign and ordering.

\paragraph{Connection to Response Length}
Our asymmetry comes from sign-dependent rescaling of advantages under the token-level expansion and the empirical constraints in
\Cref{def:adaptive-norm}.
Since \Cref{sec:token-expand} uses $A_{i,t}=\tilde r_i$ for all $t\in\{1,\ldots,T_i\}$, the prompt-group statistics in
\Cref{def:adaptive-norm} are implicitly length-weighted. In particular, the positive/negative token index sets satisfy
\[
|\mathcal P| = \sum_{i:\tilde r_i>0} T_i,
\qquad
|\mathcal N| = \sum_{i:\tilde r_i<0} T_i,
\]
and the ratio $\kappa$ in \Cref{def:adaptive-norm} can be written as
\[
\kappa
=
\frac{\sum_{(i,t)\in\mathcal P} A_{i,t}}{-\sum_{(i,t)\in\mathcal N} A_{i,t}}
=
\frac{\sum_{i:\tilde r_i>0} T_i\,\tilde r_i}{-\sum_{i:\tilde r_i<0} T_i\,\tilde r_i}.
\]
Therefore the closed-form scales $(\alpha,\beta)$ in Eq.~\eqref{eq:alpha-beta} depend not only on reward magnitudes but also on the ratio of
positive-token mass to negative-token mass within the same prompt group, which is directly affected by response lengths $(T_i)$.
This is the sense in which the advantage scaling implicitly incorporates response-length information, rather than implementing any form of
ratio clipping.

This length dependence can be beneficial in long-horizon reasoning settings, where token-level expansion makes each response contribute
roughly in proportion to its length. When success is rare, most tokens in a prompt group come from incorrect responses; without additional
balancing, the negative-token mass can dominate the effective gradient budget. The sign-dependent scaling $(\alpha,\beta)$ adapts to this
imbalance by reweighting positive and negative token groups based on their empirical token mass, stabilizing the relative strength of
reinforcement versus punishment signals within each prompt group.

To make this effect explicit, consider an idealized prompt group where $\tilde r_i\in\{+1,-1\}$ and hence $A_{i,t}=\tilde r_i$ for all
$t\in\{1,\ldots,T_i\}$. Then
\[
\sum_{(i,t)\in\mathcal P} A_{i,t}^2 = |\mathcal P|,
\qquad
\sum_{(i,t)\in\mathcal N} A_{i,t}^2 = |\mathcal N|,
\qquad
\kappa = \frac{|\mathcal P|}{|\mathcal N|}.
\]
Substituting into Eq.~\eqref{eq:alpha-beta} yields
\[
\alpha
=
\sqrt{\frac{|\mathcal P|+|\mathcal N|}{|\mathcal P|+\kappa^2|\mathcal N|}},
\qquad
\beta=\alpha\kappa.
\]
In the sparse-success regime $|\mathcal P|\ll|\mathcal N|$, this implies $\alpha\approx\sqrt{|\mathcal N|/|\mathcal P|}$ while
$\beta\approx\sqrt{|\mathcal P|/|\mathcal N|}$: positive tokens are amplified and abundant negative tokens are attenuated in a way that
automatically compensates for length-induced imbalance.

This balancing is particularly helpful when long candidate solutions are more likely to be truncated and judged incorrect. Such
trajectories can contribute a large negative-token mass (large $T_i$ with $\tilde r_i<0$), which would otherwise impose an outsized
penalty under token-level expansion. By shrinking the negative scaling factor $\beta$ when negative-token mass dominates, adaptive
normalization can mitigate over-penalization of long-horizon exploration induced by truncation-driven failures. This does not correct
reward mislabeling; it only addresses the induced gradient imbalance under token-level training.

\paragraph{Uniform Scale Mode}
When all $n$ samples share the same reward, the leave-one-out baseline yields zero advantages, eliminating gradient signal. An optional
\emph{uniform scale} mode recovers this signal by setting $\tilde{r}_i = r_i/n$ and skipping normalization, improving sample utilization.
See \Cref{app:uniform-scale} for details.
\par
\noindent\textbf{Numerical safeguards.} In implementation, we include simple numerical checks and clamping to prevent extreme or unstable
scales; see \Cref{app:numerical}.


\section{REINFORCE Pro: Prefix Causal Trust Region Masking}
\label{sec:reinforce-pro}

\subsection{Off-Policy Ratio Setup}
\label{sec:offpolicy-problem}

In PPO-style LLM-RL, samples are generated by a fixed rollout policy $\piroll$, and gradients are computed under a training policy
$\pitheta$ that is updated by gradient steps. This matches the trust-region setup in~\citet{li2025trm}.

The per-token importance ratio between these two policies is
\begin{equation*}
    \rho_t := \frac{\pitheta(y_t \mid c_t)}{\piroll(y_t \mid c_t)},
\end{equation*}
as defined in Eq.~\eqref{eq:ratio}. Throughout this section, we restrict attention to masking strategies constructed solely from
$(\rho_t)_{t=1}^T$ (equivalently, the log-ratios $(\log\rho_t)_{t=1}^T$ as in Eq.~\eqref{eq:log-ratio}), and we do not introduce additional
intermediate policies.

\paragraph{A Causal Trust Region Viewpoint}
Trust Region Masking (TRM; \citet{li2025trm}) introduces a \emph{sequence-level} rejection mechanism via a binary mask
$M^{\mathrm{TRM}}(x,y)$, yielding a masked surrogate objective of the form
\begin{equation*}
    \mathcal{L}_{\mathrm{masked}}^{\mathrm{TRM}}(\pitheta)
    := \E_{\piroll}\!\left[
        M^{\mathrm{TRM}}(x,y)\cdot A(x,y)\cdot \sum_{t=1}^{T}\rho_t
    \right].
\end{equation*}
This matches the masked surrogate objective in \citet{li2025trm} up to notation.
Motivated by the same trust-region perspective for long-horizon generation, we introduce a prefix-level, per-position gate
$M_t^{\mathrm{pre}}$ computed from a ratio-based \emph{prefix} statistic and multiply it into the token-level loss. Unlike TRM, which
accepts or rejects an entire trajectory via a single gate $M^{\mathrm{TRM}}(x,y)$, the prefix gate can mask only the positions whose prefixes violate the
threshold, thereby retaining positional granularity while respecting the autoregressive order. For comparison, a prefix-level masked
surrogate can be written as
\begin{equation*}
    \mathcal{L}_{\mathrm{masked}}^{\mathrm{pre}}(\pitheta)
    := \E_{\piroll}\!\left[
        A(x,y)\cdot \sum_{t=1}^{T} M_t^{\mathrm{pre}}\rho_t
    \right],
\end{equation*}
where the gate is applied at token granularity. Our implementation uses a per-token importance-weighted surrogate with normalized
advantages; see \Cref{sec:combined-loss} for the exact objective. Importantly, our prefix criterion is a tractable sample-level
\emph{proxy} for Causal Trust Region filtering and is not the sequence-level max-divergence criterion used in TRM (based on
$\max_t \DKL(c_t)$).

\subsection{Limitations of Existing Methods}

\begin{remark}[Token-Level Importance Sampling Ignores Causal Structure]
\label{prop:token-is-fails}
This is a standard limitation of token-level ratio truncation and masking.
Fix thresholds $\epslow\in(0,1)$ and $\epshigh>0$ and define the acceptance interval
\[
    [1-\epslow,\,1+\epshigh].
\]
Consider a length-$T$ trajectory such that
\begin{equation*}
    \rho_1 \notin [1-\epslow,1+\epshigh],
    \qquad
    \rho_t \in [1-\epslow,1+\epshigh]\ \text{ for all }\ t\in\{2,\ldots,T\}.
\end{equation*}
Then the token-level IcePop mask,
\begin{equation*}
    M_t^{\mathrm{tok}} := \mathbb{I}\!\big[\rho_t \in [1-\epslow,1+\epshigh]\big],
\end{equation*}
masks only the first position. In particular, it does not prevent later positions $t\ge 2$ from contributing gradients even though the
contexts $(c_t)_{t\ge2}$ are sampled under the behavior distribution induced by $\piroll$, which differs from the context distribution that
would be induced by the updated policy $\pitheta$ when $\rho_1 \neq 1$.
\par
\noindent The conclusion follows immediately from the definition of $M_t^{\mathrm{tok}}$ and context shift propagation
(\Cref{lem:coupling}); see also \citet{li2025trm,yao2025offpolicy}.
\end{remark}

\begin{remark}[Sequence-Level Importance Sampling Loses Positional Information]
\label{prop:seq-is-fails}
This is a standard limitation of sequence-level aggregation and gating.
Let $\rhoseq$ denote the geometric-mean sequence likelihood ratio from \Cref{sec:existing-is}. Then its log is the sequence-average
log-ratio,
\begin{equation*}
    \log\rhoseq = \frac{1}{T}\sum_{t=1}^T \log\rho_t.
\end{equation*}
Assume there exists $t^*\in\{1,\ldots,T\}$ such that $|\log\rho_t|\le \varepsilon$ for all $t\neq t^*$. Then
\begin{equation*}
    \Big|\log\rhoseq - \frac{1}{T}\log\rho_{t^*}\Big| \le \frac{T-1}{T}\,\varepsilon \le \varepsilon.
\end{equation*}
In particular, in the pure single-spike case $\log\rho_t=0$ for $t\neq t^*$, one has $\log\rhoseq=\log\rho_{t^*}/T$, so
$\rhoseq = (\rho_{t^*})^{1/T}\to 1$ as $T\to\infty$ for any fixed $\rho_{t^*}>0$.
\par
\noindent This dilution is a direct consequence of averaging $\log\rho_t$ over the full horizon; see also \citet{gspo2025}.
\end{remark}

\begin{example}[Numerical Dilution at Long Horizons]
Consider the pure single-spike case with $T=4096$, $\log\rho_{t^*}=3$, and $\log\rho_t=0$ for $t\neq t^*$. Then
\begin{equation*}
    \rhoseq = \exp(3/4096) \approx 1.00073,
\end{equation*}
which can easily fall inside practical acceptance intervals $[1-\epslow,1+\epshigh]$ even though the local deviation at $t^*$ is large in
log-ratio magnitude.
\end{example}

\subsection{Prefix Causal Trust Region Proxy: Definition and Properties}
\label{sec:prefix-is}

\begin{definition}[Prefix Cumulative Ratio Proxy]
\label{def:prefix-is}
For a response trajectory $y=(y_1,\ldots,y_T)$ with per-token log-ratios $\log\rho_t$ defined in Eq.~\eqref{eq:log-ratio}, define the prefix
cumulative ratio statistic
\begin{equation}
    \rhoprefix{t}
    := \exp\!\left(\frac{1}{t}\sum_{s=1}^{t} \log\rho_s\right)
    = \left(\prod_{s=1}^{t}\rho_s\right)^{\frac{1}{t}},
    \qquad
    t\in\{1,\ldots,T\}.
    \label{eq:prefix-is}
\end{equation}
The statistic $\rhoprefix{t}$ is the geometric mean of per-token importance ratios over the length-$t$ prefix.
\end{definition}

\begin{definition}[Prefix Causal Trust Region Mask]
\label{def:prefix-trm-proxy}
Given thresholds $\epslow\in(0,1)$ and $\epshigh>0$, define the per-position prefix mask
\begin{equation}
    M_t^{\mathrm{pre}} := \mathbb{I}\!\big[\rhoprefix{t} \in [1-\epslow, 1+\epshigh]\big].
    \label{eq:prefix-mask}
\end{equation}
\end{definition}

\Cref{app:mask-variants} collects several compatible variants (Variants~I--IV) that modify the prefix gate while retaining the same
ratio setup based on $(\piroll,\pitheta)$.

Three structural properties distinguish the prefix-level mask from existing methods:

\textbf{Causality.} $\rhoprefix{t}$ depends only on the prefix ratios $(\rho_s)_{s \le t}$, respecting the autoregressive order.

\textbf{Stability under in-bound increments.} The following lemma formalizes a basic stability property of prefix geometric means.

\begin{lemma}[Stability of Prefix Geometric Means]
\label{lem:prefix-stability}
Fix thresholds $\epslow\in(0,1)$ and $\epshigh>0$, and define $\rhoprefix{t}$ as in Eq.~\eqref{eq:prefix-is}. If
\begin{equation*}
    \rhoprefix{t}\in[1-\epslow,1+\epshigh]
    \qquad\text{and}\qquad
    \rho_{t+1}\in[1-\epslow,1+\epshigh],
\end{equation*}
then $\rhoprefix{t+1}\in[1-\epslow,1+\epshigh]$.
\end{lemma}

\begin{proof}
Taking logarithms and using Eq.~\eqref{eq:prefix-is}, we have
\begin{equation*}
    \log\rhoprefix{t}
    = \frac{1}{t}\sum_{s=1}^{t} \log\rho_s.
\end{equation*}
Therefore,
\begin{align*}
    \log\rhoprefix{t+1}
    &= \frac{1}{t+1}\sum_{s=1}^{t+1} \log\rho_s
    \\
    &= \frac{t}{t+1}\left(\frac{1}{t}\sum_{s=1}^{t} \log\rho_s\right) + \frac{1}{t+1}\log\rho_{t+1}
    \\
    &= \frac{t}{t+1}\log\rhoprefix{t} + \frac{1}{t+1}\log\rho_{t+1}.
\end{align*}
Since $\rhoprefix{t}\in[1-\epslow,1+\epshigh]$ and $\rho_{t+1}\in[1-\epslow,1+\epshigh]$, both
$\log\rhoprefix{t}$ and $\log\rho_{t+1}$ lie in $[\log(1-\epslow),\log(1+\epshigh)]$. The last display is a convex combination, hence
$\log\rhoprefix{t+1}\in[\log(1-\epslow),\log(1+\epshigh)]$, which implies $\rhoprefix{t+1}\in[1-\epslow,1+\epshigh]$.
\end{proof}

\textbf{Granularity.} The mask is computed at \emph{per-position} granularity: a single trajectory can contribute gradients at some
positions while being masked at others. This can be viewed as a prefix-level refinement of the sequence-level rejection mechanism in TRM.

\noindent\textbf{Notation.} To avoid overloading symbols, we reserve $(\epsilon,\delta)$ for the divergence quantities in
\Cref{sec:preliminaries}. The ratio-truncation thresholds are denoted by $(\epslow,\epshigh)$, defining the acceptance interval
\[
    [1-\epslow,\,1+\epshigh].
\]
For convenience, we also use the equivalent log-thresholds
\[
    \tau_+ := \log(1+\epshigh),
    \qquad
    \tau_- := -\log(1-\epslow),
\]
so that $\rho_t \in [1-\epslow,1+\epshigh]$ is equivalent to $\log\rho_t \in [-\tau_-,\tau_+]$.
In the deviation-pattern analysis below, we use $\varepsilon$ to denote a uniform bound on the per-token log-ratios $\log\rho_t$.

\begin{theorem}[Causality-Aware Masking with Conditional Dominance]
\label{thm:prefix-tighter}
Let $M^{\mathrm{pre}}_t$, $M^{\mathrm{tok}}_t$, and $M^{\mathrm{seq}}_t$ denote the prefix-level mask
(\Cref{def:prefix-trm-proxy}), the token-level IcePop mask, and a sequence-level (GSPO-style) mask respectively, all with the same
acceptance interval $[1-\epslow, 1+\epshigh]$ for some $\epslow\in(0,1)$ and $\epshigh>0$.
Concretely, the sequence-level mask is defined by a single statistic
\begin{equation*}
    \rhoseq
    := \exp\!\left(\frac{1}{T}\sum_{s=1}^{T}\log\rho_s\right)
    = \left(\prod_{s=1}^{T}\rho_s\right)^{\frac{1}{T}},
\end{equation*}
and $M_t^{\mathrm{seq}} := \mathbb{I}[\rhoseq \in [1-\epslow,1+\epshigh]]$ for all $t$.
Assume policy support overlap (\Cref{asm:support}). Then:

\begin{enumerate}
    \item[(a)] \textbf{Early deviation: the prefix mask rejects a (possibly longer) prefix.} Assume $\rho_1 \notin [1-\epslow,1+\epshigh]$ and
    $|\log\rho_t| \le \varepsilon$ for all $t>1$, where $0 \le \varepsilon \le \min(\tau_+,\tau_-)$. Then token-level IcePop masks
    only position $t=1$, while the prefix mask rejects at least position $t=1$. Moreover, if $\log\rho_1>\tau_+$ then for every position $t$
    satisfying
    \begin{equation}
        t < \frac{\log\rho_1+\varepsilon}{\tau_+ + \varepsilon},
        \label{eq:early-upper-sufficient}
    \end{equation}
    we have $M_t^{\mathrm{pre}}=0$. Symmetrically, if $\log\rho_1<-\tau_-$ then for every position $t$ satisfying
    \begin{equation}
        t < \frac{-\log\rho_1+\varepsilon}{\tau_- + \varepsilon},
        \label{eq:early-lower-sufficient}
    \end{equation}
    we have $M_t^{\mathrm{pre}}=0$. In particular, in the early-deviation regime the prefix mask rejects at least as many tokens as
    token-level IcePop.

    \item[(b)] \textbf{Late deviation: the prefix mask is more local than sequence masking.} Suppose there exists $t^*\in\{1,\ldots,T\}$ such that
    $|\log\rho_t| \le \varepsilon$ for all $t\neq t^*$ and $\rho_{t^*}\notin[1-\epslow,1+\epshigh]$, where $0 \le \varepsilon \le \min(\tau_+,
    \tau_-)$. Then prefix masking can only occur at positions $t \ge t^*$, so
    \begin{equation*}
    \big|\{t : M_t^{\mathrm{pre}} = 0\}\big| \le T - t^* + 1.
    \end{equation*}
    In contrast, sequence-level masking is all-or-nothing: $|\{t : M_t^{\mathrm{seq}} = 0\}| \in \{0, T\}$. Moreover, in the pure
    single-spike case where $\rho_t=1$ for $t\neq t^*$ and $\rho_{t^*}>1$, one has
    \[
        \log\rhoprefix{t^*} = \frac{1}{t^*}\log\rho_{t^*} \ge \frac{1}{T}\log\rho_{t^*} = \log\rhoseq,
    \]
    so there exist regimes where $M_{t^*}^{\mathrm{pre}}=0$ while $M_t^{\mathrm{seq}}=1$ for all $t$ (sequence geometric mean diluted by
    large $T$). The lower-spike case $\rho_{t^*}<1$ is analogous with the lower threshold.

\item[(c)] \textbf{Monotone violation: all methods reject.} If $\log\rho_s > \tau_+$ for all $s$ (upper violation), or
    $\log\rho_s < -\tau_-$ for all $s$ (lower violation), then $M_t^{\mathrm{pre}} = 0$ and $M_t^{\mathrm{seq}} = 0$ for all $t$,
    so both methods reject all $T$ tokens.
\end{enumerate}
\end{theorem}

\begin{proof}[Proof sketch]
The full proof is in \Cref{app:prefix-proof}. We highlight the key steps.

\textbf{Part (a).} The $\varepsilon$-bounded condition implies $\log\rho_t \in [-\tau_-, \tau_+]$ for all $t>1$, hence IcePop masks
only $t=1$. For the prefix statistic, note that
\begin{equation*}
    \log\rhoprefix{t}
    = \frac{1}{t}\sum_{s=1}^{t} \log\rho_s
    = \frac{1}{t}\left(\log\rho_1+\sum_{s=2}^{t}\log\rho_s\right).
\end{equation*}
Using the bounds
\begin{align*}
    \sum_{s=2}^{t}\log\rho_s &\ge -(t-1)\varepsilon,
    \\
    \sum_{s=2}^{t}\log\rho_s &\le (t-1)\varepsilon,
\end{align*}
which yield the sufficient conditions \Cref{eq:early-upper-sufficient,eq:early-lower-sufficient}.

\textbf{Part (b).} For $t<t^*$, all active log-ratios in the prefix satisfy $\log\rho_s\in[-\tau_-,\tau_+]$, so repeated
application of \Cref{lem:prefix-stability} implies $\rhoprefix{t}\in[1-\epslow,1+\epshigh]$ (equivalently,
$\log\rhoprefix{t}\in[-\tau_-,\tau_+]$) and hence
$M_t^{\mathrm{pre}}=1$. Therefore masking can only occur at positions $t\ge t^*$, yielding the stated bound. The single-spike
comparison follows by specializing to $\rho_t=1$ for $t\neq t^*$ and $\rho_{t^*}>1$, which gives $\log\rhoseq=\log\rho_{t^*}/T$ and
$\log\rhoprefix{t^*}=\log\rho_{t^*}/t^*$, so $\log\rhoprefix{t^*}\ge \log\rhoseq$ since $t^*\le T$.

\textbf{Part (c).} If each $\log\rho_s$ is strictly beyond the same threshold, then their running average is also strictly beyond the
threshold, so both prefix and sequence masks reject all tokens.
\end{proof}

\begin{remark}[Same-Sign but In-Bound Deviations]
When all $\log\rho_s$ share the same sign but do not all exceed the corresponding threshold, the prefix log-statistic $\log\rhoprefix{t}$ need not be
monotone in $t$ and may return within bounds after crossing a threshold. In this regime there is no unconditional dominance ordering
between prefix masking and token-level masking: token-level methods detect large single-token outliers, while prefix masking detects
cumulative drift from many moderate deviations.
\end{remark}

\subsection{Connection to the Adaptive Bound}
\label{sec:adaptive-connection}

We relate prefix masking to the per-position, causal structure that appears in trust-region error bounds for long-horizon
LLM-RL~\citep{li2025trm}. In our notation, these bounds analyze the policy pair $(\piroll,\pitheta)$.

\begin{remark}[KL Chain Rule for the Cumulative Log-Ratio]
\label{lem:cumsum-kl}
Under Assumptions~\ref{asm:support}--\ref{asm:finite-length}, for any $t \le T$,
\begin{equation}
    \E_{\piroll}\!\Big[\sum_{s=1}^{t} \log\rho_s\Big]
    = -\sum_{s=1}^{t} \E_{c_s \sim d_s^{\piroll}}\!\big[\DKL(\piroll(\cdot|c_s) \| \pitheta(\cdot|c_s))\big]
    = -\DKL(d_{t+1}^{\piroll} \| d_{t+1}^{\pitheta}),
    \label{eq:cumsum-kl}
\end{equation}
where the final equality is the KL chain rule applied to the joint prefix distributions.
\par
\noindent This identity is the KL chain rule under autoregressive factorization; see, e.g., \citet{li2025trm}.
\end{remark}

\begin{remark}[Prefix Filtering as a Sample-Level Proxy]
\label{rem:prefix-proxy}
Thresholding $\rhoprefix{t}$ is equivalent to a per-position, sample-level constraint on the prefix average log-ratio:
\begin{align*}
    \rhoprefix{t} \in [1-\epslow, 1+\epshigh]
    &\iff \log\rhoprefix{t} \in [\log(1-\epslow), \log(1+\epshigh)]
    \\
    &\iff \frac{1}{t}\sum_{s=1}^{t}\log\rho_s \in [\log(1-\epslow), \log(1+\epshigh)].
\end{align*}
In contrast, \Cref{lem:cumsum-kl} is an \emph{expectation} identity over the behavior distribution. Therefore, the threshold condition
should be interpreted as a practical, sample-level proxy for Causal Trust Region filtering rather than a population-level guarantee
without additional assumptions. Separately, Pinsker's inequality relates the population-level KL in Eq.~\eqref{eq:cumsum-kl} to a TV bound:
\begin{equation*}
    \|d_{t+1}^{\pitheta} - d_{t+1}^{\piroll}\|_{\mathrm{TV}}
    \le \sqrt{\frac{\DKL(d_{t+1}^{\piroll} \| d_{t+1}^{\pitheta})}{2}}.
\end{equation*}
\end{remark}

\begin{remark}[Per-Position Sample-Level Causal Trust Region Constraint]
\label{cor:per-position-trust}
Under prefix Causal Trust Region masking with threshold $[1-\epslow, 1+\epshigh]$, for all accepted tokens ($M_t^{\mathrm{pre}} = 1$), the sample-level cumulative log-ratio satisfies:
\begin{equation*}
    \log\rhoprefix{t}
    = \frac{1}{t}\sum_{s=1}^{t} \log\rho_s
    \in [\log(1-\epslow), \log(1+\epshigh)].
\end{equation*}
This provides a per-position, sample-level trust region constraint analogous to the per-position structure in the Adaptive bound. The constraint operates on individual trajectories; its connection to the population-level KL is through the expectation identity in Eq.~\eqref{eq:cumsum-kl}.
\end{remark}

\subsection{The Combined Loss}
\label{sec:combined-loss}

We define the loss as the negative surrogate objective, $\mathcal{L} = -\mathcal{L}_{\piroll}(\pitheta)$, so that gradient descent on
$\mathcal{L}$ corresponds to maximizing the surrogate. The REINFORCE Pro Max loss applies the prefix mask to the standard per-token
importance-weighted surrogate:
\begin{equation}
    \mathcal{L}^{\mathrm{ProMax}}
    := -\sum_{t=1}^{T} M_t^{\mathrm{pre}}\,\rho_t\,\hat{A}_t.
    \label{eq:promax-loss}
\end{equation}
Here $\hat{A}_t$ is the adaptively normalized advantage from \Cref{sec:adaptive-norm}, $M_t^{\mathrm{pre}}$ is the prefix-level
Causal Trust Region mask from \Cref{def:prefix-trm-proxy},
$\rho_t$ is the per-token ratio defined in Eq.~\eqref{eq:ratio}.

\textbf{Gradient flow.} Gradients propagate only through $\rho_t$. The prefix mask $M_t^{\mathrm{pre}}$ and the normalized advantage
$\hat{A}_t$ are detached from the computation graph. When $M_t^{\mathrm{pre}} = 0$, the loss at position $t$ is zero and contributes
no gradient.
