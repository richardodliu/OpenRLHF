\section{REINFORCE Max: Variance-Reduced Advantage Estimation}
\label{sec:reinforce-max}

Given a prompt $x$, we sample $n$ responses $\{y^{(1)}, \ldots, y^{(n)}\}$ with rewards $\{r_1, \ldots, r_n\}$. Each response
$y^{(i)}$ consists of $T_i$ action tokens. REINFORCE Max combines three ingredients: a leave-one-out baseline
(\Cref{eq:rloo-baseline}), token-level expansion (\Cref{sec:token-expand}), and adaptive normalization (\Cref{sec:adaptive-norm}).

We adopt the leave-one-out (RLOO) baseline~\citep{ahmadian2024back} for its statistical independence, which avoids correlated baseline artifacts present in the mean baseline (see \Cref{app:rloo-proof} for a detailed analysis).

\begin{definition}[Leave-One-Out Baseline]
\label{def:rloo}
Assume $n \ge 2$. For sample $i$, the baseline and shaped reward are:
\begin{equation}
    b_i = \frac{1}{n-1}\sum_{j \neq i} r_j, \qquad \tilde{r}_i = r_i - b_i.
    \label{eq:rloo-baseline}
\end{equation}
\end{definition}

\subsection{Token-Level Expansion}
\label{sec:token-expand}

Since the reward is a scalar per response and we use $\gamma = 1.0$ (see \Cref{app:gamma-one} for a formal justification), the token-level advantage is simply the shaped reward itself:
\begin{equation}
    A_{i,t} = \tilde{r}_i = r_i - b_i, \quad \forall\, t \in \{1, \ldots, T_i\}.
    \label{eq:token-advantage}
\end{equation}

\subsection{Adaptive Asymmetric Normalization}
\label{sec:adaptive-norm}

Standard normalization $\hat{A} = (A - \mu)/\sigma$ applies a uniform scaling to all advantages. In reasoning tasks, the advantage distribution is typically highly skewed: most samples fail (negative advantages) while few succeed (positive advantages). Uniform scaling distorts the relative balance between reinforcement and punishment signals.

\begin{definition}[Adaptive Normalization]
\label{def:adaptive-norm}
For a prompt group with token-level advantages $\{A_{i,t}\}$, define:
\begin{equation}
    \mathcal{P} = \{(i,t) : A_{i,t} > 0\}, \quad \mathcal{N} = \{(i,t) : A_{i,t} < 0\},
\end{equation}
\begin{equation}
    S^+ = \!\sum_{(i,t) \in \mathcal{P}}\! A_{i,t},\; S^- = \!\sum_{(i,t) \in \mathcal{N}}\! A_{i,t},\; Q^+ = \!\sum_{(i,t) \in \mathcal{P}}\! A_{i,t}^2,\; Q^- = \!\sum_{(i,t) \in \mathcal{N}}\! A_{i,t}^2.
\end{equation}
Let $N = |\mathcal{P}| + |\mathcal{N}|$ be the count of non-zero tokens. The normalized advantage is:
\begin{equation}
    \hat{A}_{i,t} = \begin{cases} \alpha \cdot A_{i,t} & \text{if } A_{i,t} > 0, \\ \beta \cdot A_{i,t} & \text{if } A_{i,t} < 0, \\ 0 & \text{if } A_{i,t} = 0, \end{cases}
    \label{eq:adaptive-norm}
\end{equation}
where $\alpha, \beta > 0$ are chosen to satisfy the \emph{empirical} constraints on non-zero tokens:
\begin{equation}
    \frac{1}{N}\sum_{(i,t)\in\mathcal{P}\cup\mathcal{N}}\hat{A}_{i,t} = 0, \qquad
    \frac{1}{N}\sum_{(i,t)\in\mathcal{P}\cup\mathcal{N}}\hat{A}_{i,t}^2 = 1.
\end{equation}
Equivalently, $\alpha S^+ + \beta S^- = 0$ and $\alpha^2 Q^+ + \beta^2 Q^- = N$.
\end{definition}

\begin{proposition}[Closed-Form Solution]
\label{prop:alpha-beta}
Assume $\mathcal{P} \neq \emptyset$ and $\mathcal{N} \neq \emptyset$. The unique solution satisfying $\alpha S^+ + \beta S^- = 0$ and $\alpha^2 Q^+ + \beta^2 Q^- = N$ is:
\begin{equation}
    \alpha = \sqrt{\frac{N}{Q^+ + \left(\frac{S^+}{S^-}\right)^{\!2} Q^-}}, \qquad \beta = -\alpha \cdot \frac{S^+}{S^-}.
    \label{eq:alpha-beta}
\end{equation}
\end{proposition}

\begin{proof}
The zero-mean constraint gives $\beta = -\alpha \cdot S^+/S^-$. Substituting into the unit-variance constraint:
$\alpha^2 Q^+ + \alpha^2 (S^+/S^-)^{2} Q^- = N$,
yielding $\alpha^2 = N / (Q^+ + (S^+/S^-)^2 Q^-)$. Since $Q^+, Q^-, N > 0$ when both $\mathcal{P}$ and $\mathcal{N}$ are non-empty, $\alpha > 0$. Since $S^+ > 0$ and $S^- < 0$, $\beta = -\alpha S^+/S^- > 0$.
\end{proof}

\begin{proposition}[Advantage Sign Preservation]
\label{prop:gradient-direction}
The adaptive normalization preserves the sign of the advantage at every token: $\mathrm{sign}(\hat{A}_{i,t}) = \mathrm{sign}(A_{i,t})$ for all $(i,t)$ with $A_{i,t} \neq 0$. The relative ordering of magnitudes within $\mathcal{P}$ and within $\mathcal{N}$ is also preserved. Consequently, the policy gradient update reinforces (or penalizes) the same tokens as the unnormalized advantage.
\end{proposition}

\begin{proof}
Since $\alpha > 0$ and $\beta > 0$ (\Cref{prop:alpha-beta}), multiplication by a positive constant preserves both sign and relative ordering.
\end{proof}

\begin{remark}[Connection to asymmetric clipping]
Adaptive normalization is a soft analog of PPO's asymmetric clipping. PPO clips the importance ratio differently depending on the advantage sign; adaptive normalization scales the advantage magnitude differently. Both control the effective step size asymmetrically, but normalization acts through the advantage rather than the ratio, avoiding the gradient leakage problem identified by~\citet{li2025trm}.
\end{remark}

\begin{remark}[Uniform Scale Mode]
When all $n$ samples share the same reward, the leave-one-out baseline yields zero advantages, eliminating gradient signal. An optional \emph{uniform scale} mode recovers this signal by setting $\tilde{r}_i = r_i/n$ and skipping normalization, improving sample utilization. See \Cref{app:uniform-scale} for details.
\end{remark}


\section{REINFORCE Pro: Prefix Cumulative IS Correction}
\label{sec:reinforce-pro}

\subsection{The Off-Policy Correction Problem}
\label{sec:offpolicy-problem}

The rollout policy $\piroll$ (vLLM) and the actor policy $\piold$ (training framework) differ due to backend discrepancies, MoE routing, and distributed staleness. The per-token IS log-ratio is $\ell_t = \log \piold(y_t|c_t) - \log \piroll(y_t|c_t)$. The corrected loss takes the form:
\begin{equation}
    \mathcal{L} = \E_{\piroll}\!\Big[\phi(\ell_1, \ldots, \ell_T) \cdot \mathcal{L}^{\mathrm{PPO}}\Big],
\end{equation}
where $\phi$ is a correction function (detached from the computation graph) and $\mathcal{L}^{\mathrm{PPO}}$ is the standard PPO clipped loss using ratio $\rho_t^{\mathrm{PPO}} = \pitheta(y_t|c_t)/\piold(y_t|c_t)$.

\subsection{Limitations of Existing Methods}
\label{sec:is-limitations}

\begin{proposition}[Token-Level IS Ignores Causal Structure]
\label{prop:token-is-fails}
Consider a sequence where $w_1 = \exp(\ell_1) \gg 1$ but $w_t \approx 1$ for $t > 1$. Token-level IS (TIS, ICEPOP) applies correction only at position~1, leaving positions $2, \ldots, T$ uncorrected. However, the context $c_2 = (x, y_1)$ was sampled from $\piroll$, while under $\piold$ the distribution over $y_1$ differs substantially. The entire future trajectory is off-policy, but token-level IS does not detect this.
\end{proposition}

\begin{proof}
Under ICEPOP, $\mathrm{mask}_t = \mathbb{I}[w_t \in [\lambda, \Lambda]]$. Since $w_t \approx 1$ for $t > 1$, all future tokens pass the filter. Yet the context distribution shift at position $t$ satisfies $\|d_t^{\piold} - d_t^{\piroll}\|_{\mathrm{TV}} > 0$ by the context shift propagation (\Cref{lem:coupling}), so the loss at these positions is computed under a mismatched context.
\end{proof}

\begin{proposition}[Sequence-Level IS Loses Positional Information]
\label{prop:seq-is-fails}
For a sequence of length $T$ where $|\ell_t| \le \delta$ for all $t \neq t^*$ and $|\ell_{t^*}| = L$, the sequence-level geometric mean satisfies:
\begin{equation}
    \exp\!\Big(\frac{1}{T}\sum_{t=1}^T \ell_t\Big) \approx \exp\!\Big(\frac{L}{T}\Big).
\end{equation}
For $T = 4096$ and $L = 3$, this gives $\approx 1.0007$, well within any reasonable threshold, causing the deviation to go undetected.
\end{proposition}

\begin{proof}
Under the stated conditions, $|\sum_{t \neq t^*} \ell_t| \le (T-1)\delta$. Thus $\frac{1}{T}\sum_t \ell_t = \frac{L + O((T-1)\delta)}{T} = \frac{L}{T} + O(\delta)$. For $\delta \to 0$, the geometric mean converges to $\exp(L/T)$, which approaches $1$ as $T \to \infty$ regardless of $L$.
\end{proof}

\subsection{Prefix Cumulative IS: Definition and Properties}
\label{sec:prefix-is}

\begin{definition}[Prefix Cumulative IS]
\label{def:prefix-is}
For each position $t$ with action mask $m_t \in \{0, 1\}$, define:
\begin{align}
    L_t &= \sum_{s=1}^{t} \ell_s \cdot m_s, \qquad P_t = \sum_{s=1}^{t} m_s, \label{eq:prefix-cumsum} \\
    \mathrm{prefix\_is}(t) &= \exp\!\Big(\frac{L_t}{P_t}\Big), \label{eq:prefix-is}
\end{align}
where $P_t \ge 1$. This is the geometric mean of IS ratios over active positions up to $t$.
\end{definition}

\begin{definition}[Prefix IS Masking and Loss]
Given thresholds $[\lambda, \Lambda]$:
\begin{align}
    M_t^{\mathrm{prefix}} &= \mathbb{I}\!\big[\mathrm{prefix\_is}(t) \in [\lambda, \Lambda]\big], \label{eq:prefix-mask} \\
    \mathcal{L} &= \sum_t M_t^{\mathrm{prefix}} \cdot \exp(\ell_t) \cdot \mathcal{L}_t^{\mathrm{PPO}}, \label{eq:prefix-loss}
\end{align}
where $\exp(\ell_t)$ is the detached token-level IS weight and $\mathcal{L}_t^{\mathrm{PPO}}$ is the per-token PPO clipped loss.
\end{definition}

Three structural properties distinguish prefix IS from existing methods:

\textbf{Causality.} $\mathrm{prefix\_is}(t)$ depends only on $\{\ell_s\}_{s \le t}$, respecting the autoregressive order.

\textbf{Stability under in-bound increments.} If $L_t/P_t \in [\log\lambda, \log\Lambda]$ and the next active log-ratio $\ell_{t+1}$
also lies in $[\log\lambda, \log\Lambda]$, then $L_{t+1}/P_{t+1} \in [\log\lambda, \log\Lambda]$. Hence, once the prefix average
re-enters the threshold interval, it cannot leave it unless a subsequent per-token log-ratio is itself out of bounds.

\textbf{Granularity.} Different tokens in the same sequence are independently masked based on cumulative divergence up to that point.

\begin{theorem}[Causality-Aware Masking with Conditional Dominance]
\label{thm:prefix-tighter}
Let $M^{\mathrm{prefix}}_t$, $M^{\mathrm{token}}_t$, and $M^{\mathrm{seq}}_t$ denote the masks produced by prefix cumulative IS, token-level IS (ICEPOP), and sequence-level IS (seq-mask-tis) respectively, all with the same threshold $[\lambda, \Lambda]$ where $0 < \lambda \le 1 \le \Lambda$. Assume policy support overlap (\Cref{asm:support}). Then:

\begin{enumerate}
    \item[(a)] \textbf{Early deviation: prefix masks a (possibly longer) prefix.} Assume $w_1 \notin [\lambda, \Lambda]$ and
    $|\ell_t| \le \epsilon$ for all $t>1$, where $0 \le \epsilon \le \min(\log\Lambda, |\log\lambda|)$. Then token-level ICEPOP masks
    only position $t=1$, while prefix IS masks at least position $t=1$. Moreover, if $\ell_1>\log\Lambda$ then for every position $t$
    with $P_t \ge 1$ satisfying
    \begin{equation}
        P_t < \frac{\ell_1+\epsilon}{\log\Lambda+\epsilon},
        \label{eq:early-upper-sufficient}
    \end{equation}
    we have $M_t^{\mathrm{prefix}}=0$. Symmetrically, if $\ell_1<\log\lambda$ then for every position $t$ with $P_t \ge 1$ satisfying
    \begin{equation}
        P_t < \frac{-\ell_1+\epsilon}{|\log\lambda|+\epsilon},
        \label{eq:early-lower-sufficient}
    \end{equation}
    we have $M_t^{\mathrm{prefix}}=0$. In particular, in the early-deviation regime prefix IS masks at least as many tokens as
    token-level ICEPOP.

    \item[(b)] \textbf{Late deviation: prefix is more local than sequence masking.} Suppose there exists $t^*\in\{1,\ldots,T\}$ such that
    $|\ell_t| \le \epsilon$ for all $t\neq t^*$ and $w_{t^*}\notin[\lambda,\Lambda]$, where $0 \le \epsilon \le \min(\log\Lambda,
    |\log\lambda|)$. Then prefix IS can only mask tokens at positions $t \ge t^*$, so
    \begin{equation}
        \big|\{t : M_t^{\mathrm{prefix}} = 0\}\big| \le T - t^* + 1.
    \end{equation}
    In contrast, sequence-level masking is all-or-nothing: $|\{t : M_t^{\mathrm{seq}} = 0\}| \in \{0, T\}$. Moreover, in the pure
    single-spike case $\ell_{t^*}=L$ and $\ell_t=0$ for $t\neq t^*$, one has $\frac{L}{t^*} \ge \frac{L}{T}$, so there exist regimes
    where $M_{t^*}^{\mathrm{prefix}}=0$ while $M_t^{\mathrm{seq}}=1$ for all $t$ (sequence geometric mean diluted by large $T$).

    \item[(c)] \textbf{Monotone violation: all methods reject.} If $\ell_s > \log\Lambda$ for all $s$ (upper violation), or
    $\ell_s < \log\lambda$ for all $s$ (lower violation), then $M_t^{\mathrm{prefix}} = 0$ and $M_t^{\mathrm{seq}} = 0$ for all $t$,
    so both methods reject all $T$ tokens.
\end{enumerate}
\end{theorem}

\begin{proof}
The full proof is in \Cref{app:prefix-proof}. We highlight the main steps.

\textbf{Part (a).} The $\epsilon$-bounded condition implies $\ell_t \in [\log\lambda, \log\Lambda]$ for all $t>1$, hence ICEPOP masks
only $t=1$. For prefix IS, the bounds $\sum_{s=2}^{t}\ell_s m_s \ge -(P_t-1)\epsilon$ and $\sum_{s=2}^{t}\ell_s m_s \le (P_t-1)\epsilon$
yield the sufficient conditions \Cref{eq:early-upper-sufficient,eq:early-lower-sufficient}.

\textbf{Part (b).} For $t<t^*$, the prefix statistic averages only in-bound log-ratios and stays in-bound, so masking can only occur at
positions $t\ge t^*$, yielding the stated bound. The single-spike comparison follows from $t^*\le T$.

\textbf{Part (c).} If each $\ell_s$ is strictly beyond the same threshold, then their running average is also strictly beyond the
threshold, so both prefix and sequence masks reject all tokens.
\end{proof}

\begin{remark}[Same-sign but in-bound deviations]
When all $\ell_s$ share the same sign but do not all exceed the corresponding threshold, the running average $L_t/P_t$ need not be
monotone in $t$ and may return within bounds after crossing a threshold. In this regime there is no unconditional dominance ordering
between prefix masking and token-level masking: token-level methods detect large single-token outliers, while prefix masking detects
cumulative drift from many moderate deviations.
\end{remark}

\subsection{Connection to the Adaptive Bound}
\label{sec:adaptive-connection}

We relate prefix cumulative IS to the per-position, causal structure that appears in trust-region style error bounds for long-horizon
LLM-RL~\citep{li2025trm}. This subsection focuses on the rollout/training mismatch $\piold/\piroll$; relating it to $\pitheta$ requires
additional assumptions on the PPO update and is discussed in \Cref{sec:unified}.

\begin{lemma}[KL chain rule for the cumulative log-ratio]
\label{lem:cumsum-kl}
Under Assumptions~\ref{asm:support}--\ref{asm:finite-length}, for any $t \le T$,
\begin{equation}
    \E_{\piroll}\!\Big[\sum_{s=1}^{t} \ell_s\Big]
    = -\sum_{s=1}^{t} \E_{c_s \sim d_s^{\piroll}}\!\big[\DKL(\piroll(\cdot|c_s) \| \piold(\cdot|c_s))\big]
    = -\DKL(d_{t+1}^{\piroll} \| d_{t+1}^{\piold}),
    \label{eq:cumsum-kl}
\end{equation}
where the final equality is the KL chain rule applied to the joint prefix distributions.
\end{lemma}

\begin{proof}
For each context $c_s$, the conditional expectation under $y_s \sim \piroll(\cdot|c_s)$ satisfies
\[
    \E[\ell_s \mid c_s]
    = \sum_{v}\piroll(v|c_s)\log\frac{\piold(v|c_s)}{\piroll(v|c_s)}
    = -\DKL(\piroll(\cdot|c_s)\|\piold(\cdot|c_s)).
\]
Taking expectation over $c_s \sim d_s^{\piroll}$ and summing over $s \le t$ yields the first equality in \eqref{eq:cumsum-kl}. The
second equality is the KL chain rule for autoregressive factorizations, identifying the sum of per-step conditional KL terms with the
joint KL between prefix distributions $d_{t+1}^{\piroll}$ and $d_{t+1}^{\piold}$.
\end{proof}

\begin{remark}[Prefix filtering as a sample-level proxy]
\label{rem:prefix-proxy}
Thresholding $\mathrm{prefix\_is}(t)$ is equivalent to a per-position, sample-level constraint on the prefix average log-ratio:
\begin{equation}
    \mathrm{prefix\_is}(t) \in [\lambda, \Lambda]
    \iff \frac{1}{P_t}\sum_{s=1}^{t}\ell_s m_s \in [\log\lambda, \log\Lambda].
    \label{eq:threshold-kl}
\end{equation}
In contrast, \Cref{lem:cumsum-kl} is an \emph{expectation} identity over the rollout distribution. Therefore, the threshold condition
should be interpreted as a practical, sample-level proxy for trust-region style filtering rather than a population-level guarantee
without additional assumptions. Separately, Pinsker's inequality relates the population-level KL in \eqref{eq:cumsum-kl} to a TV bound:
\begin{equation}
    \|d_{t+1}^{\piold} - d_{t+1}^{\piroll}\|_{\mathrm{TV}}
    \le \sqrt{\frac{\DKL(d_{t+1}^{\piroll} \| d_{t+1}^{\piold})}{2}}.
    \label{eq:context-shift-prefix}
\end{equation}
\end{remark}

\begin{corollary}[Per-Position Sample-Level Trust Region Constraint]
\label{cor:per-position-trust}
Under prefix IS masking with threshold $[\lambda, \Lambda]$, for all accepted tokens ($M_t^{\mathrm{prefix}} = 1$), the sample-level cumulative log-ratio satisfies:
\begin{equation}
    \frac{1}{P_t}\sum_{s=1}^{t} \big(\log \piold(y_s|c_s) - \log \piroll(y_s|c_s)\big) m_s \in [\log\lambda, \log\Lambda].
\end{equation}
This provides a per-position, sample-level trust region constraint analogous to the per-position structure in the Adaptive bound. The constraint operates on individual trajectories; its connection to the population-level KL is through the expectation identity in Eq.~\eqref{eq:cumsum-kl}.
\end{corollary}

\begin{proof}
Follows directly from \Cref{def:prefix-is} and Eq.~\eqref{eq:prefix-mask}: $M_t^{\mathrm{prefix}} = 1$ iff $\mathrm{prefix\_is}(t) \in [\lambda, \Lambda]$ iff $L_t/P_t \in [\log\lambda, \log\Lambda]$.
\end{proof}

\subsection{The Combined Loss}
\label{sec:combined-loss}

We define the loss as the negative surrogate objective, $\mathcal{L} = -L_{\piroll}(\pitheta)$, so that gradient descent on $\mathcal{L}$ corresponds to maximizing the surrogate. The full REINFORCE Pro Max loss combines both components:
\begin{equation}
    \mathcal{L}^{\mathrm{ProMax}} = -\sum_t M_t^{\mathrm{prefix}} \cdot \exp(\ell_t) \cdot \min\!\big(\rho_t^{\mathrm{PPO}} \hat{A}_t,\; \mathrm{clip}(\rho_t^{\mathrm{PPO}}, 1\!-\!\epsilon_c, 1\!+\!\epsilon_c)\, \hat{A}_t\big),
    \label{eq:promax-loss}
\end{equation}
where $\hat{A}_t$ is the adaptively normalized advantage from \Cref{sec:adaptive-norm}, $M_t^{\mathrm{prefix}}$ is the prefix IS mask, $\exp(\ell_t)$ is the detached token-level IS weight, and $\rho_t^{\mathrm{PPO}} = \pitheta(y_t|c_t)/\piold(y_t|c_t)$ is the standard PPO ratio (which carries gradients). The negative sign ensures that minimizing $\mathcal{L}^{\mathrm{ProMax}}$ maximizes the surrogate objective.

\textbf{Gradient flow.} Gradients propagate only through $\rho_t^{\mathrm{PPO}}$. The prefix mask $M_t^{\mathrm{prefix}}$, IS weight $\exp(\ell_t)$, and normalized advantage $\hat{A}_t$ are all detached from the computation graph. When $M_t^{\mathrm{prefix}} = 0$, the loss at position $t$ is zero and contributes no gradient.
