\section{REINFORCE Max: Variance-Reduced Advantage Estimation}
\label{sec:reinforce-max}

Given a prompt $x$, we sample $n$ responses $\{y^{(1)}, \ldots, y^{(n)}\}$ with rewards $\{r_1, \ldots, r_n\}$. Each response
$y^{(i)}$ consists of $T_i$ action tokens. REINFORCE Max combines three ingredients: a leave-one-out baseline
(\Cref{eq:rloo-baseline}), token-level expansion (\Cref{sec:token-expand}), and adaptive normalization (\Cref{sec:adaptive-norm}).

We adopt the leave-one-out (RLOO) baseline~\citep{ahmadian2024back} for its statistical independence, which avoids correlated baseline artifacts present in the mean baseline (see \Cref{app:rloo-proof} for a detailed analysis).

\begin{definition}[Leave-One-Out Baseline]
\label{def:rloo}
Assume $n \ge 2$. For sample $i$, the baseline and shaped reward are:
\begin{equation}
    b_i = \frac{1}{n-1}\sum_{j \neq i} r_j, \qquad \tilde{r}_i = r_i - b_i.
    \label{eq:rloo-baseline}
\end{equation}
\end{definition}

\subsection{Token-Level Expansion}
\label{sec:token-expand}

Since the reward is a scalar per response and we use $\gamma = 1.0$ (see \Cref{app:gamma-one} for a formal justification), the token-level advantage is simply the shaped reward itself:
\begin{equation}
    A_{i,t} = \tilde{r}_i = r_i - b_i, \quad \forall\, t \in \{1, \ldots, T_i\}.
    \label{eq:token-advantage}
\end{equation}

\subsection{Adaptive Asymmetric Normalization}
\label{sec:adaptive-norm}

Standard normalization $\hat{A} = (A - \mu)/\sigma$ applies a uniform scaling to all advantages. In reasoning tasks, the advantage distribution is typically highly skewed: most samples fail (negative advantages) while few succeed (positive advantages). Uniform scaling distorts the relative balance between reinforcement and punishment signals.

\begin{definition}[Adaptive Normalization]
\label{def:adaptive-norm}
Fix a prompt group with token-level advantages $\{A_{i,t}\}$, and exclude zero-advantage positions from the normalization statistics.
Define the index sets and summary statistics
\begin{align}
    \mathcal{P}
    &:= \{(i,t) : A_{i,t} > 0\},
    &
    \mathcal{N}
    &:= \{(i,t) : A_{i,t} < 0\},
    \\
    S^+
    &:= \sum_{(i,t) \in \mathcal{P}} A_{i,t},
    &
    S^-
    &:= \sum_{(i,t) \in \mathcal{N}} A_{i,t},
    \\
    Q^+
    &:= \sum_{(i,t) \in \mathcal{P}} A_{i,t}^2,
    &
    Q^-
    &:= \sum_{(i,t) \in \mathcal{N}} A_{i,t}^2,
    \\
    N
    &:= |\mathcal{P}| + |\mathcal{N}|.
\end{align}
The normalized advantage is defined by a sign-dependent rescaling:
\begin{equation}
    \hat{A}_{i,t} = \begin{cases} \alpha \cdot A_{i,t} & \text{if } A_{i,t} > 0, \\ \beta \cdot A_{i,t} & \text{if } A_{i,t} < 0, \\ 0 & \text{if } A_{i,t} = 0, \end{cases}
    \label{eq:adaptive-norm}
\end{equation}
where $\alpha,\beta>0$ are chosen to satisfy the following \emph{empirical} constraints over non-zero tokens:
\begin{align}
    \frac{1}{N}\sum_{(i,t)\in\mathcal{P}\cup\mathcal{N}}\hat{A}_{i,t} &= 0,
    \label{eq:adaptive-mean}
    \\
    \frac{1}{N}\sum_{(i,t)\in\mathcal{P}\cup\mathcal{N}}\hat{A}_{i,t}^2 &= 1.
    \label{eq:adaptive-var}
\end{align}
Equivalently, these constraints can be written as
\begin{equation}
    \alpha S^+ + \beta S^- = 0,
    \qquad
    \alpha^2 Q^+ + \beta^2 Q^- = N.
\end{equation}
\end{definition}

\begin{proposition}[Closed-Form Solution]
\label{prop:alpha-beta}
Assume $\mathcal{P} \neq \emptyset$ and $\mathcal{N} \neq \emptyset$. The unique solution satisfying $\alpha S^+ + \beta S^- = 0$ and $\alpha^2 Q^+ + \beta^2 Q^- = N$ is:
\begin{equation}
    \alpha = \sqrt{\frac{N}{Q^+ + \left(\frac{S^+}{S^-}\right)^{\!2} Q^-}}, \qquad \beta = -\alpha \cdot \frac{S^+}{S^-}.
    \label{eq:alpha-beta}
\end{equation}
\end{proposition}

\begin{proof}
We solve the two scalar constraints in \Cref{def:adaptive-norm} and then enforce $\alpha,\beta>0$.

\textbf{Step 1 (zero empirical mean).} The constraint $\alpha S^+ + \beta S^- = 0$ implies
\begin{equation}
    \beta = -\alpha \cdot \frac{S^+}{S^-}.
    \label{eq:beta-from-mean}
\end{equation}
Since $\mathcal{P}\neq\emptyset$ and $\mathcal{N}\neq\emptyset$, we have $S^+>0$ and $S^-<0$, so $S^-/S^+$ is well-defined.

\textbf{Step 2 (unit empirical second moment).} Substituting \eqref{eq:beta-from-mean} into $\alpha^2 Q^+ + \beta^2 Q^- = N$ yields
\begin{align}
    \alpha^2 Q^+ + \beta^2 Q^-
    &= N,
    \\
    \alpha^2 Q^+ + \alpha^2\Big(\frac{S^+}{S^-}\Big)^{\!2} Q^-
    &= N,
    \\
    \alpha^2
    &= \frac{N}{Q^+ + \left(\frac{S^+}{S^-}\right)^{2} Q^-}.
\end{align}
The denominator is strictly positive because $Q^+>0$, $Q^->0$, and $N>0$ under the same assumption. Taking the positive square root
gives the stated $\alpha>0$, and then \eqref{eq:beta-from-mean} yields the stated $\beta$.

\textbf{Uniqueness.} The mean constraint uniquely specifies the ratio $\beta/\alpha$, and the second-moment constraint uniquely
specifies $\alpha^2$. Enforcing $\alpha>0$ and $\beta>0$ therefore yields a unique solution.
\end{proof}

\begin{proposition}[Advantage Sign Preservation]
\label{prop:gradient-direction}
The adaptive normalization preserves the sign of the advantage at every token: $\sign(\hat{A}_{i,t}) = \sign(A_{i,t})$ for all $(i,t)$
with $A_{i,t} \neq 0$. The relative ordering of magnitudes within $\mathcal{P}$ and within $\mathcal{N}$ is also preserved.
Consequently, the policy gradient update reinforces (or penalizes) the same tokens as the unnormalized advantage.
\end{proposition}

\begin{proof}
Since $\alpha > 0$ and $\beta > 0$ (\Cref{prop:alpha-beta}), multiplication by a positive constant preserves both sign and relative ordering.
\end{proof}

\begin{remark}[Connection to asymmetric clipping]
Adaptive normalization is a soft analog of PPO's asymmetric clipping. PPO clips the importance ratio differently depending on the advantage sign; adaptive normalization scales the advantage magnitude differently. Both control the effective step size asymmetrically, but normalization acts through the advantage rather than the ratio, avoiding the gradient leakage problem identified by~\citet{li2025trm}.
\end{remark}

\begin{remark}[Uniform Scale Mode]
When all $n$ samples share the same reward, the leave-one-out baseline yields zero advantages, eliminating gradient signal. An optional \emph{uniform scale} mode recovers this signal by setting $\tilde{r}_i = r_i/n$ and skipping normalization, improving sample utilization. See \Cref{app:uniform-scale} for details.
\end{remark}


\section{REINFORCE Pro: Prefix-Cumulative Ratio Masking}
\label{sec:reinforce-pro}

\subsection{Two-Policy Ratio Setup}
\label{sec:offpolicy-problem}

We adopt the standard \emph{two-policy} view of PPO-style LLM-RL: samples are generated by a fixed rollout policy $\piroll$, and the
training policy $\pitheta$ is updated from $\piroll$ by gradient steps. This matches the trust-region setup in~\citet{li2025trm}.

The per-token importance ratio between these two policies is
\begin{equation}
    \rho_t = \frac{\pitheta(y_t \mid c_t)}{\piroll(y_t \mid c_t)},
\end{equation}
as defined in \eqref{eq:ratio}. Throughout this section, we restrict attention to masking strategies constructed solely from
$(\rho_t)_{t=1}^T$ (or equivalently, the log-ratios $r_t=\log\rho_t$ in \eqref{eq:log-ratio}), and we do not introduce additional
intermediate policies.

\subsection{Limitations of Existing Methods}
\label{sec:is-limitations}

\begin{proposition}[Token-Level IS Ignores Causal Structure]
\label{prop:token-is-fails}
Fix thresholds $0<\lambda\le 1\le \Lambda$. Consider a length-$T$ trajectory such that
\begin{equation}
    \rho_1 \notin [\lambda,\Lambda],
    \qquad
    \rho_t \in [\lambda,\Lambda]\ \text{ for all }\ t\in\{2,\ldots,T\}.
    \label{eq:token-early-deviation}
\end{equation}
Then the token-level ICEPOP mask,
\begin{equation}
    M_t^{\mathrm{token}} := \mathbb{I}\!\big[\rho_t \in [\lambda,\Lambda]\big],
\end{equation}
masks only the first position. In particular, it does not prevent later positions $t\ge 2$ from contributing gradients even though the
contexts $(c_t)_{t\ge2}$ are sampled under the behavior distribution induced by $\piroll$, which differs from the context distribution that
would be induced by the updated policy $\pitheta$ when $\rho_1 \neq 1$.
\end{proposition}

\begin{proof}
By definition, \eqref{eq:token-early-deviation} implies $M_1^{\mathrm{token}}=0$ and $M_t^{\mathrm{token}}=1$ for all $t\ge2$.

On the other hand, since $\rho_1 \neq 1$, we have $\pitheta(y_1\mid c_1) \neq \piroll(y_1\mid c_1)$, hence the conditional token
distributions $\pitheta(\cdot\mid c_1)$ and $\piroll(\cdot\mid c_1)$ are not equal. Therefore $\Dtvtok(c_1;\piroll,\pitheta)>0$.
Applying the context-shift propagation lemma (\Cref{lem:coupling}) with $(\pi,\pi')=(\piroll,\pitheta)$ and $s=1$ yields
\begin{equation}
    \|d_t^{\piroll} - d_t^{\pitheta}\|_{\mathrm{TV}} > 0
    \qquad
    \text{for all } t \ge 2.
\end{equation}
Therefore, even though ICEPOP masks the local deviation at $t=1$, the remaining accepted tokens $t\ge2$ are still evaluated under
mismatched context visitation distributions, which is precisely the causal mismatch that token-independent masking fails to address.
\end{proof}

\begin{proposition}[Sequence-Level IS Loses Positional Information]
\label{prop:seq-is-fails}
Let $\bar{r}$ denote the sequence-level average log-ratio,
\begin{equation}
    \bar{r} := \frac{1}{T}\sum_{t=1}^T r_t.
\end{equation}
Assume there exists $t^*\in\{1,\ldots,T\}$ such that $|r_{t^*}| = L$ and $|r_t|\le \varepsilon$ for all $t\neq t^*$. Then
\begin{equation}
    \big|\bar{r} - L/T\big| \le \frac{T-1}{T}\,\varepsilon \le \varepsilon.
    \label{eq:seq-mean-dilution}
\end{equation}
In particular, in the pure single-spike case $r_{t^*}=L$ and $r_t=0$ for $t\neq t^*$, one has $\bar{r}=L/T$, so
$\exp(\bar{r})=\exp(L/T)\to 1$ as $T\to\infty$ for any fixed $L$.
\end{proposition}

\begin{proof}
Write
\begin{equation}
    \bar{r}
    = \frac{1}{T}r_{t^*} + \frac{1}{T}\sum_{t\neq t^*}r_t.
\end{equation}
By the triangle inequality and the bound $|r_t|\le\varepsilon$ for $t\neq t^*$,
\begin{equation}
    \big|\bar{r} - r_{t^*}/T\big|
    = \frac{1}{T}\Big|\sum_{t\neq t^*}r_t\Big|
    \le \frac{T-1}{T}\,\varepsilon,
\end{equation}
which implies \eqref{eq:seq-mean-dilution}. The single-spike specialization follows by setting $r_t=0$ for $t\neq t^*$.
\end{proof}

\begin{example}[Numerical dilution at long horizons]
Consider the pure single-spike case with $T=4096$, $r_{t^*}=3$, and $r_t=0$ for $t\neq t^*$. Then
\begin{equation}
    \exp(\bar{r}) = \exp(3/4096) \approx 1.00073,
\end{equation}
which can easily fall inside practical acceptance intervals $[\lambda,\Lambda]$ even though the local deviation at $t^*$ is large in
log-ratio magnitude.
\end{example}

\subsection{Prefix Cumulative IS: Definition and Properties}
\label{sec:prefix-is}

\begin{definition}[Prefix Cumulative IS]
\label{def:prefix-is}
For a trajectory with action mask $m_t \in \{0,1\}$ and per-token log-ratios $r_t$ defined in \eqref{eq:log-ratio}, define the
cumulative sums
\begin{align}
    L_t &= \sum_{s=1}^{t} r_s \cdot m_s, \qquad P_t = \sum_{s=1}^{t} m_s, \label{eq:prefix-cumsum} \\
    \bar{r}_t &= \frac{L_t}{P_t}, \qquad \mathrm{prefix\_is}(t) = \exp(\bar{r}_t), \label{eq:prefix-is}
\end{align}
where $\bar{r}_t$ is defined only for $t$ such that $P_t\ge 1$ (i.e., the prefix contains at least one active action token). The
statistic $\mathrm{prefix\_is}(t)$ is the geometric mean of IS ratios over active positions up to $t$.
\end{definition}

\begin{definition}[Prefix IS Masking]
Given thresholds $[\lambda, \Lambda]$:
\begin{equation}
    M_t^{\mathrm{prefix}} := \mathbb{I}\!\big[\mathrm{prefix\_is}(t) \in [\lambda, \Lambda]\big].
    \label{eq:prefix-mask}
\end{equation}
\end{definition}

\begin{remark}[Action-token indexing]
\label{rem:action-indexing}
For the theoretical analysis, we may assume (without loss of generality) that we index only over active action-token positions and that the
trajectory is not padded. In this case, one can take $m_t\equiv 1$ and $P_t=t$. We retain the mask notation to match the batched,
variable-length implementation.
\end{remark}

Three structural properties distinguish prefix IS from existing methods:

\textbf{Causality.} $\mathrm{prefix\_is}(t)$ depends only on $\{r_s\}_{s \le t}$, respecting the autoregressive order.

\textbf{Stability under in-bound increments.} The following lemma formalizes a basic stability property of prefix averages.

\begin{lemma}[Stability of prefix averages]
\label{lem:prefix-stability}
Fix thresholds $0<\lambda\le 1\le \Lambda$, and define $\bar{r}_t$ as in \eqref{eq:prefix-is}. Suppose $P_t\ge 1$ and
$\bar{r}_t\in[\log\lambda,\log\Lambda]$. If the next position is active ($m_{t+1}=1$) and satisfies
$r_{t+1}\in[\log\lambda,\log\Lambda]$, then $\bar{r}_{t+1}\in[\log\lambda,\log\Lambda]$.
\end{lemma}

\begin{proof}
When $m_{t+1}=1$, we have $L_{t+1}=L_t+r_{t+1}$ and $P_{t+1}=P_t+1$. Therefore
\begin{equation}
    \bar{r}_{t+1}
    = \frac{L_{t+1}}{P_{t+1}}
    = \frac{P_t}{P_t+1}\,\bar{r}_t + \frac{1}{P_t+1}\,r_{t+1}.
\end{equation}
The coefficients are nonnegative and sum to one, so $\bar{r}_{t+1}$ is a convex combination of two numbers in
$[\log\lambda,\log\Lambda]$. Hence $\bar{r}_{t+1}\in[\log\lambda,\log\Lambda]$.
\end{proof}

\textbf{Granularity.} Different tokens in the same sequence are independently masked based on cumulative divergence up to that point.

\noindent\textbf{Notation.} To avoid overloading symbols, we reserve $(\epsilon,\delta)$ for the divergence quantities in
\Cref{sec:preliminaries}. In the deviation-pattern analysis below, we use $\varepsilon$ to denote a uniform bound on the per-token
log-ratios $r_t$.

\begin{theorem}[Causality-Aware Masking with Conditional Dominance]
\label{thm:prefix-tighter}
Let $M^{\mathrm{prefix}}_t$, $M^{\mathrm{token}}_t$, and $M^{\mathrm{seq}}_t$ denote the masks produced by prefix cumulative IS, token-level IS (ICEPOP), and sequence-level IS (seq-mask-tis) respectively, all with the same threshold $[\lambda, \Lambda]$ where $0 < \lambda \le 1 \le \Lambda$. Assume policy support overlap (\Cref{asm:support}). Then:

\begin{enumerate}
    \item[(a)] \textbf{Early deviation: prefix masks a (possibly longer) prefix.} Assume $\rho_1 \notin [\lambda, \Lambda]$ and
    $|r_t| \le \varepsilon$ for all $t>1$, where $0 \le \varepsilon \le \min(\log\Lambda, |\log\lambda|)$. Then token-level ICEPOP masks
    only position $t=1$, while prefix IS masks at least position $t=1$. Moreover, if $r_1>\log\Lambda$ then for every position $t$
    with $P_t \ge 1$ satisfying
    \begin{equation}
        P_t < \frac{r_1+\varepsilon}{\log\Lambda+\varepsilon},
        \label{eq:early-upper-sufficient}
    \end{equation}
    we have $M_t^{\mathrm{prefix}}=0$. Symmetrically, if $r_1<\log\lambda$ then for every position $t$ with $P_t \ge 1$ satisfying
    \begin{equation}
        P_t < \frac{-r_1+\varepsilon}{|\log\lambda|+\varepsilon},
        \label{eq:early-lower-sufficient}
    \end{equation}
    we have $M_t^{\mathrm{prefix}}=0$. In particular, in the early-deviation regime prefix IS masks at least as many tokens as
    token-level ICEPOP.

    \item[(b)] \textbf{Late deviation: prefix is more local than sequence masking.} Suppose there exists $t^*\in\{1,\ldots,T\}$ such that
    $|r_t| \le \varepsilon$ for all $t\neq t^*$ and $\rho_{t^*}\notin[\lambda,\Lambda]$, where $0 \le \varepsilon \le \min(\log\Lambda,
    |\log\lambda|)$. Then prefix IS can only mask tokens at positions $t \ge t^*$, so
    \begin{equation}
        \big|\{t : M_t^{\mathrm{prefix}} = 0\}\big| \le T - t^* + 1.
    \end{equation}
    In contrast, sequence-level masking is all-or-nothing: $|\{t : M_t^{\mathrm{seq}} = 0\}| \in \{0, T\}$. Moreover, in the pure
    single-spike case $r_{t^*}=L$ and $r_t=0$ for $t\neq t^*$, one has $\frac{L}{P_{t^*}} \ge \frac{L}{P_T}$, so there exist regimes
    where $M_{t^*}^{\mathrm{prefix}}=0$ while $M_t^{\mathrm{seq}}=1$ for all $t$ (sequence geometric mean diluted by large $T$).

    \item[(c)] \textbf{Monotone violation: all methods reject.} If $r_s > \log\Lambda$ for all $s$ (upper violation), or
    $r_s < \log\lambda$ for all $s$ (lower violation), then $M_t^{\mathrm{prefix}} = 0$ and $M_t^{\mathrm{seq}} = 0$ for all $t$,
    so both methods reject all $T$ tokens.
\end{enumerate}
\end{theorem}

\begin{proof}[Proof sketch]
The full proof is in \Cref{app:prefix-proof}. We highlight the key steps.

\textbf{Part (a).} The $\varepsilon$-bounded condition implies $r_t \in [\log\lambda, \log\Lambda]$ for all $t>1$, hence ICEPOP masks
only $t=1$. For prefix IS, write $L_t=r_1+\sum_{s=2}^{t}r_s m_s$ and use the bounds
\begin{align}
    \sum_{s=2}^{t}r_s m_s &\ge -(P_t-1)\varepsilon,
    \\
    \sum_{s=2}^{t}r_s m_s &\le (P_t-1)\varepsilon,
\end{align}
which yield the sufficient conditions \Cref{eq:early-upper-sufficient,eq:early-lower-sufficient}.

\textbf{Part (b).} For $t<t^*$, all active log-ratios in the prefix satisfy $r_s\in[\log\lambda,\log\Lambda]$, so repeated
application of \Cref{lem:prefix-stability} implies $\bar{r}_t\in[\log\lambda,\log\Lambda]$ and hence
$M_t^{\mathrm{prefix}}=1$. Therefore masking can only occur at positions $t\ge t^*$, yielding the stated bound. The single-spike
comparison follows from $P_{t^*}\le P_T$.

\textbf{Part (c).} If each $r_s$ is strictly beyond the same threshold, then their running average is also strictly beyond the
threshold, so both prefix and sequence masks reject all tokens.
\end{proof}

\begin{remark}[Same-sign but in-bound deviations]
When all $r_s$ share the same sign but do not all exceed the corresponding threshold, the running average $L_t/P_t$ need not be
monotone in $t$ and may return within bounds after crossing a threshold. In this regime there is no unconditional dominance ordering
between prefix masking and token-level masking: token-level methods detect large single-token outliers, while prefix masking detects
cumulative drift from many moderate deviations.
\end{remark}

\subsection{Connection to the Adaptive Bound}
\label{sec:adaptive-connection}

We relate prefix cumulative IS to the per-position, causal structure that appears in trust-region style error bounds for long-horizon
LLM-RL~\citep{li2025trm}. In our notation, these bounds analyze the policy pair $(\piroll,\pitheta)$.

\begin{lemma}[KL chain rule for the cumulative log-ratio]
\label{lem:cumsum-kl}
Under Assumptions~\ref{asm:support}--\ref{asm:finite-length}, for any $t \le T$,
\begin{equation}
    \E_{\piroll}\!\Big[\sum_{s=1}^{t} r_s\Big]
    = -\sum_{s=1}^{t} \E_{c_s \sim d_s^{\piroll}}\!\big[\DKL(\piroll(\cdot|c_s) \| \pitheta(\cdot|c_s))\big]
    = -\DKL(d_{t+1}^{\piroll} \| d_{t+1}^{\pitheta}),
    \label{eq:cumsum-kl}
\end{equation}
where the final equality is the KL chain rule applied to the joint prefix distributions.
\end{lemma}

\begin{proof}
We start from the definition $r_s = \log \pitheta(y_s\mid c_s) - \log \piroll(y_s\mid c_s)$ in \eqref{eq:log-ratio}. Conditioning on
$c_s$ and taking expectation over $y_s\sim \piroll(\cdot\mid c_s)$ gives
\begin{align}
    \E_{\piroll}\!\big[r_s \mid c_s\big]
    &= \sum_{v} \piroll(v\mid c_s)\,\log\frac{\pitheta(v\mid c_s)}{\piroll(v\mid c_s)}
    \\
    &= - \sum_{v} \piroll(v\mid c_s)\,\log\frac{\piroll(v\mid c_s)}{\pitheta(v\mid c_s)}
    \\
    &= -\DKL\!\big(\piroll(\cdot\mid c_s)\,\|\,\pitheta(\cdot\mid c_s)\big).
\end{align}
Taking expectation over $c_s\sim d_s^{\piroll}$ and summing from $s=1$ to $t$ yields the first equality in \eqref{eq:cumsum-kl}.

For the second equality, apply the KL chain rule to the joint prefix distributions under the autoregressive factorizations:
\begin{equation}
    \DKL\!\big(d_{t+1}^{\piroll}\,\|\,d_{t+1}^{\pitheta}\big)
    = \sum_{s=1}^{t}\E_{c_s \sim d_s^{\piroll}}\!\Big[\DKL\!\big(\piroll(\cdot\mid c_s)\,\|\,\pitheta(\cdot\mid c_s)\big)\Big].
\end{equation}
Combining the two displays yields \eqref{eq:cumsum-kl}.
\end{proof}

\begin{remark}[Prefix filtering as a sample-level proxy]
\label{rem:prefix-proxy}
Thresholding $\mathrm{prefix\_is}(t)$ is equivalent to a per-position, sample-level constraint on the prefix average log-ratio:
\begin{equation}
    \mathrm{prefix\_is}(t) \in [\lambda, \Lambda]
    \iff \frac{1}{P_t}\sum_{s=1}^{t}r_s m_s \in [\log\lambda, \log\Lambda].
    \label{eq:threshold-kl}
\end{equation}
In contrast, \Cref{lem:cumsum-kl} is an \emph{expectation} identity over the behavior distribution. Therefore, the threshold condition
should be interpreted as a practical, sample-level proxy for trust-region style filtering rather than a population-level guarantee
without additional assumptions. Separately, Pinsker's inequality relates the population-level KL in \eqref{eq:cumsum-kl} to a TV bound:
\begin{equation}
    \|d_{t+1}^{\pitheta} - d_{t+1}^{\piroll}\|_{\mathrm{TV}}
    \le \sqrt{\frac{\DKL(d_{t+1}^{\piroll} \| d_{t+1}^{\pitheta})}{2}}.
    \label{eq:context-shift-prefix}
\end{equation}
\end{remark}

\begin{corollary}[Per-Position Sample-Level Trust Region Constraint]
\label{cor:per-position-trust}
Under prefix IS masking with threshold $[\lambda, \Lambda]$, for all accepted tokens ($M_t^{\mathrm{prefix}} = 1$), the sample-level cumulative log-ratio satisfies:
\begin{equation}
    \frac{1}{P_t}\sum_{s=1}^{t} \big(\log \pitheta(y_s|c_s) - \log \piroll(y_s|c_s)\big) m_s \in [\log\lambda, \log\Lambda].
\end{equation}
This provides a per-position, sample-level trust region constraint analogous to the per-position structure in the Adaptive bound. The constraint operates on individual trajectories; its connection to the population-level KL is through the expectation identity in Eq.~\eqref{eq:cumsum-kl}.
\end{corollary}

\begin{proof}
Follows directly from \Cref{def:prefix-is} and Eq.~\eqref{eq:prefix-mask}: $M_t^{\mathrm{prefix}} = 1$ iff $\mathrm{prefix\_is}(t) \in [\lambda, \Lambda]$ iff $L_t/P_t \in [\log\lambda, \log\Lambda]$.
\end{proof}

\subsection{The Combined Loss}
\label{sec:combined-loss}

We define the loss as the negative surrogate objective, $\mathcal{L} = -L_{\piroll}(\pitheta)$, so that gradient descent on
$\mathcal{L}$ corresponds to maximizing the surrogate. We write the per-token PPO clipped surrogate as
\begin{equation}
    \mathrm{PPOClip}(\rho_t, \hat{A}_t)
    := \min\!\Big(
        \rho_t \hat{A}_t,\;
        \mathrm{clip}(\rho_t, 1-\epsilon_c, 1+\epsilon_c)\,\hat{A}_t
    \Big),
    \label{eq:ppo-clip}
\end{equation}
where $\epsilon_c$ is the PPO clipping threshold. The REINFORCE Pro Max loss combines both components as
\begin{equation}
    \mathcal{L}^{\mathrm{ProMax}}
    = -\sum_t M_t^{\mathrm{prefix}} \cdot \mathrm{PPOClip}(\rho_t, \hat{A}_t).
    \label{eq:promax-loss}
\end{equation}
Here $\hat{A}_t$ is the adaptively normalized advantage from \Cref{sec:adaptive-norm}, $M_t^{\mathrm{prefix}}$ is the prefix IS mask,
$\rho_t$ is the per-token importance ratio defined in \eqref{eq:ratio}.

\textbf{Gradient flow.} Gradients propagate only through $\rho_t$. The prefix mask $M_t^{\mathrm{prefix}}$ and the normalized advantage
$\hat{A}_t$ are detached from the computation graph. When $M_t^{\mathrm{prefix}} = 0$, the loss at position $t$ is zero and contributes
no gradient.
