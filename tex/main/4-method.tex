\section{REINFORCE Max: Variance-Reduced Advantage Estimation}
\label{sec:reinforce-max}

Given a prompt $q$ (denoted $x$ in \Cref{sec:preliminaries}), we sample $n$ responses $\{o_1, \ldots, o_n\}$ (each $o_i$ corresponds to $y$ in the preliminaries) with rewards $\{r_1, \ldots, r_n\}$. Each response $o_i$ consists of $T_i$ action tokens. REINFORCE Max combines three ingredients: a leave-one-out baseline (\Cref{eq:rloo-baseline}), token-level expansion (\Cref{sec:token-expand}), and adaptive normalization (\Cref{sec:adaptive-norm}).

We adopt the leave-one-out (RLOO) baseline~\citep{ahmadian2024back} for its statistical independence, which avoids correlated baseline artifacts present in the mean baseline (see \Cref{app:rloo-proof} for a detailed analysis).

\begin{definition}[Leave-One-Out Baseline]
\label{def:rloo}
For sample $i$, the baseline and shaped reward are:
\begin{equation}
    b_i = \frac{1}{n-1}\sum_{j \neq i} r_j, \qquad \tilde{r}_i = r_i - b_i.
    \label{eq:rloo-baseline}
\end{equation}
\end{definition}

\subsection{Token-Level Expansion}
\label{sec:token-expand}

Since the reward is a scalar per response and we use $\gamma = 1.0$ (see \Cref{app:gamma-one} for a formal justification), the token-level advantage is simply the shaped reward itself:
\begin{equation}
    A_{i,t} = \tilde{r}_i = r_i - b_i, \quad \forall\, t \in \{1, \ldots, T_i\}.
    \label{eq:token-advantage}
\end{equation}

\subsection{Adaptive Asymmetric Normalization}
\label{sec:adaptive-norm}

Standard normalization $\hat{A} = (A - \mu)/\sigma$ applies a uniform scaling to all advantages. In reasoning tasks, the advantage distribution is typically highly skewed: most samples fail (negative advantages) while few succeed (positive advantages). Uniform scaling distorts the relative balance between reinforcement and punishment signals.

\begin{definition}[Adaptive Normalization]
\label{def:adaptive-norm}
For a prompt group with token-level advantages $\{A_{i,t}\}$, define:
\begin{equation}
    \mathcal{P} = \{(i,t) : A_{i,t} > 0\}, \quad \mathcal{N} = \{(i,t) : A_{i,t} < 0\},
\end{equation}
\begin{equation}
    S^+ = \!\sum_{(i,t) \in \mathcal{P}}\! A_{i,t},\; S^- = \!\sum_{(i,t) \in \mathcal{N}}\! A_{i,t},\; Q^+ = \!\sum_{(i,t) \in \mathcal{P}}\! A_{i,t}^2,\; Q^- = \!\sum_{(i,t) \in \mathcal{N}}\! A_{i,t}^2.
\end{equation}
Let $N = |\mathcal{P}| + |\mathcal{N}|$ be the count of non-zero tokens. The normalized advantage is:
\begin{equation}
    \hat{A}_{i,t} = \begin{cases} \alpha \cdot A_{i,t} & \text{if } A_{i,t} > 0, \\ \beta \cdot A_{i,t} & \text{if } A_{i,t} < 0, \\ 0 & \text{if } A_{i,t} = 0, \end{cases}
    \label{eq:adaptive-norm}
\end{equation}
where $\alpha, \beta > 0$ satisfy $\E[\hat{A}] = 0$ and $\Var[\hat{A}] = 1$ on non-zero tokens.
\end{definition}

\begin{proposition}[Closed-Form Solution]
\label{prop:alpha-beta}
Assume $\mathcal{P} \neq \emptyset$ and $\mathcal{N} \neq \emptyset$. The unique solution satisfying $\alpha S^+ + \beta S^- = 0$ and $\alpha^2 Q^+ + \beta^2 Q^- = N$ is:
\begin{equation}
    \alpha = \sqrt{\frac{N}{Q^+ + \left(\frac{S^+}{S^-}\right)^{\!2} Q^-}}, \qquad \beta = -\alpha \cdot \frac{S^+}{S^-}.
    \label{eq:alpha-beta}
\end{equation}
\end{proposition}

\begin{proof}
The zero-mean constraint gives $\beta = -\alpha \cdot S^+/S^-$. Substituting into the unit-variance constraint:
$\alpha^2 Q^+ + \alpha^2 (S^+/S^-)^{2} Q^- = N$,
yielding $\alpha^2 = N / (Q^+ + (S^+/S^-)^2 Q^-)$. Since $Q^+, Q^-, N > 0$ when both $\mathcal{P}$ and $\mathcal{N}$ are non-empty, $\alpha > 0$. Since $S^+ > 0$ and $S^- < 0$, $\beta = -\alpha S^+/S^- > 0$.
\end{proof}

\begin{proposition}[Advantage Sign Preservation]
\label{prop:gradient-direction}
The adaptive normalization preserves the sign of the advantage at every token: $\mathrm{sign}(\hat{A}_{i,t}) = \mathrm{sign}(A_{i,t})$ for all $(i,t)$ with $A_{i,t} \neq 0$. The relative ordering of magnitudes within $\mathcal{P}$ and within $\mathcal{N}$ is also preserved. Consequently, the policy gradient update reinforces (or penalizes) the same tokens as the unnormalized advantage.
\end{proposition}

\begin{proof}
Since $\alpha > 0$ and $\beta > 0$ (\Cref{prop:alpha-beta}), multiplication by a positive constant preserves both sign and relative ordering.
\end{proof}

\begin{remark}[Connection to asymmetric clipping]
Adaptive normalization is a soft analog of PPO's asymmetric clipping. PPO clips the importance ratio differently depending on the advantage sign; adaptive normalization scales the advantage magnitude differently. Both control the effective step size asymmetrically, but normalization acts through the advantage rather than the ratio, avoiding the gradient leakage problem identified by~\citet{li2025trm}.
\end{remark}

\begin{remark}[Uniform Scale Mode]
When all $n$ samples share the same reward, the leave-one-out baseline yields zero advantages, eliminating gradient signal. An optional \emph{uniform scale} mode recovers this signal by setting $\tilde{r}_i = r_i/n$ and skipping normalization, improving sample utilization. See \Cref{app:uniform-scale} for details.
\end{remark}


\section{REINFORCE Pro: Prefix Cumulative IS Correction}
\label{sec:reinforce-pro}

\subsection{The Off-Policy Correction Problem}
\label{sec:offpolicy-problem}

The rollout policy $\piroll$ (vLLM) and the actor policy $\piold$ (training framework) differ due to backend discrepancies, MoE routing, and distributed staleness. The per-token IS log-ratio is $\ell_t = \log \piold(y_t|c_t) - \log \piroll(y_t|c_t)$. The corrected loss takes the form:
\begin{equation}
    \mathcal{L} = \E_{\piroll}\!\Big[\phi(\ell_1, \ldots, \ell_T) \cdot \mathcal{L}^{\mathrm{PPO}}\Big],
\end{equation}
where $\phi$ is a correction function (detached from the computation graph) and $\mathcal{L}^{\mathrm{PPO}}$ is the standard PPO clipped loss using ratio $\rho_t^{\mathrm{PPO}} = \pitheta(y_t|c_t)/\piold(y_t|c_t)$.

\subsection{Limitations of Existing Methods}
\label{sec:is-limitations}

\begin{proposition}[Token-Level IS Ignores Causal Structure]
\label{prop:token-is-fails}
Consider a sequence where $w_1 = \exp(\ell_1) \gg 1$ but $w_t \approx 1$ for $t > 1$. Token-level IS (TIS, ICEPOP) applies correction only at position~1, leaving positions $2, \ldots, T$ uncorrected. However, the context $c_2 = (x, y_1)$ was sampled from $\piroll$, while under $\piold$ the distribution over $y_1$ differs substantially. The entire future trajectory is off-policy, but token-level IS does not detect this.
\end{proposition}

\begin{proof}
Under ICEPOP, $\mathrm{mask}_t = \mathbb{I}[w_t \in [\lambda, \Lambda]]$. Since $w_t \approx 1$ for $t > 1$, all future tokens pass the filter. Yet the context distribution shift at position $t$ satisfies $\|d_t^{\piold} - d_t^{\piroll}\|_{\mathrm{TV}} > 0$ by the context shift propagation (\Cref{lem:coupling}), so the loss at these positions is computed under a mismatched context.
\end{proof}

\begin{proposition}[Sequence-Level IS Loses Positional Information]
\label{prop:seq-is-fails}
For a sequence of length $T$ where $|\ell_t| \le \delta$ for all $t \neq t^*$ and $|\ell_{t^*}| = L$, the sequence-level geometric mean satisfies:
\begin{equation}
    \exp\!\Big(\frac{1}{T}\sum_{t=1}^T \ell_t\Big) \approx \exp\!\Big(\frac{L}{T}\Big).
\end{equation}
For $T = 4096$ and $L = 3$, this gives $\approx 1.0007$, well within any reasonable threshold, causing the deviation to go undetected.
\end{proposition}

\begin{proof}
Under the stated conditions, $|\sum_{t \neq t^*} \ell_t| \le (T-1)\delta$. Thus $\frac{1}{T}\sum_t \ell_t = \frac{L + O((T-1)\delta)}{T} = \frac{L}{T} + O(\delta)$. For $\delta \to 0$, the geometric mean converges to $\exp(L/T)$, which approaches $1$ as $T \to \infty$ regardless of $L$.
\end{proof}

\subsection{Prefix Cumulative IS: Definition and Properties}
\label{sec:prefix-is}

\begin{definition}[Prefix Cumulative IS]
\label{def:prefix-is}
For each position $t$ with action mask $m_t \in \{0, 1\}$, define:
\begin{align}
    L_t &= \sum_{s=1}^{t} \ell_s \cdot m_s, \qquad P_t = \sum_{s=1}^{t} m_s, \label{eq:prefix-cumsum} \\
    \mathrm{prefix\_is}(t) &= \exp\!\Big(\frac{L_t}{P_t}\Big), \label{eq:prefix-is}
\end{align}
where $P_t \ge 1$. This is the geometric mean of IS ratios over active positions up to $t$.
\end{definition}

\begin{definition}[Prefix IS Masking and Loss]
Given thresholds $[\lambda, \Lambda]$:
\begin{align}
    M_t^{\mathrm{prefix}} &= \mathbb{I}\!\big[\mathrm{prefix\_is}(t) \in [\lambda, \Lambda]\big], \label{eq:prefix-mask} \\
    \mathcal{L} &= \sum_t M_t^{\mathrm{prefix}} \cdot \exp(\ell_t) \cdot \mathcal{L}_t^{\mathrm{PPO}}, \label{eq:prefix-loss}
\end{align}
where $\exp(\ell_t)$ is the detached token-level IS weight and $\mathcal{L}_t^{\mathrm{PPO}}$ is the per-token PPO clipped loss.
\end{definition}

Three structural properties distinguish prefix IS from existing methods:

\textbf{Causality.} $\mathrm{prefix\_is}(t)$ depends only on $\{\ell_s\}_{s \le t}$, respecting the autoregressive order.

\textbf{Monotonic detection.} If $\ell_s$ is consistently biased, $L_t/P_t$ drifts from zero, eventually crossing the threshold and masking all subsequent tokens.

\textbf{Granularity.} Different tokens in the same sequence are independently masked based on cumulative divergence up to that point.

\begin{theorem}[Causality-Aware Masking with Conditional Dominance]
\label{thm:prefix-tighter}
Let $M^{\mathrm{prefix}}_t$, $M^{\mathrm{token}}_t$, and $M^{\mathrm{seq}}_t$ denote the masks produced by prefix cumulative IS, token-level IS (ICEPOP), and sequence-level IS (seq-mask-tis) respectively, all with the same threshold $[\lambda, \Lambda]$ where $0 < \lambda \le 1 \le \Lambda$. Assume policy support overlap (\Cref{asm:support}). Then:

\begin{enumerate}
    \item[(a)] \textbf{Early deviation detection.} If $w_1 \notin [\lambda, \Lambda]$ and $|\ell_t| \le \epsilon$ for $t > 1$ with $0 \le \epsilon \le \min(\log\Lambda, |\log\lambda|)$, then:
    \begin{equation}
        \big|\{t : M_t^{\mathrm{prefix}} = 0\}\big| \ge \big|\{t : M_t^{\mathrm{token}} = 0\}\big|.
    \end{equation}
    Prefix IS masks position 1 and a contiguous block of subsequent positions whose length is determined by the condition $|\ell_1| > P_t(\tau + \epsilon) - \epsilon$ where $\tau = \log\Lambda$ if $\ell_1 > 0$ and $\tau = |\log\lambda|$ if $\ell_1 < 0$ (see proof); larger $|\ell_1|$ and smaller $\epsilon$ allow more positions to satisfy this condition. Token-level IS masks only position 1.

    \item[(b)] \textbf{Late deviation efficiency.} If $|\ell_t| \le \epsilon$ for $t < t^*$ with $0 \le \epsilon \le \min(\log\Lambda, |\log\lambda|)$, and $w_{t^*} \notin [\lambda, \Lambda]$ with $t^*$ close to $T$, then:
    \begin{equation}
        \big|\{t : M_t^{\mathrm{prefix}} = 0\}\big| \le T - t^* + 1.
    \end{equation}
    Prefix IS masks at most $T - t^* + 1$ tokens (from $t^*$ onward, where the cumulative mean may cross the threshold). In contrast, sequence-level IS (seq-mask-tis) computes a single geometric mean over the entire sequence: if this crosses the threshold, $|\{t : M_t^{\mathrm{seq}} = 0\}| = T$ (all tokens rejected); if not, $|\{t : M_t^{\mathrm{seq}} = 0\}| = 0$ (all tokens accepted). When $t^*$ is close to $T$, the sequence-level geometric mean $\exp(L/T)$ is diluted toward 1, likely accepting the entire sequence and missing the late deviation entirely, while prefix IS can still detect it locally.

    \item[(c)] \textbf{Monotone deviation: interpolation between token and sequence granularity.} When all per-token log-ratios have the same sign and each strictly exceeds the corresponding threshold---i.e., $\ell_s > \log\Lambda$ for all $s$ (upper violation), or $\ell_s < \log\lambda$ for all $s$ (lower violation)---the prefix cumulative mean $L_t/P_t$ remains strictly beyond the corresponding threshold for all $t$. In this regime:
    \begin{itemize}
        \item The prefix mean exceeds the threshold from the first position onward, producing a contiguous suffix of masked tokens starting at $t_0 = 1$.
        \item The number of prefix-masked tokens satisfies $|\{t : M_t^{\mathrm{prefix}} = 0\}| = |\{t : M_t^{\mathrm{seq}} = 0\}| = T$, since under the strict violation assumption the sequence-level geometric mean also exceeds the threshold, so both methods reject all $T$ tokens.
    \end{itemize}
    More generally, when all $\ell_s$ have the same sign but some do not individually exceed the corresponding threshold (e.g., $0 \le \ell_s < \log\Lambda$ in the upper case, or $\log\lambda < \ell_s \le 0$ in the lower case), the prefix mean $L_t/P_t$ tends to drift away from zero as deviations accumulate. However, the running average is \emph{not} monotonically non-decreasing in general: adding a term $\ell_{t+1}$ with $0 \le \ell_{t+1} < L_t/P_t$ decreases the average. In this case, once the prefix mean crosses the threshold at position $t_0$, it may subsequently return within bounds if later terms are sufficiently small. The masking behavior thus depends on the specific magnitude profile of the $\ell_s$ values. The relationship with token-level masking is complementary rather than strictly ordered: token-level IS detects individual large $\ell_t$ values, while prefix IS detects cumulative drift that may consist of many small deviations (as shown in Part~(a)).
\end{enumerate}
\end{theorem}

\begin{proof}
Detailed numerical examples are in \Cref{app:prefix-proof}.

\textbf{Part (a).} Under ICEPOP, $M_t^{\mathrm{token}} = \mathbb{I}[w_t \in [\lambda, \Lambda]]$. Since $|\ell_t| \le \epsilon$ for $t > 1$, we have $w_t \in [e^{-\epsilon}, e^{\epsilon}]$. The condition $\epsilon \le \min(\log\Lambda, |\log\lambda|)$ ensures $[e^{-\epsilon}, e^{\epsilon}] \subseteq [\lambda, \Lambda]$, so $M_t^{\mathrm{token}} = 1$ for $t > 1$, so only position 1 is masked. For prefix IS, $\mathrm{prefix\_is}(t) = \exp(L_t/P_t)$ where $L_t = \ell_1 + \sum_{s=2}^t \ell_s m_s$. Since $|\ell_s| \le \epsilon$ for $s > 1$, we have $|L_t - \ell_1| \le (P_t - 1)\epsilon$. We consider two cases depending on the sign of $\ell_1$. If $\ell_1 > 0$: $L_t/P_t \ge (\ell_1 - (P_t-1)\epsilon)/P_t$, which remains above $\log\Lambda$ as long as $\ell_1 > P_t(\log\Lambda + \epsilon) - \epsilon$. If $\ell_1 < 0$: $L_t/P_t \le (\ell_1 + (P_t-1)\epsilon)/P_t$, which remains below $\log\lambda$ as long as $\ell_1 < -P_t(|\log\lambda| + \epsilon) + \epsilon$. Writing $\tau = \log\Lambda$ if $\ell_1 > 0$ and $\tau = |\log\lambda|$ if $\ell_1 < 0$, both cases unify to $|\ell_1| > P_t(\tau + \epsilon) - \epsilon$. For $\epsilon = 0$, this reduces to $P_t \le |\ell_1|/\tau$. Thus prefix IS masks a contiguous block of tokens starting from position 1.

\textbf{Part (b).} For $t < t^*$, $|L_t/P_t| \le \epsilon$ since all $|\ell_s| \le \epsilon$, so $\mathrm{prefix\_is}(t) \in [e^{-\epsilon}, e^{\epsilon}]$. The condition $\epsilon \le \min(\log\Lambda, |\log\lambda|)$ ensures $[e^{-\epsilon}, e^{\epsilon}] \subseteq [\lambda, \Lambda]$, hence $M_t^{\mathrm{prefix}} = 1$. For $t \ge t^*$, the cumulative mean shifts: $L_t/P_t = (\ell_{t^*} + \sum_{s \neq t^*} \ell_s m_s)/P_t$, where $|\sum_{s \neq t^*} \ell_s m_s| \le (P_t - 1)\epsilon$. If $t^*$ is close to $T$, the denominator $P_t$ is large, so the shift may or may not cross the threshold---but at most $T - t^* + 1$ tokens are affected. In contrast, seq-mask-tis computes a single sequence-level geometric mean; if this crosses the threshold, all $T$ tokens are rejected.

\textbf{Part (c).} Consider the upper-violation case: all $\ell_s > \log\Lambda$ (the lower-violation case $\ell_s < \log\lambda$ is symmetric with $\log\lambda$ replacing $\log\Lambda$). The running average $L_t/P_t = \frac{1}{P_t}\sum_{s \le t} \ell_s m_s > \log\Lambda$ for all $t$, since the average of values each strictly greater than $\log\Lambda$ is itself strictly greater than $\log\Lambda$ (note: the average remains beyond the threshold, though it is not necessarily monotonically non-decreasing). Thus $M_t^{\mathrm{prefix}} = 0$ for all $t$, and the entire sequence is masked. For sequence-level IS, the geometric mean $\exp(\frac{1}{T}\sum_s \ell_s) > \Lambda$ since each $\ell_s > \log\Lambda$, so the sequence is also rejected and all $T$ tokens are masked. Hence $|\{t : M_t^{\mathrm{prefix}} = 0\}| = |\{t : M_t^{\mathrm{seq}} = 0\}| = T$. In the general same-sign case where some $\ell_s$ do not individually exceed the corresponding threshold (e.g., $0 \le \ell_s < \log\Lambda$ in the upper case), the running average may cross the threshold at some $t_0$ but can subsequently decrease if later terms $\ell_{t+1} < L_t/P_t$, potentially returning within bounds. The masking is therefore not necessarily a contiguous suffix in the general case. The relationship with token-level IS is complementary: token-level IS detects individual large $\ell_t$ values, while prefix IS detects cumulative drift from many small deviations (as in Part~(a)).
\end{proof}

\subsection{Connection to the Adaptive Bound}
\label{sec:adaptive-connection}

We now establish the theoretical link between prefix cumulative IS and the Adaptive bound from the trust region framework~\citep{li2025trm}. \emph{Important scope note:} The following theorem describes structural correspondences between the two frameworks. Parts~(a) and~(b) are structural observations. Part~(c) establishes an expectation identity over the full rollout distribution $\piroll$; the prefix IS filtering then restricts gradient updates to the \emph{accepted subset} of trajectories (those with $M_t^{\mathrm{prefix}} = 1$), so the effective training distribution has controlled cumulative divergence. The population-level KL bounds apply to the full distribution, not the filtered subset; the filtering acts as a practical mechanism to exclude high-divergence trajectories. Importantly, this theorem controls only the $\piold/\piroll$ gap; the remaining $\pitheta/\piold$ gap is bounded by PPO clipping and the full $\pitheta/\piroll$ error decomposition via the triangle inequality is presented in \Cref{sec:unified}.

\begin{theorem}[Prefix IS as a Sample-Level Approximation of the Adaptive Bound]
\label{thm:prefix-adaptive}
Under Assumptions~\ref{asm:support}--\ref{asm:finite-length}, with threshold $0 < \lambda \le 1 \le \Lambda$, the Adaptive bound (\Cref{eq:adaptive-bound}) and prefix cumulative IS share three structural correspondences. The connection between sample-level IS ratios and population-level KL divergence operates through the filtering mechanism: prefix IS constrains sample-level quantities that, in expectation over the rollout distribution $\piroll$, correspond to the KL terms in the Adaptive bound. Formally, the expectation identity (Part~(c) below) holds over the full rollout distribution; the filtering step restricts gradient updates to the accepted subset where the sample-level cumulative log-ratio is bounded, so that the effective training distribution has controlled cumulative divergence.

\begin{enumerate}
    \item[(a)] \textbf{Per-position granularity.} Both operate at per-position granularity: the Adaptive bound uses per-position $\Dbar_t$, and prefix IS computes a per-position $\mathrm{prefix\_is}(t)$.

    \item[(b)] \textbf{Causal accumulation.} The Adaptive bound's factor $\Dbar_t = \E_{c_t \sim d_t^{\piroll}}[\Dtvtok(c_t)]$ captures the expected per-position TV divergence (the advantage factor), while the context shift factor $\|d_t^{\piold} - d_t^{\piroll}\|_{\mathrm{TV}}$ depends on the cumulative divergence from positions $1$ to $t-1$. The prefix IS $\mathrm{prefix\_is}(t) = \exp(L_t/P_t)$ helps control this cumulative context shift on retained samples via the KL chain rule, providing a sample-level proxy for the context shift factor in the error decomposition. (The gap from $\piold$ to $\pitheta$ is addressed via PPO clipping in \Cref{sec:unified}.)

    \item[(c)] \textbf{Connection via KL chain rule.} The expected cumulative log-ratio satisfies:
    \begin{equation}
        \E_{\piroll}\!\Big[\sum_{s=1}^{t} \ell_s\Big] = -\sum_{s=1}^{t} \E_{c_s \sim d_s^{\piroll}}\!\big[\DKL(\piroll(\cdot|c_s) \| \piold(\cdot|c_s))\big] = -\DKL(d_{t+1}^{\piroll} \| d_{t+1}^{\piold}),
        \label{eq:cumsum-kl}
    \end{equation}
    where the second equality follows from the KL chain rule. Thresholding $\mathrm{prefix\_is}(t)$ constrains the sample-level cumulative log-ratio:
    \begin{equation}
        \mathrm{prefix\_is}(t) \in [\lambda, \Lambda] \iff \frac{1}{P_t}\sum_{s=1}^{t}\ell_s m_s \in [\log\lambda, \log\Lambda].
        \label{eq:threshold-kl}
    \end{equation}
    Note that $L_t = \sum_{s=1}^t \ell_s m_s$ is a \emph{sample-level} quantity, not the expected KL. The threshold acts as a sample-based proxy: by filtering out trajectories where the cumulative log-ratio exceeds the threshold, we restrict the effective training distribution to samples where the cumulative divergence is controlled. On the accepted subset ($M_t^{\mathrm{prefix}} = 1$), applying Pinsker's inequality~\citep{pinsker1964information} to the population-level KL yields:
    \begin{equation}
        \|d_{t+1}^{\piold} - d_{t+1}^{\piroll}\|_{\mathrm{TV}} \le \sqrt{\frac{\DKL(d_{t+1}^{\piroll} \| d_{t+1}^{\piold})}{2}}.
        \label{eq:context-shift-prefix}
    \end{equation}
    The sample-level constraint $|L_t| \le P_t \cdot \max(|\log\lambda|, \log\Lambda)$ does not directly imply a bound on the population-level KL; rather, it serves as a practical filter that excludes high-divergence trajectories, indirectly controlling the effective KL on the retained samples.
\end{enumerate}
\end{theorem}

\begin{proof}
\textbf{Part (a)} and \textbf{(b)} are structural observations.

\textbf{Part (c).} For each position $s$, the log IS ratio $\ell_s = \log \piold(y_s|c_s) - \log \piroll(y_s|c_s)$ satisfies:
\begin{equation}
    \E_{y_s \sim \piroll(\cdot|c_s)}[\ell_s] = \sum_{v} \piroll(v|c_s) \log\frac{\piold(v|c_s)}{\piroll(v|c_s)} = -\DKL(\piroll(\cdot|c_s) \| \piold(\cdot|c_s)).
\end{equation}
Taking the expectation over $c_s \sim d_s^{\piroll}$ and summing:
\begin{equation}
    \E_{\piroll}\!\Big[\sum_{s=1}^{t}\ell_s\Big] = -\sum_{s=1}^{t}\E_{c_s \sim d_s^{\piroll}}[\DKL(\piroll(\cdot|c_s) \| \piold(\cdot|c_s))].
\end{equation}
By the KL chain rule (Lemma in~\citet{li2025trm}), $\sum_{s=1}^{t}\E_{c_s}[\DKL(c_s)] = \DKL(d_{t+1}^{\piroll} \| d_{t+1}^{\piold})$, establishing Eq.~\eqref{eq:cumsum-kl}. This connects the expected value of the sample-level cumulative log-ratio to the population-level joint KL divergence.

For accepted tokens where $\mathrm{prefix\_is}(t) \in [\lambda, \Lambda]$, we have $|L_t| \le P_t \cdot \max(|\log\lambda|, \log\Lambda)$. This is a per-trajectory constraint. The filtering mechanism ensures that only trajectories satisfying this constraint contribute to the gradient update, effectively restricting the training distribution to a subset where the cumulative log-ratio---and hence, in expectation, the cumulative KL---is bounded. The population-level bound in Eq.~\eqref{eq:context-shift-prefix} follows from Pinsker's inequality applied to $\DKL(d_{t+1}^{\piroll} \| d_{t+1}^{\piold})$, which is the quantity controlled (in expectation) by the sample-level filter.
\end{proof}

\begin{corollary}[Per-Position Sample-Level Trust Region Constraint]
\label{cor:per-position-trust}
Under prefix IS masking with threshold $[\lambda, \Lambda]$, for all accepted tokens ($M_t^{\mathrm{prefix}} = 1$), the sample-level cumulative log-ratio satisfies:
\begin{equation}
    \frac{1}{P_t}\sum_{s=1}^{t} \big(\log \piold(y_s|c_s) - \log \piroll(y_s|c_s)\big) m_s \in [\log\lambda, \log\Lambda].
\end{equation}
This provides a per-position, sample-level trust region constraint analogous to the per-position structure in the Adaptive bound. The constraint operates on individual trajectories; its connection to the population-level KL is through the expectation identity in Eq.~\eqref{eq:cumsum-kl}.
\end{corollary}

\begin{proof}
Follows directly from \Cref{def:prefix-is} and Eq.~\eqref{eq:prefix-mask}: $M_t^{\mathrm{prefix}} = 1$ iff $\mathrm{prefix\_is}(t) \in [\lambda, \Lambda]$ iff $L_t/P_t \in [\log\lambda, \log\Lambda]$.
\end{proof}

\subsection{The Combined Loss}
\label{sec:combined-loss}

We define the loss as the negative surrogate objective, $\mathcal{L} = -L_{\piroll}(\pitheta)$, so that gradient descent on $\mathcal{L}$ corresponds to maximizing the surrogate. The full REINFORCE Pro Max loss combines both components:
\begin{equation}
    \mathcal{L}^{\mathrm{ProMax}} = -\sum_t M_t^{\mathrm{prefix}} \cdot \exp(\ell_t) \cdot \min\!\big(\rho_t^{\mathrm{PPO}} \hat{A}_t,\; \mathrm{clip}(\rho_t^{\mathrm{PPO}}, 1\!-\!\epsilon_c, 1\!+\!\epsilon_c)\, \hat{A}_t\big),
    \label{eq:promax-loss}
\end{equation}
where $\hat{A}_t$ is the adaptively normalized advantage from \Cref{sec:adaptive-norm}, $M_t^{\mathrm{prefix}}$ is the prefix IS mask, $\exp(\ell_t)$ is the detached token-level IS weight, and $\rho_t^{\mathrm{PPO}} = \pitheta(y_t|c_t)/\piold(y_t|c_t)$ is the standard PPO ratio (which carries gradients). The negative sign ensures that minimizing $\mathcal{L}^{\mathrm{ProMax}}$ maximizes the surrogate objective.

\textbf{Gradient flow.} Gradients propagate only through $\rho_t^{\mathrm{PPO}}$. The prefix mask $M_t^{\mathrm{prefix}}$, IS weight $\exp(\ell_t)$, and normalized advantage $\hat{A}_t$ are all detached from the computation graph. When $M_t^{\mathrm{prefix}} = 0$, the loss at position $t$ is zero and contributes no gradient.
