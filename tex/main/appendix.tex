\newpage
\appendix

\section{Proof of Leave-One-Out Baseline Properties}
\label{app:rloo-proof}

We provide a rigorous analysis of the leave-one-out baseline estimator used in \Cref{sec:reinforce-max}.

\begin{proposition}[Properties of the Leave-One-Out Baseline]
\label{prop:rloo-variance}
Fix a prompt $x$ and draw $n \ge 2$ i.i.d.\ responses $\{y^{(1)}, \ldots, y^{(n)}\} \sim \pitheta(\cdot|x)$ with rewards
$r_i = R(x, y^{(i)})$. Let
\begin{equation}
    f_i = \nabla_\theta \log P^{\pitheta}(y^{(i)} \mid x)
\end{equation}
denote the trajectory score function for sample $i$, and define the per-sample policy gradient contribution with baseline $b$ as
$g_i(b) = (r_i - b) f_i$. With the leave-one-out baseline $b_i = \frac{1}{n-1}\sum_{j \neq i} r_j$, the estimator
\begin{equation}
    \hat{g} = \frac{1}{n}\sum_{i=1}^{n} g_i(b_i)
\end{equation}
satisfies:
\begin{enumerate}
    \item \textbf{Unbiasedness:} $\E[\hat{g}] = \nabla_\theta J(\pitheta)$.
    \item \textbf{Baseline independence:} For each $i$, $b_i$ is independent of $(r_i, f_i)$; in particular, $\E[b_i f_i] = 0$ and
    $\Cov(f_i, b_i) = 0$.
    \item \textbf{Mean-baseline scaling:} Let $b^{\mathrm{mean}} = \frac{1}{n}\sum_{j=1}^{n} r_j$ and
    $\hat{g}^{\mathrm{mean}} = \frac{1}{n}\sum_{i=1}^{n}(r_i - b^{\mathrm{mean}}) f_i$. Then deterministically
    $\hat{g}^{\mathrm{mean}} = \frac{n-1}{n}\hat{g}$, and hence $\E[\hat{g}^{\mathrm{mean}}] = \frac{n-1}{n}\nabla_\theta J(\pitheta)$.
    \item \textbf{Second-moment decomposition:} Let $\mu(x) = \E[r_i \mid x]$ and $\sigma^2(x) = \Var(r_i \mid x)$. Then
    $\Var(b_i \mid x) = \sigma^2(x)/(n-1)$, and
    \begin{equation}
        \E\!\big[(r_i - b_i)^2 f_i f_i^\top \mid x\big]
        = \E\!\big[(r_i - \mu(x))^2 f_i f_i^\top \mid x\big] + \Var(b_i \mid x)\, \E[f_i f_i^\top \mid x].
        \label{eq:rloo-second-moment}
    \end{equation}
\end{enumerate}
\end{proposition}

\begin{remark}[Relationship between the two estimators]
\label{rem:rloo-mean-scaling}
The deterministic proportionality $\hat{g}^{\mathrm{mean}} = \frac{n-1}{n}\hat{g}$ implies that the mean-baseline estimator is a
rescaled (and therefore biased) version of the leave-one-out estimator: its smaller raw variance is entirely due to the constant scaling
factor. In practice, this shrinkage can be absorbed into the learning rate, but it breaks exact unbiasedness. The main advantage of the
leave-one-out baseline is that $b_i$ is independent of sample $i$, which simplifies moment calculations and avoids self-correlation terms
that arise when the baseline depends on $r_i$.
\end{remark}

\begin{remark}
The leave-one-out baseline is equivalent to the leave-one-out control variate from the classical REINFORCE literature~\citep{williams1992simple}. In the LLM-RL setting with $n$ samples per prompt, each sample's baseline is independent of its own trajectory, making it particularly well-suited for group-based sampling strategies.
\end{remark}

\begin{proof}
We prove all statements conditional on a fixed prompt $x$; taking the outer expectation over $x\sim P(x)$ yields the proposition for the
full objective $J(\pitheta)$.

\textbf{(1) Unbiasedness.} Since $b_i$ depends only on $\{r_j\}_{j\neq i}$, it is independent of $(r_i,f_i)$ conditional on $x$. Hence
\begin{align}
    \E\!\big[(r_i-b_i)f_i \mid x\big]
    &= \E\!\big[r_i f_i \mid x\big] - \E\!\big[b_i \mid x\big]\E\!\big[f_i \mid x\big].
\end{align}
By the score-function identity,
\begin{equation}
    \E\!\big[f_i \mid x\big] = \E\!\big[\nabla_\theta \log P^{\pitheta}(y^{(i)}\mid x)\mid x\big]
    = \nabla_\theta \E\!\big[1 \mid x\big] = 0,
\end{equation}
and
\begin{equation}
    \E\!\big[r_i f_i \mid x\big]
    = \E\!\Big[r_i\,\nabla_\theta \log P^{\pitheta}(y^{(i)}\mid x)\,\Big|\,x\Big]
    = \nabla_\theta \E[r_i \mid x].
\end{equation}
Therefore $\E[(r_i-b_i)f_i\mid x] = \nabla_\theta \E[r_i\mid x]$. Averaging over $i$ yields
\begin{equation}
    \E[\hat{g}\mid x] = \nabla_\theta \E[r_1\mid x] = \nabla_\theta J_x(\pitheta),
    \qquad
    J_x(\pitheta) := \E_{y\sim \pitheta(\cdot\mid x)}[R(x,y)].
\end{equation}
Taking the outer expectation over $x$ gives $\E[\hat{g}] = \nabla_\theta J(\pitheta)$.

\textbf{(2) Baseline independence.} The same conditional independence implies
\begin{equation}
    \E[b_i f_i \mid x] = \E[b_i\mid x]\E[f_i\mid x] = 0,
\end{equation}
so $\Cov(f_i,b_i)=0$.

\textbf{(3) Mean-baseline scaling.} For each $i$, write
\begin{align}
    b^{\mathrm{mean}}
    &= \frac{1}{n}\sum_{j=1}^{n} r_j
    = \frac{r_i}{n} + \frac{1}{n}\sum_{j\neq i} r_j
    = \frac{r_i}{n} + \frac{n-1}{n}\,b_i,
    \\
    r_i - b^{\mathrm{mean}}
    &= r_i - \frac{r_i}{n} - \frac{n-1}{n}\,b_i
    = \frac{n-1}{n}(r_i-b_i).
\end{align}
Summing over $i$ gives the deterministic identity $\hat{g}^{\mathrm{mean}} = \frac{n-1}{n}\hat{g}$, and taking expectations yields
$\E[\hat{g}^{\mathrm{mean}}] = \frac{n-1}{n}\nabla_\theta J(\pitheta)$.

\textbf{(4) Second-moment decomposition.} Conditional on $x$, $b_i$ is the average of $n-1$ i.i.d.\ rewards $\{r_j\}_{j\neq i}$, hence
\begin{equation}
    \Var(b_i \mid x) = \frac{\sigma^2(x)}{n-1}.
\end{equation}
Write $r_i-b_i = (r_i-\mu(x)) - (b_i-\mu(x))$. Then
\begin{align}
    (r_i-b_i)^2
    &= (r_i-\mu(x))^2 - 2(r_i-\mu(x))(b_i-\mu(x)) + (b_i-\mu(x))^2.
\end{align}
Multiplying by $f_i f_i^\top$ and taking conditional expectation gives
\begin{align}
    \E\!\big[(r_i-b_i)^2 f_i f_i^\top \mid x\big]
    &= \E\!\big[(r_i-\mu(x))^2 f_i f_i^\top \mid x\big]
    - 2\,\E\!\big[(r_i-\mu(x))(b_i-\mu(x)) f_i f_i^\top \mid x\big]
    \nonumber\\
    &\quad
    + \E\!\big[(b_i-\mu(x))^2 f_i f_i^\top \mid x\big].
\end{align}
The cross term vanishes by conditional independence and $\E[b_i-\mu(x)\mid x]=0$:
\begin{align}
    \E\!\big[(r_i-\mu(x))(b_i-\mu(x)) f_i f_i^\top \mid x\big]
    &= \E[b_i-\mu(x)\mid x]\;\E\!\big[(r_i-\mu(x)) f_i f_i^\top \mid x\big]
    = 0.
\end{align}
The last term factorizes as
\begin{align}
    \E\!\big[(b_i-\mu(x))^2 f_i f_i^\top \mid x\big]
    &= \E\!\big[(b_i-\mu(x))^2 \mid x\big]\;\E\!\big[f_i f_i^\top \mid x\big]
    = \Var(b_i\mid x)\,\E[f_i f_i^\top\mid x],
\end{align}
which yields \eqref{eq:rloo-second-moment}.
\end{proof}


\section{Optimality of \texorpdfstring{$\gamma = 1.0$}{gamma = 1.0}}
\label{app:gamma-one}

\begin{proposition}[Optimality of $\gamma = 1.0$]
\label{prop:gamma-one}
When the extrinsic reward is sparse (assigned only at the final token), $\gamma = 1.0$ is the unique discount factor that preserves unbiasedness of the REINFORCE gradient estimator for the undiscounted objective $J(\pitheta) = \E[R(x,y)]$.
\end{proposition}

\begin{proof}
For the undiscounted objective $J(\pitheta)=\E[R(x,y)]$, the REINFORCE identity gives
\begin{equation}
    \nabla_\theta J(\pitheta)
    = \E\!\Big[R(x,y)\sum_{t=1}^{T}\nabla_\theta \log \pitheta(y_t \mid c_t)\Big].
\end{equation}
In a sparse terminal-reward setting, using a discounted return with $\gamma<1$ amounts to reweighting earlier score terms, yielding the
alternative estimator
\begin{equation}
    g_\gamma(\theta)
    := \E\!\Big[R(x,y)\sum_{t=1}^{T}\gamma^{T-t}\nabla_\theta \log \pitheta(y_t \mid c_t)\Big].
\end{equation}
The difference is
\begin{align}
    g_\gamma(\theta) - \nabla_\theta J(\pitheta)
    &= \E\!\Big[R(x,y)\sum_{t=1}^{T}\big(\gamma^{T-t}-1\big)\nabla_\theta \log \pitheta(y_t \mid c_t)\Big]
    \nonumber\\
    &= \E\!\Big[R(x,y)\sum_{t=1}^{T-1}\big(\gamma^{T-t}-1\big)\nabla_\theta \log \pitheta(y_t \mid c_t)\Big],
    \label{eq:gamma-bias-diff}
\end{align}
where the last equality uses $\gamma^{T-T}-1=0$ at $t=T$. For any $\gamma<1$, the coefficients satisfy $\gamma^{T-t}-1<0$ for all
$t\le T-1$. Unless the expectations $\E[R(x,y)\nabla_\theta \log \pitheta(y_t\mid c_t)]$ vanish for all $t\le T-1$ (a degenerate case in
autoregressive generation), \eqref{eq:gamma-bias-diff} is nonzero, hence the estimator is biased. The only choice of $\gamma$ that
removes the reweighting for all $t$ is $\gamma=1$.
\end{proof}


\section{Proof of Theorem~\ref{thm:prefix-tighter}}
\label{app:prefix-proof}

We provide a complete proof of \Cref{thm:prefix-tighter} under the definitions in \Cref{sec:prefix-is}. For convenience, write the
log-threshold interval as
\begin{equation}
    [\log\lambda, \log\Lambda] = [-\tau_-, \tau_+],
    \qquad
    \tau_+ := \log\Lambda,
    \qquad
    \tau_- := -\log\lambda,
\end{equation}
so that $\IStoken{t} \in [\lambda,\Lambda]$ is equivalent to $r_t \in [-\tau_-,\tau_+]$.

\paragraph{Mask definitions.} The three masks compared in \Cref{thm:prefix-tighter} are:
\begin{itemize}
    \item \textbf{Token-level (ICEPOP / token-level TRM-style mask):}
    \begin{equation}
        M_t^{\mathrm{token}}
        := \mathbb{I}\!\big[\IStoken{t} \in [\lambda,\Lambda]\big]
        = \mathbb{I}\!\big[r_t \in [-\tau_-,\tau_+]\big].
    \end{equation}
    \item \textbf{Sequence-level (GSPO-style / sequence-level TRM-style mask):}
    \begin{equation}
        M_t^{\mathrm{seq}}
        := \mathbb{I}\!\big[\bar{r} \in [-\tau_-,\tau_+]\big],
        \qquad
        \bar{r} := \frac{1}{P_T}\sum_{s=1}^{T}r_s m_s,
    \end{equation}
    for all $t$ (equivalently, $\ISseq=\exp(\bar{r})$).
    \item \textbf{Prefix-level (TRM-style proxy):}
    \begin{equation}
        M_t^{\mathrm{prefix}}
        := \mathbb{I}\!\big[L_t/P_t \in [-\tau_-,\tau_+]\big],
    \end{equation}
    where $L_t = \sum_{s\le t}r_s m_s$ and $P_t=\sum_{s\le t}m_s$.
\end{itemize}

\begin{proof}[Proof of \Cref{thm:prefix-tighter}]
\noindent\textbf{Action-token indexing.}
By \Cref{rem:action-indexing}, we may index only over active action-token positions and assume the trajectory is unpadded. Equivalently,
we take $m_t\equiv 1$, so $P_t=t$ and $L_t=\sum_{s=1}^{t}r_s$.

\par
\noindent\textbf{Part (a): Early deviation.}
Assume $\IStoken{1} \notin [\lambda,\Lambda]$ and $|r_t| \le \varepsilon$ for all $t>1$, with
$0 \le \varepsilon \le \min(\tau_+,\tau_-)$. Then for every $t>1$,
\begin{equation}
    r_t \in [-\varepsilon,\varepsilon] \subseteq [-\tau_-,\tau_+],
\end{equation}
so $M_t^{\mathrm{token}}=1$ for all $t>1$, while $M_1^{\mathrm{token}}=0$ (since $\IStoken{1} \notin [\lambda,\Lambda]$). Hence token-level
ICEPOP masks only position $t=1$.

\smallskip
For the prefix mask, note first that $M_1^{\mathrm{prefix}}=0$ because the prefix average at $t=1$ is $\bar{r}_1=r_1 \notin
[-\tau_-,\tau_+]$. To quantify how long the rejection can persist, write
\begin{equation}
    L_t = r_1 + \sum_{s=2}^{t}r_s,
    \qquad
    P_t = t.
\end{equation}
If $r_1>\tau_+$, then $r_s \ge -\varepsilon$ for all $s\ge2$, so
\begin{equation}
    L_t \ge r_1 - \varepsilon(P_t-1).
\end{equation}
Therefore, if $P_t < (r_1+\varepsilon)/(\tau_+ + \varepsilon)$, then
\begin{align}
    \frac{L_t}{P_t}
    &\ge \frac{r_1-\varepsilon(P_t-1)}{P_t}
    = \frac{r_1+\varepsilon}{P_t} - \varepsilon
    > \tau_+,
\end{align}
so $M_t^{\mathrm{prefix}}=0$. The lower-violation case $r_1<-\tau_-$ is symmetric: since $r_s \le \varepsilon$ for $s\ge2$, we have
$L_t \le r_1 + \varepsilon(P_t-1)$. If $P_t < (-r_1+\varepsilon)/(\tau_-+\varepsilon)$, then
\begin{align}
    \frac{L_t}{P_t}
    &\le \frac{r_1+\varepsilon(P_t-1)}{P_t}
    = \frac{r_1-\varepsilon}{P_t} + \varepsilon
    < -(\tau_-+\varepsilon) + \varepsilon
    = -\tau_-,
\end{align}
and hence $M_t^{\mathrm{prefix}}=0$. This proves the sufficient conditions in \Cref{eq:early-upper-sufficient,eq:early-lower-sufficient}.

\par
\noindent\textbf{Part (b): Late deviation.}
Assume there exists $t^*\in\{1,\ldots,T\}$ such that $\IStoken{t^*}\notin[\lambda,\Lambda]$ and $|r_t|\le \varepsilon$
for all $t\neq t^*$, with
$0 \le \varepsilon \le \min(\tau_+,\tau_-)$. For any $t<t^*$, all active log-ratios in the prefix satisfy $r_s \in [-\tau_-,\tau_+]$.
Since $L_t/P_t$ is an average of values in $[-\tau_-,\tau_+]$, it follows that $L_t/P_t \in [-\tau_-,\tau_+]$, and hence
$M_t^{\mathrm{prefix}}=1$ for all $t<t^*$. Therefore prefix masking can only occur for positions $t\ge t^*$, which implies
\begin{equation}
    \big|\{t : M_t^{\mathrm{prefix}} = 0\}\big| \le T - t^* + 1.
\end{equation}
The sequence-level IS mask $M_t^{\mathrm{seq}}$ is constant across positions by definition, so it either rejects all tokens or rejects none.

For the claimed regime where prefix rejects but sequence-level accepts, consider the pure single-spike case $r_{t^*}=L$ and $r_t=0$
for $t\neq t^*$. Then $\bar{r} = L/P_T$ and $L_{t^*}/P_{t^*} = L/P_{t^*}$. Since $P_{t^*} \le P_T$, we have $L/P_{t^*} \ge L/P_T$.
Thus whenever
\begin{equation}
    \frac{L}{P_{t^*}} > \tau_+
    \quad\text{and}\quad
    \frac{L}{P_T} \le \tau_+,
\end{equation}
we obtain $M_{t^*}^{\mathrm{prefix}}=0$ while $M_t^{\mathrm{seq}}=1$ for all $t$. (The lower-threshold case is analogous with $-\tau_-$.)

\par
\noindent\textbf{Part (c): Monotone violation.}
If $r_s > \tau_+$ for all $s$, then for every $t$ the prefix average satisfies $L_t/P_t > \tau_+$, since it is an
average of values each strictly larger than $\tau_+$. Hence $M_t^{\mathrm{prefix}}=0$ for all $t$. The sequence-level average
$\bar{r}$ is also strictly larger than $\tau_+$, so $M_t^{\mathrm{seq}}=0$ for all $t$. The lower-violation case $r_s < -\tau_-$
is symmetric.
\end{proof}

\section{Alternative Trust-Region Masking Variants}
\label{app:mask-variants}

This appendix collects several masking variants that are compatible with the same two-policy ratio setup as in the main text
(\Cref{sec:offpolicy-problem}). Throughout, samples are generated by the behavior policy $\piroll$ and gradients are computed under the
training policy $\pitheta$. We write the per-token importance ratio and log-ratio as
\begin{equation}
    \rho_t := \frac{\pitheta(y_t \mid c_t)}{\piroll(y_t \mid c_t)},
    \qquad
    r_t := \log \rho_t.
\end{equation}
We also reuse the prefix cumulative sums from \Cref{def:prefix-is}:
\begin{equation}
    L_t := \sum_{s=1}^{t} r_s m_s,
    \qquad
    P_t := \sum_{s=1}^{t} m_s,
    \qquad
    \bar{r}_t := \frac{L_t}{P_t}\quad(P_t\ge 1).
\end{equation}
For thresholds $0<\lambda\le 1 \le \Lambda$, define the log-threshold interval
\begin{equation}
    [\log\lambda,\log\Lambda] = [-\tau_-,\tau_+],
    \qquad
    \tau_+ := \log\Lambda,
    \qquad
    \tau_- := -\log\lambda.
\end{equation}

\subsection{Variant B: Absorbing Prefix-Average Mask}
\label{app:variant-b}

The main text uses a pointwise prefix gate at each position $t$. A more rejection-style variant enforces an \emph{absorbing} semantics:
once a prefix violates the threshold, all subsequent positions are rejected.

\begin{equation}
    \widetilde{M}_t^{\mathrm{prefix}}
    := \mathbb{I}\!\big[\bar{r}_t \in [-\tau_-,\tau_+]\big],
    \qquad
    M_t^{\mathrm{abs\text{-}prefix}}
    := \prod_{u=1}^{t}\widetilde{M}_u^{\mathrm{prefix}}.
\end{equation}

\subsection{Variant C: Prefix-Max Mask (TRM-Max Style)}
\label{app:variant-c}

TRM-style max criteria can be expressed via a sample-based proxy $k_2(\rho)$:
\begin{equation}
    k_2(\rho) := \tfrac{1}{2}(\log\rho)^2.
\end{equation}
The corresponding prefix-max statistic and mask are
\begin{equation}
    S_t^{\max} := \max_{1\le s\le t} k_2(\rho_s),
    \qquad
    M_t^{\mathrm{prefix\text{-}max}} := \mathbb{I}\!\big[S_t^{\max} \le \delta_{\max}\big].
\end{equation}
Equivalently, $M_t^{\mathrm{prefix\text{-}max}}=1$ iff $\max_{s\le t}|\log\rho_s| \le \sqrt{2\delta_{\max}}$.

\subsection{Variant D: Prefix-Score Mask (Bound-Driven)}
\label{app:variant-d}

Score-based masking controls a prefix-level aggregate of per-token ratio deviations. Define a nonnegative per-token deviation
\begin{equation}
    a_t := |\rho_t - 1|
\end{equation}
and deterministic nonnegative weights $(w_t)_{t\ge1}$ with $\sum_{s\le t} w_s > 0$. The prefix score is
\begin{equation}
    \widetilde{W}_t
    := \frac{\sum_{s=1}^{t} w_s a_s}{\sum_{s=1}^{t} w_s},
\end{equation}
and the corresponding mask is
\begin{equation}
    M_t^{\mathrm{prefix\text{-}score}} := \mathbb{I}\!\big[\widetilde{W}_t \le \delta_W\big].
\end{equation}
A special case is the uniform-weight choice $w_s\equiv 1$, which yields $\widetilde{W}_T = \frac{1}{T}\sum_{t=1}^{T}|\rho_t-1|$ at the
full horizon.

\subsection{Variant E: Soft (Continuous) Prefix Gating}
\label{app:variant-e}

Hard masking can be relaxed to a continuous gate $\omega_t\in[0,1]$ constructed from the same prefix statistic $\bar{r}_t$. For example,
with temperature $\kappa>0$,
\begin{equation}
    \omega_t^{\mathrm{soft}}
    := \mathrm{sigmoid}\!\big(\kappa(\tau_+ - \bar{r}_t)\big)\cdot \mathrm{sigmoid}\!\big(\kappa(\tau_- + \bar{r}_t)\big).
\end{equation}
The corresponding soft-gated surrogate loss replaces the binary mask in \eqref{eq:promax-loss}:
\begin{equation}
    \mathcal{L}_{\mathrm{soft}}
    := -\sum_t \omega_t^{\mathrm{soft}} \cdot \mathrm{PPOClip}(\rho_t, \hat{A}_t).
\end{equation}
Unlike binary rejection rules, this variant does not implement rejection sampling and is included only as an alternative engineering
design.


\section{Numerical Stability}
\label{app:numerical}

The adaptive normalization (\Cref{sec:adaptive-norm}) includes several safeguards:

\begin{remark}[Numerical thresholds vs.\ divergence bounds]
The symbols $\varepsilon_{\mathrm{num}}$ and $\varepsilon_{\mathrm{tol}}$ in this appendix denote implementation hyperparameters used for
numerical stability and uniform-reward detection. They are unrelated to the divergence upper bound $\epsilon$ in \Cref{def:max-div}.
\end{remark}

\begin{enumerate}
    \item \textbf{Sum threshold:} If $|S^+| < \varepsilon_{\mathrm{num}}$ or $|S^-| < \varepsilon_{\mathrm{num}}$ (default $\varepsilon_{\mathrm{num}} = 10^{-8}$), normalization is skipped and the original advantages are returned.

    \item \textbf{Scale clamping:} $\alpha$ and $\beta$ are clamped to $[\varepsilon_{\mathrm{num}}, s_{\max}]$ (default $s_{\max} = 10$) to prevent extreme scaling.

    \item \textbf{Intermediate overflow protection:} The quantity $(S^+/S^-)^2 Q^-$ is clamped to $10^8$ before computing $\alpha$.

    \item \textbf{Finiteness check:} If $\alpha$ or $\beta$ is NaN or $\pm\infty$ after computation, normalization is skipped.
\end{enumerate}

These safeguards may cause the normalized advantages to deviate slightly from the exact empirical constraints in
\Cref{def:adaptive-norm}, but in practice the deviation is negligible ($|\text{mean}| < 0.001$, $|\text{var} - 1| < 0.01$). These
thresholds are heuristic defaults; formal validation is deferred to the experimental evaluation.

\textbf{Fallback conditions.} Normalization is skipped entirely when:
\begin{itemize}
    \item All advantages have the same sign ($\mathcal{P} = \emptyset$ or $\mathcal{N} = \emptyset$), indicating the group has uniform quality and the leave-one-out-shaped advantages are already meaningful.
    \item The uniform scale mode (\Cref{app:uniform-scale}) is active and the group has uniform rewards.
\end{itemize}


\section{Uniform Scale Mode}
\label{app:uniform-scale}

The uniform scale mode is an optional technique to improve sample utilization. When all $n$ samples for a prompt have identical rewards (all correct or all incorrect), the leave-one-out baseline yields $\tilde{r}_i = 0$ for all $i$, eliminating the gradient signal. Uniform scale mode recovers this signal.

\begin{definition}[Uniform Scale]
For prompt groups where $\mathrm{std}(\{r_1, \ldots, r_n\}) < \varepsilon_{\mathrm{tol}}$ (uniform-reward groups), the shaped reward is set to $\tilde{r}_i = r_i / n$, and the adaptive normalization (\Cref{sec:adaptive-norm}) is skipped.
\end{definition}

\begin{proposition}
Under uniform scale mode, the gradient for uniform-reward groups is proportional to $r_i \sum_t \nabla_\theta \log \pitheta(y_{i,t}|c_{i,t})$, which pushes the policy toward all sampled responses when $r_i > 0$ (all correct) and away from them when $r_i < 0$ (all incorrect), preserving gradient signal that the leave-one-out baseline would otherwise eliminate.
\end{proposition}

\begin{proof}
With $\tilde{r}_i = r_i/n$ and no normalization, the sample-$i$ contribution to the REINFORCE gradient is proportional to
\[
    \frac{r_i}{n} \sum_t \nabla_\theta \log \pitheta(y_{i,t}\mid c_{i,t}).
\]
Hence $\sign(r_i)$ determines the update direction: when $r_i > 0$ the policy increases the log-probability of the sampled tokens, and
when $r_i < 0$ it decreases them.
\end{proof}
