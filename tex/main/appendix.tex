\newpage
\appendix

\section{Details on Off-Policy Mismatch in LLM-RL}
\label{app:mismatch}

Modern LLM-RL pipelines decouple inference from training: rollouts are produced by an inference engine (e.g., vLLM or SGLang) under the
rollout policy $\piroll$, while gradient updates are computed in a separate training stack under the training policy $\pitheta$. This
architectural separation can introduce off-policy mismatch even when parameters are nominally aligned.

\paragraph{Mismatch sources.}
Common sources of mismatch include backend discrepancies (different attention kernels, precision formats, or reduction order), MoE routing
discontinuities (Top-$K$ routing can flip under small perturbations), and distributed staleness (actor--learner lag between rollout and
consumption). These effects can compound autoregressively over long horizons and lead to volatile per-token ratios.

\paragraph{Anchoring and the apparent three-distribution structure.}
Practical pipelines can exhibit an apparent three-distribution structure: (i) a rollout engine generates tokens according to an
inference-time distribution, (ii) a training framework computes gradients under $\pitheta$, and (iii) some implementations recompute the
behavior log-probabilities inside the training stack, yielding a recomputed anchor that can differ from the true rollout distribution even
when parameters are identical. Importantly, this third object is not an additional policy in our theory; it is an alternative
implementation-time anchor for the denominator of $\rho_t$.

\paragraph{Trust Region Setup.}
Following~\citet{qi2026rethinkingtrustregionllm}, the trust region (and hence the importance ratio) must be anchored to the original rollout
distribution that generated the data, which we denote by $\piroll$. Accordingly, we define $\rho_t := \pitheta(y_t\mid c_t) /
\piroll(y_t\mid c_t)$ and assume access to rollout-time cached log-probabilities $\log \piroll(y_t\mid c_t)$. The theoretical development
therefore involves only the policy pair $(\piroll,\pitheta)$, consistent with long-horizon trust-region analyses for LLM-RL (e.g.,
\citet{li2025trm}).

\paragraph{PPO-style $\piold$ Mismatch.}
In PPO-style implementations, the rollout snapshot used to generate data is often denoted by $\piold$; within a single policy update, this
snapshot coincides with the data-generating policy $\piroll$ (so $\piold \equiv \piroll$ in our notation). We use $\piold$ only for
implementation mapping and keep the theory in terms of $(\piroll,\pitheta)$.

\section{Why PPO Clipping Can Leak Gradients Under Mismatch}
\label{app:ppo-clip-leak}

\paragraph{PPO clipped surrogate.}
PPO~\citep{schulman2017proximal} constrains policy updates by clipping the per-token importance ratio. In the notation of this paper, the
ratio is
\begin{equation}
    \rho_t := \frac{\pitheta(y_t \mid c_t)}{\piroll(y_t \mid c_t)}.
\end{equation}
The clipped surrogate takes the form
\begin{equation}
    \mathcal{L}^{\mathrm{CLIP}}(\pitheta)
    = \E_{\piroll}\!\left[\sum_{t=1}^{T}\min\!\Big(
        \rho_t \hat{A}_t,\;
        \mathrm{clip}(\rho_t, 1-\epsilon_{\mathrm{clip}}, 1+\epsilon_{\mathrm{clip}})\,\hat{A}_t
    \Big)\right],
\end{equation}
where $\hat{A}_t$ is an advantage estimator and $\epsilon_{\mathrm{clip}}>0$ is the clipping threshold.

\paragraph{Asymmetry and gradient leakage.}
The clipping operator is asymmetric in the sign of $\hat{A}_t$. When $\hat{A}_t<0$ and $\rho_t \gg 1+\epsilon_{\mathrm{clip}}$, the
unclipped term can still be selected:
\begin{equation}
    \rho_t > 1+\epsilon_{\mathrm{clip}},\ \hat{A}_t<0
    \quad\Longrightarrow\quad
    \rho_t\hat{A}_t < (1+\epsilon_{\mathrm{clip}})\hat{A}_t
    = \mathrm{clip}(\rho_t, 1-\epsilon_{\mathrm{clip}}, 1+\epsilon_{\mathrm{clip}})\,\hat{A}_t,
\end{equation}
hence $\min(\cdot)$ returns $\rho_t\hat{A}_t$. In this regime, the gradient magnitude still scales with $\rho_t$, so extremely large ratios
can produce large updates even though they lie far outside the nominal trust region.

\paragraph{Why this is benign in standard RL.}
In classical on-policy settings, large ratios typically reflect genuine policy changes that over-emphasize low-quality actions; keeping the
unclipped penalty for $\hat{A}_t<0$ can be desirable as it discourages such actions.

\paragraph{Failure mode under implementation divergence.}
In LLM-RL pipelines with inference--training mismatch, large ratios can arise from numerical or backend discrepancies rather than meaningful
policy improvements. For example, a routing discontinuity in a Mixture-of-Experts model can yield a token $v$ such that
\begin{equation}
    \piroll(v\mid c_t) \approx 0.001,
    \qquad
    \pitheta(v\mid c_t) \approx 0.9,
    \qquad
    \rho_t \approx 900.
\end{equation}
Such spikes reflect implementation-level divergence and can induce a massive, erroneous gradient contribution under the unclipped branch
when $\hat{A}_t<0$. This motivates using a detached trust-region mask to discard mismatched tokens or trajectories rather than relying on a
token-level clipping heuristic.

\section{Proof of Leave-One-Out Baseline Properties}
\label{app:rloo-proof}

We provide a rigorous analysis of the leave-one-out baseline estimator used in \Cref{sec:reinforce-max}.

\begin{proposition}[Properties of the Leave-One-Out Baseline]
\label{prop:rloo-variance}
Fix a prompt $x$ and draw $n \ge 2$ i.i.d.\ responses $\{y^{(1)}, \ldots, y^{(n)}\} \sim \pitheta(\cdot|x)$ with rewards
$r_i = R(x, y^{(i)})$. Let
\begin{equation}
    f_i = \nabla_\theta \log P^{\pitheta}(y^{(i)} \mid x)
\end{equation}
denote the trajectory score function for sample $i$, and define the per-sample policy gradient contribution with baseline $b$ as
$g_i(b) = (r_i - b) f_i$. With the leave-one-out baseline $b_i = \frac{1}{n-1}\sum_{j \neq i} r_j$, the estimator
\begin{equation}
    \hat{g} = \frac{1}{n}\sum_{i=1}^{n} g_i(b_i)
\end{equation}
satisfies:
\begin{enumerate}
    \item \textbf{Unbiasedness:} $\E[\hat{g}] = \nabla_\theta J(\pitheta)$.
    \item \textbf{Baseline independence:} For each $i$, $b_i$ is independent of $(r_i, f_i)$; in particular, $\E[b_i f_i] = 0$ and
    $\Cov(f_i, b_i) = 0$.
    \item \textbf{Mean-baseline scaling:} Let $b^{\mathrm{mean}} = \frac{1}{n}\sum_{j=1}^{n} r_j$ and
    $\hat{g}^{\mathrm{mean}} = \frac{1}{n}\sum_{i=1}^{n}(r_i - b^{\mathrm{mean}}) f_i$. Then deterministically
    $\hat{g}^{\mathrm{mean}} = \frac{n-1}{n}\hat{g}$, and hence $\E[\hat{g}^{\mathrm{mean}}] = \frac{n-1}{n}\nabla_\theta J(\pitheta)$.
    \item \textbf{Second-moment decomposition:} Let $\mu(x) = \E[r_i \mid x]$ and $\sigma^2(x) = \Var(r_i \mid x)$. Then
    $\Var(b_i \mid x) = \sigma^2(x)/(n-1)$, and
    \begin{equation}
        \E\!\big[(r_i - b_i)^2 f_i f_i^\top \mid x\big]
        = \E\!\big[(r_i - \mu(x))^2 f_i f_i^\top \mid x\big] + \Var(b_i \mid x)\, \E[f_i f_i^\top \mid x].
        \label{eq:rloo-second-moment}
    \end{equation}
\end{enumerate}
\end{proposition}

\begin{remark}[Relationship between the two estimators]
The deterministic proportionality $\hat{g}^{\mathrm{mean}} = \frac{n-1}{n}\hat{g}$ implies that the mean-baseline estimator is a
rescaled (and therefore biased) version of the leave-one-out estimator: its smaller raw variance is entirely due to the constant scaling
factor. In practice, this shrinkage can be absorbed into the learning rate, but it breaks exact unbiasedness. The main advantage of the
leave-one-out baseline is that $b_i$ is independent of sample $i$, which simplifies moment calculations and avoids self-correlation terms
that arise when the baseline depends on $r_i$.
\end{remark}

\begin{remark}
The leave-one-out baseline is equivalent to the leave-one-out control variate from the classical REINFORCE literature~\citep{williams1992simple}. In the LLM-RL setting with $n$ samples per prompt, each sample's baseline is independent of its own trajectory, making it particularly well-suited for group-based sampling strategies.
\end{remark}

\begin{proof}
We prove all statements conditional on a fixed prompt $x$; taking the outer expectation over $x\sim P(x)$ yields the proposition for the
full objective $J(\pitheta)$.

\textbf{(1) Unbiasedness.} Since $b_i$ depends only on $\{r_j\}_{j\neq i}$, it is independent of $(r_i,f_i)$ conditional on $x$. Hence
\begin{align}
    \E\!\big[(r_i-b_i)f_i \mid x\big]
    &= \E\!\big[r_i f_i \mid x\big] - \E\!\big[b_i \mid x\big]\E\!\big[f_i \mid x\big].
\end{align}
By the score-function identity,
\begin{equation}
    \E\!\big[f_i \mid x\big] = \E\!\big[\nabla_\theta \log P^{\pitheta}(y^{(i)}\mid x)\mid x\big]
    = \nabla_\theta \E\!\big[1 \mid x\big] = 0,
\end{equation}
and
\begin{equation}
    \E\!\big[r_i f_i \mid x\big]
    = \E\!\Big[r_i\,\nabla_\theta \log P^{\pitheta}(y^{(i)}\mid x)\,\Big|\,x\Big]
    = \nabla_\theta \E[r_i \mid x].
\end{equation}
Therefore $\E[(r_i-b_i)f_i\mid x] = \nabla_\theta \E[r_i\mid x]$. Averaging over $i$ yields
\begin{equation}
    \E[\hat{g}\mid x] = \nabla_\theta \E[r_1\mid x] = \nabla_\theta J_x(\pitheta),
    \qquad
    J_x(\pitheta) := \E_{y\sim \pitheta(\cdot\mid x)}[R(x,y)].
\end{equation}
Taking the outer expectation over $x$ gives $\E[\hat{g}] = \nabla_\theta J(\pitheta)$.

\textbf{(2) Baseline independence.} The same conditional independence implies
\begin{equation}
    \E[b_i f_i \mid x] = \E[b_i\mid x]\E[f_i\mid x] = 0,
\end{equation}
so $\Cov(f_i,b_i)=0$.

\textbf{(3) Mean-baseline scaling.} For each $i$, write
\begin{align}
    b^{\mathrm{mean}}
    &= \frac{1}{n}\sum_{j=1}^{n} r_j
    = \frac{r_i}{n} + \frac{1}{n}\sum_{j\neq i} r_j
    = \frac{r_i}{n} + \frac{n-1}{n}\,b_i,
    \\
    r_i - b^{\mathrm{mean}}
    &= r_i - \frac{r_i}{n} - \frac{n-1}{n}\,b_i
    = \frac{n-1}{n}(r_i-b_i).
\end{align}
Summing over $i$ gives the deterministic identity $\hat{g}^{\mathrm{mean}} = \frac{n-1}{n}\hat{g}$, and taking expectations yields
$\E[\hat{g}^{\mathrm{mean}}] = \frac{n-1}{n}\nabla_\theta J(\pitheta)$.

\textbf{(4) Second-moment decomposition.} Conditional on $x$, $b_i$ is the average of $n-1$ i.i.d.\ rewards $\{r_j\}_{j\neq i}$, hence
\begin{equation}
    \Var(b_i \mid x) = \frac{\sigma^2(x)}{n-1}.
\end{equation}
Write $r_i-b_i = (r_i-\mu(x)) - (b_i-\mu(x))$. Then
\begin{align}
    (r_i-b_i)^2
    &= (r_i-\mu(x))^2 - 2(r_i-\mu(x))(b_i-\mu(x)) + (b_i-\mu(x))^2.
\end{align}
Multiplying by $f_i f_i^\top$ and taking conditional expectation gives
\begin{align}
    \E\!\big[(r_i-b_i)^2 f_i f_i^\top \mid x\big]
    &= \E\!\big[(r_i-\mu(x))^2 f_i f_i^\top \mid x\big]
    - 2\,\E\!\big[(r_i-\mu(x))(b_i-\mu(x)) f_i f_i^\top \mid x\big]
    \nonumber\\
    &\quad
    + \E\!\big[(b_i-\mu(x))^2 f_i f_i^\top \mid x\big].
\end{align}
The cross term vanishes by conditional independence and $\E[b_i-\mu(x)\mid x]=0$:
\begin{align}
    \E\!\big[(r_i-\mu(x))(b_i-\mu(x)) f_i f_i^\top \mid x\big]
    &= \E[b_i-\mu(x)\mid x]\;\E\!\big[(r_i-\mu(x)) f_i f_i^\top \mid x\big]
    = 0.
\end{align}
The last term factorizes as
\begin{align}
    \E\!\big[(b_i-\mu(x))^2 f_i f_i^\top \mid x\big]
    &= \E\!\big[(b_i-\mu(x))^2 \mid x\big]\;\E\!\big[f_i f_i^\top \mid x\big]
    = \Var(b_i\mid x)\,\E[f_i f_i^\top\mid x],
\end{align}
which yields \eqref{eq:rloo-second-moment}.
\end{proof}


\section{Optimality of \texorpdfstring{$\gamma = 1.0$}{gamma = 1.0}}
\label{app:gamma-one}

\begin{proposition}[Optimality of $\gamma = 1.0$]
When the extrinsic reward is sparse (assigned only at the final token), $\gamma = 1.0$ is the unique discount factor that preserves unbiasedness of the REINFORCE gradient estimator for the undiscounted objective $J(\pitheta) = \E[R(x,y)]$.
\end{proposition}

\begin{proof}
For the undiscounted objective $J(\pitheta)=\E[R(x,y)]$, the REINFORCE identity gives
\begin{equation}
    \nabla_\theta J(\pitheta)
    = \E\!\Big[R(x,y)\sum_{t=1}^{T}\nabla_\theta \log \pitheta(y_t \mid c_t)\Big].
\end{equation}
In a sparse terminal-reward setting, using a discounted return with $\gamma<1$ amounts to reweighting earlier score terms, yielding the
alternative estimator
\begin{equation}
    g_\gamma(\theta)
    := \E\!\Big[R(x,y)\sum_{t=1}^{T}\gamma^{T-t}\nabla_\theta \log \pitheta(y_t \mid c_t)\Big].
\end{equation}
The difference is
\begin{align}
    g_\gamma(\theta) - \nabla_\theta J(\pitheta)
    &= \E\!\Big[R(x,y)\sum_{t=1}^{T}\big(\gamma^{T-t}-1\big)\nabla_\theta \log \pitheta(y_t \mid c_t)\Big]
    \nonumber\\
    &= \E\!\Big[R(x,y)\sum_{t=1}^{T-1}\big(\gamma^{T-t}-1\big)\nabla_\theta \log \pitheta(y_t \mid c_t)\Big],
    \label{eq:gamma-bias-diff}
\end{align}
where the last equality uses $\gamma^{T-T}-1=0$ at $t=T$. For any $\gamma<1$, the coefficients satisfy $\gamma^{T-t}-1<0$ for all
$t\le T-1$. Unless the expectations $\E[R(x,y)\nabla_\theta \log \pitheta(y_t\mid c_t)]$ vanish for all $t\le T-1$ (a degenerate case in
autoregressive generation), \eqref{eq:gamma-bias-diff} is nonzero, hence the estimator is biased. The only choice of $\gamma$ that
removes the reweighting for all $t$ is $\gamma=1$.
\end{proof}


\section{Proof of Theorem~\ref{thm:prefix-tighter}}
\label{app:prefix-proof}

We provide a complete proof of \Cref{thm:prefix-tighter} under the definitions in \Cref{sec:prefix-is}. For convenience, write the
log-threshold interval as
\begin{equation}
    [\log(1-\epslow), \log(1+\epshigh)] = [-\tau_-, \tau_+],
    \qquad
    \tau_+ := \log(1+\epshigh),
    \qquad
    \tau_- := -\log(1-\epslow),
\end{equation}
so that $\rho_t \in [1-\epslow,1+\epshigh]$ is equivalent to $\log\rho_t \in [-\tau_-,\tau_+]$.

\paragraph{Mask definitions.} The three masks compared in \Cref{thm:prefix-tighter} are:
\begin{itemize}
    \item \textbf{Token-level (IcePop / token-level ratio mask):}
    \begin{equation}
        M_t^{\mathrm{tok}}
        := \mathbb{I}\!\big[\rho_t \in [1-\epslow,1+\epshigh]\big]
        = \mathbb{I}\!\big[\log\rho_t \in [-\tau_-,\tau_+]\big].
    \end{equation}
    \item \textbf{Sequence-level (GSPO / sequence-level ratio gate):}
    \begin{equation}
        M_t^{\mathrm{seq}}
        := \mathbb{I}\!\big[\log\rhoseq \in [-\tau_-,\tau_+]\big],
        \qquad
        \log\rhoseq := \frac{1}{T}\sum_{s=1}^{T}\log\rho_s,
    \end{equation}
    for all $t$.
    \item \textbf{Prefix-level (Causal Trust Region proxy):}
    \begin{equation}
        M_t^{\mathrm{pre}}
        := \mathbb{I}\!\big[\rhoprefix{t} \in [1-\epslow,1+\epshigh]\big]
        = \mathbb{I}\!\big[\log\rhoprefix{t} \in [-\tau_-,\tau_+]\big],
    \end{equation}
    where $\rhoprefix{t}$ is defined in \eqref{eq:prefix-is}.
\end{itemize}

\begin{proof}[Proof of \Cref{thm:prefix-tighter}]
\noindent\textbf{Part (a): Early deviation.}
Assume $\rho_1 \notin [1-\epslow,1+\epshigh]$ and $|\log\rho_t| \le \varepsilon$ for all $t>1$, with
$0 \le \varepsilon \le \min(\tau_+,\tau_-)$. Then for every $t>1$,
\begin{equation}
    \log\rho_t \in [-\varepsilon,\varepsilon] \subseteq [-\tau_-,\tau_+],
\end{equation}
so $M_t^{\mathrm{tok}}=1$ for all $t>1$, while $M_1^{\mathrm{tok}}=0$ (since $\rho_1 \notin [1-\epslow,1+\epshigh]$). Hence token-level
IcePop masks only position $t=1$.

\smallskip
For the prefix mask, note first that $M_1^{\mathrm{pre}}=0$ because $\rhoprefix{1}=\rho_1\notin[1-\epslow,1+\epshigh]$. To quantify how
long the rejection can persist, use \eqref{eq:prefix-is}:
\begin{equation}
    \log\rhoprefix{t}
    = \frac{1}{t}\sum_{s=1}^{t} \log\rho_s
    = \frac{1}{t}\left(\log\rho_1 + \sum_{s=2}^{t}\log\rho_s\right).
\end{equation}
If $\log\rho_1>\tau_+$, then $\log\rho_s \ge -\varepsilon$ for all $s\ge2$, hence $\sum_{s=2}^{t}\log\rho_s \ge -(t-1)\varepsilon$ and therefore
\begin{align}
    \log\rhoprefix{t}
    &\ge \frac{1}{t}\big(\log\rho_1-(t-1)\varepsilon\big)
    = \frac{\log\rho_1+\varepsilon}{t} - \varepsilon.
\end{align}
Thus, if $t < (\log\rho_1+\varepsilon)/(\tau_+ + \varepsilon)$, then $\log\rhoprefix{t}>\tau_+$ and hence $M_t^{\mathrm{pre}}=0$.
The lower-violation case $\log\rho_1<-\tau_-$ is symmetric: since $\log\rho_s \le \varepsilon$ for $s\ge2$, we have
$\sum_{s=2}^{t}\log\rho_s \le \varepsilon(t-1)$ and therefore
\begin{align}
    \log\rhoprefix{t}
    &\le \frac{1}{t}\big(\log\rho_1+\varepsilon(t-1)\big)
    = \frac{\log\rho_1-\varepsilon}{t} + \varepsilon.
\end{align}
If $t < (-\log\rho_1+\varepsilon)/(\tau_-+\varepsilon)$, then $\log\rhoprefix{t} < -\tau_-$ and hence $M_t^{\mathrm{pre}}=0$.
This proves the sufficient conditions in \Cref{eq:early-upper-sufficient,eq:early-lower-sufficient}.

\par
\noindent\textbf{Part (b): Late deviation.}
Assume there exists $t^*\in\{1,\ldots,T\}$ such that $\rho_{t^*}\notin[1-\epslow,1+\epshigh]$ and $|\log\rho_t|\le \varepsilon$
for all $t\neq t^*$, with
$0 \le \varepsilon \le \min(\tau_+,\tau_-)$. For any $t<t^*$, all active log-ratios in the prefix satisfy $\log\rho_s \in [-\tau_-,\tau_+]$.
Since $\log\rhoprefix{t} = \frac{1}{t}\sum_{s=1}^{t}\log\rho_s$ is an average of values in $[-\tau_-,\tau_+]$, it follows that
$\log\rhoprefix{t} \in [-\tau_-,\tau_+]$, and hence $M_t^{\mathrm{pre}}=1$ for all $t<t^*$. Therefore prefix masking can only occur
for positions $t\ge t^*$, which implies
\begin{equation}
    \big|\{t : M_t^{\mathrm{pre}} = 0\}\big| \le T - t^* + 1.
\end{equation}
The sequence-level IS mask $M_t^{\mathrm{seq}}$ is constant across positions by definition, so it either rejects all tokens or rejects none.

For the claimed regime where prefix rejects but sequence-level accepts, consider the pure single-spike case where $\rho_t=1$ for
$t\neq t^*$ and $\rho_{t^*}>1$. Then
\[
    \log\rhoseq = \frac{1}{T}\log\rho_{t^*},
    \qquad
    \log\rhoprefix{t^*} = \frac{1}{t^*}\log\rho_{t^*}.
\]
Since $t^* \le T$ and $\log\rho_{t^*}>0$, we have $\log\rhoprefix{t^*} \ge \log\rhoseq$. Thus whenever
\begin{equation}
    \frac{1}{t^*}\log\rho_{t^*} > \tau_+
    \quad\text{and}\quad
    \frac{1}{T}\log\rho_{t^*} \le \tau_+,
\end{equation}
we obtain $M_{t^*}^{\mathrm{pre}}=0$ while $M_t^{\mathrm{seq}}=1$ for all $t$. (The lower-threshold case is analogous with $-\tau_-$.)

\par
\noindent\textbf{Part (c): Monotone violation.}
If $\log\rho_s > \tau_+$ for all $s$, then for every $t$ the prefix average satisfies $\log\rhoprefix{t} > \tau_+$, since it is an
average of values each strictly larger than $\tau_+$. Hence $M_t^{\mathrm{pre}}=0$ for all $t$. The sequence-level average
$\log\rhoseq$ is also strictly larger than $\tau_+$, so $M_t^{\mathrm{seq}}=0$ for all $t$. The lower-violation case $\log\rho_s < -\tau_-$
is symmetric.
\end{proof}

\section{Alternative Trust-Region Masking Variants}
\label{app:mask-variants}

This appendix collects several masking variants that are compatible with the ratio setup in the main text
(\Cref{sec:offpolicy-problem}). Throughout, samples are generated by the behavior policy $\piroll$ and gradients are computed under the
training policy $\pitheta$. We write the per-token importance ratio as
\begin{equation}
    \rho_t := \frac{\pitheta(y_t \mid c_t)}{\piroll(y_t \mid c_t)}.
\end{equation}
We also reuse the prefix statistic $\rhoprefix{t}$ from \Cref{def:prefix-is}.
For thresholds $\epslow\in(0,1)$ and $\epshigh>0$, define the log-threshold interval
\begin{equation}
    [\log(1-\epslow),\log(1+\epshigh)] = [-\tau_-,\tau_+],
    \qquad
    \tau_+ := \log(1+\epshigh),
    \qquad
    \tau_- := -\log(1-\epslow).
\end{equation}

\paragraph{Variant I (absorbing prefix-average mask).}

The main text uses a pointwise prefix gate at each position $t$. A more rejection-style variant enforces an \emph{absorbing} semantics:
once a prefix violates the threshold, all subsequent positions are rejected.

\begin{equation}
    \widetilde{M}_t^{\mathrm{pre}}
    := \mathbb{I}\!\big[\rhoprefix{t} \in [1-\epslow,1+\epshigh]\big],
    \qquad
    M_t^{\mathrm{abs\text{-}pre}}
    := \prod_{u=1}^{t}\widetilde{M}_u^{\mathrm{pre}}.
\end{equation}

\paragraph{Variant II (prefix-max mask / max-filtering).}

Max-based trust-region criteria can be expressed via a sample-based proxy $k_2(\rho)$:
\begin{equation}
    k_2(\rho) := \tfrac{1}{2}(\log\rho)^2.
\end{equation}
The corresponding prefix-max statistic and mask are
\begin{equation}
    S_t^{\max} := \max_{1\le s\le t} k_2(\rho_s),
    \qquad
    M_t^{\mathrm{pre\text{-}max}} := \mathbb{I}\!\big[S_t^{\max} \le \delta_{\max}\big].
\end{equation}
Equivalently, $M_t^{\mathrm{pre\text{-}max}}=1$ iff $\max_{s\le t}|\log\rho_s| \le \sqrt{2\delta_{\max}}$.

\paragraph{Variant III (prefix-score mask / bound-driven).}

Score-based masking controls a prefix-level aggregate of per-token ratio deviations. Define a nonnegative per-token deviation
\begin{equation}
    a_t := |\rho_t - 1|
\end{equation}
and deterministic nonnegative weights $(w_t)_{t\ge1}$ with $\sum_{s\le t} w_s > 0$. The prefix score is
\begin{equation}
    \widetilde{W}_t
    := \frac{\sum_{s=1}^{t} w_s a_s}{\sum_{s=1}^{t} w_s},
\end{equation}
and the corresponding mask is
\begin{equation}
    M_t^{\mathrm{pre\text{-}score}} := \mathbb{I}\!\big[\widetilde{W}_t \le \delta_W\big].
\end{equation}
A special case is the uniform-weight choice $w_s\equiv 1$, which yields $\widetilde{W}_T = \frac{1}{T}\sum_{t=1}^{T}|\rho_t-1|$ at the
full horizon.

\paragraph{Variant IV (prefix abnormal-token rate / TRM-style).}

TRM~\citep{li2025trm} uses per-token divergence estimates (or their sample-based proxies) to decide whether a trajectory lies inside a
trust region. We can adapt this idea to a prefix-level gate by counting \emph{abnormal} tokens in the prefix. Concretely, define the
abnormal-token indicator
\begin{equation}
    B_t := \mathbb{I}\!\big[\Dkltok(c_t;\piroll,\pitheta) > \delta_{\mathrm{TRM}}\big],
\end{equation}
and the prefix abnormal count and rate
\begin{equation}
    N_t := \sum_{s=1}^{t} B_s,
    \qquad
    \widehat{p}_t := \frac{N_t}{t}.
\end{equation}
Given a rate threshold $\xi\in[0,1]$, the corresponding prefix mask is
\begin{equation}
    M_t^{\mathrm{pre\text{-}anom}}
    := \mathbb{I}\!\big[\widehat{p}_t \le \xi\big].
\end{equation}
In practice, one may instantiate $B_t$ using TRM's computation method for $\Dkltok(c_t;\piroll,\pitheta)$; for example, via the
sample-based proxy $k_3(\rho):=\rho-1-\log\rho$, which satisfies
$\E_{\piroll}[k_3(\rho_t)\mid c_t]=\DKL(\piroll(\cdot\mid c_t)\,\|\,\pitheta(\cdot\mid c_t))$ under the ratio definition
$\rho_t=\pitheta/\piroll$ and support overlap.


\section{Numerical Stability}
\label{app:numerical}

The adaptive normalization (\Cref{sec:adaptive-norm}) includes several safeguards:

\begin{remark}[Numerical thresholds vs.\ divergence bounds]
The symbols $\varepsilon_{\mathrm{num}}$ and $\varepsilon_{\mathrm{tol}}$ in this appendix denote implementation hyperparameters used for
numerical stability and uniform-reward detection. They are unrelated to the divergence upper bound $\epsilon$ in \Cref{def:max-div}.
\end{remark}

\begin{enumerate}
    \item \textbf{Sum threshold:} If $S^+ < \varepsilon_{\mathrm{num}}$ or $S^- < \varepsilon_{\mathrm{num}}$ (default $\varepsilon_{\mathrm{num}} = 10^{-8}$), normalization is skipped and the original advantages are returned.

    \item \textbf{Scale clamping:} $\alpha$ and $\beta$ are clamped to $[\varepsilon_{\mathrm{num}}, s_{\max}]$ (default $s_{\max} = 10$) to prevent extreme scaling.

    \item \textbf{Intermediate overflow protection:} The quantity $(S^+/S^-)^2 Q^-$ is clamped to $10^8$ before computing $\alpha$.

    \item \textbf{Finiteness check:} If $\alpha$ or $\beta$ is NaN or $\pm\infty$ after computation, normalization is skipped.
\end{enumerate}

These safeguards may cause the normalized advantages to deviate slightly from the exact empirical constraints in
\Cref{def:adaptive-norm}, but in practice the deviation is negligible ($|\text{mean}| < 0.001$, $|\text{var} - 1| < 0.01$). These
thresholds are heuristic defaults; formal validation is deferred to the experimental evaluation.

\textbf{Fallback conditions.} Normalization is skipped entirely when:
\begin{itemize}
    \item All advantages have the same sign ($\mathcal{P} = \emptyset$ or $\mathcal{N} = \emptyset$), indicating the group has uniform quality and the leave-one-out-shaped advantages are already meaningful.
    \item The uniform scale mode (\Cref{app:uniform-scale}) is active and the group has uniform rewards.
\end{itemize}


\section{Uniform Scale Mode}
\label{app:uniform-scale}

The uniform scale mode is an optional technique to improve sample utilization. When all $n$ samples for a prompt have identical rewards (all correct or all incorrect), the leave-one-out baseline yields $\tilde{r}_i = 0$ for all $i$, eliminating the gradient signal. Uniform scale mode recovers this signal.

\begin{definition}[Uniform Scale]
For prompt groups where $\mathrm{std}(\{r_1, \ldots, r_n\}) < \varepsilon_{\mathrm{tol}}$ (uniform-reward groups), the shaped reward is set to $\tilde{r}_i = r_i / n$, and the adaptive normalization (\Cref{sec:adaptive-norm}) is skipped.
\end{definition}

\begin{proposition}
Under uniform scale mode, the gradient for uniform-reward groups is proportional to $r_i \sum_t \nabla_\theta \log \pitheta(y_{i,t}|c_{i,t})$, which pushes the policy toward all sampled responses when $r_i > 0$ (all correct) and away from them when $r_i < 0$ (all incorrect), preserving gradient signal that the leave-one-out baseline would otherwise eliminate.
\end{proposition}

\begin{proof}
With $\tilde{r}_i = r_i/n$ and no normalization, the sample-$i$ contribution to the REINFORCE gradient is proportional to
\[
    \frac{r_i}{n} \sum_t \nabla_\theta \log \pitheta(y_{i,t}\mid c_{i,t}).
\]
Hence $\sign(r_i)$ determines the update direction: when $r_i > 0$ the policy increases the log-probability of the sampled tokens, and
when $r_i < 0$ it decreases them.
\end{proof}
