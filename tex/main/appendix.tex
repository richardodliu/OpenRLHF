\newpage
\appendix

\section{Proof of Leave-One-Out Baseline Properties}
\label{app:rloo-proof}

We provide a rigorous analysis of the leave-one-out baseline estimator used in \Cref{sec:reinforce-max}.

\begin{proposition}[Properties of the Leave-One-Out Baseline]
\label{prop:rloo-variance}
Fix a prompt $x$ and draw $n \ge 2$ i.i.d.\ responses $\{y^{(1)}, \ldots, y^{(n)}\} \sim \pitheta(\cdot|x)$ with rewards
$r_i = R(x, y^{(i)})$. Let
\begin{equation}
    f_i = \nabla_\theta \log P^{\pitheta}(y^{(i)} \mid x)
\end{equation}
denote the trajectory score function for sample $i$, and define the per-sample policy gradient contribution with baseline $b$ as
$g_i(b) = (r_i - b) f_i$. With the leave-one-out baseline $b_i = \frac{1}{n-1}\sum_{j \neq i} r_j$, the estimator
\begin{equation}
    \hat{g} = \frac{1}{n}\sum_{i=1}^{n} g_i(b_i)
\end{equation}
satisfies:
\begin{enumerate}
    \item \textbf{Unbiasedness:} $\E[\hat{g}] = \nabla_\theta J(\pitheta)$.
    \item \textbf{Baseline independence:} For each $i$, $b_i$ is independent of $(r_i, f_i)$; in particular, $\E[b_i f_i] = 0$ and
    $\Cov(f_i, b_i) = 0$.
    \item \textbf{Mean-baseline scaling:} Let $b^{\mathrm{mean}} = \frac{1}{n}\sum_{j=1}^{n} r_j$ and
    $\hat{g}^{\mathrm{mean}} = \frac{1}{n}\sum_{i=1}^{n}(r_i - b^{\mathrm{mean}}) f_i$. Then deterministically
    $\hat{g}^{\mathrm{mean}} = \frac{n-1}{n}\hat{g}$, and hence $\E[\hat{g}^{\mathrm{mean}}] = \frac{n-1}{n}\nabla_\theta J(\pitheta)$.
    \item \textbf{Second-moment decomposition:} Let $\mu(x) = \E[r_i \mid x]$ and $\sigma^2(x) = \Var(r_i \mid x)$. Then
    $\Var(b_i \mid x) = \sigma^2(x)/(n-1)$, and
    \begin{equation}
        \E\!\big[(r_i - b_i)^2 f_i f_i^\top \mid x\big]
        = \E\!\big[(r_i - \mu(x))^2 f_i f_i^\top \mid x\big] + \Var(b_i \mid x)\, \E[f_i f_i^\top \mid x].
        \label{eq:rloo-second-moment}
    \end{equation}
\end{enumerate}
\end{proposition}

\begin{remark}[Relationship between the two estimators]
\label{rem:rloo-mean-scaling}
The deterministic proportionality $\hat{g}^{\mathrm{mean}} = \frac{n-1}{n}\hat{g}$ implies that the mean-baseline estimator is a
rescaled (and therefore biased) version of the leave-one-out estimator: its smaller raw variance is entirely due to the constant scaling
factor. In practice, this shrinkage can be absorbed into the learning rate, but it breaks exact unbiasedness. The main advantage of the
leave-one-out baseline is that $b_i$ is independent of sample $i$, which simplifies moment calculations and avoids self-correlation terms
that arise when the baseline depends on $r_i$.
\end{remark}

\begin{remark}
The leave-one-out baseline is equivalent to the leave-one-out control variate from the classical REINFORCE literature~\citep{williams1992simple}. In the LLM-RL setting with $n$ samples per prompt, each sample's baseline is independent of its own trajectory, making it particularly well-suited for group-based sampling strategies.
\end{remark}

\begin{proof}
We give a self-contained proof under group sampling; taking the outer expectation over prompts $x \sim P(x)$ yields the statement for the
full objective $J(\pitheta)$.

\textbf{Unbiasedness.} Since $b_i$ depends only on $\{r_j\}_{j \neq i}$, it is independent of $(y^{(i)}, r_i, f_i)$. Therefore
\[
    \E[(r_i - b_i) f_i \mid x] = \E[r_i f_i \mid x] - \E[b_i \mid x]\E[f_i \mid x].
\]
By the score-function identity, $\E[f_i \mid x] = \nabla_\theta \E[1 \mid x] = 0$, and $\E[r_i f_i \mid x] = \nabla_\theta \E[r_i \mid x]$.
Thus $\E[(r_i - b_i) f_i \mid x] = \nabla_\theta \E[r_i \mid x] = \nabla_\theta J_x(\pitheta)$, where $J_x(\pitheta)=\E[R(x,y)\mid x]$.
Averaging over $i$ and then over $x$ yields $\E[\hat{g}] = \nabla_\theta J(\pitheta)$.

\textbf{Baseline independence.} The same independence implies $\E[b_i f_i \mid x] = \E[b_i \mid x]\E[f_i \mid x] = 0$, hence
$\Cov(f_i,b_i)=0$.

\textbf{Mean-baseline scaling.} Let $b^{\mathrm{mean}} = \frac{1}{n}\sum_{j=1}^{n} r_j$. For each $i$,
\[
    b^{\mathrm{mean}} = \frac{r_i}{n} + \frac{n-1}{n} b_i
    \quad\Longrightarrow\quad
    r_i - b^{\mathrm{mean}} = \frac{n-1}{n}(r_i - b_i).
\]
Summing over $i$ gives $\hat{g}^{\mathrm{mean}} = \frac{n-1}{n}\hat{g}$ deterministically. Taking expectations yields
$\E[\hat{g}^{\mathrm{mean}}] = \frac{n-1}{n}\nabla_\theta J(\pitheta)$.

\textbf{Second-moment decomposition.} Conditional on $x$, the random variable $b_i$ is the average of $n-1$ i.i.d.\ rewards
$\{r_j\}_{j\neq i}$, hence $\Var(b_i \mid x) = \sigma^2(x)/(n-1)$. For \eqref{eq:rloo-second-moment}, write
\[
    r_i - b_i = (r_i - \mu(x)) - (b_i - \mu(x)).
\]
Expanding the square and multiplying by $f_i f_i^\top$ gives
\[
    (r_i - b_i)^2 f_i f_i^\top
    = (r_i - \mu(x))^2 f_i f_i^\top - 2(r_i - \mu(x))(b_i - \mu(x)) f_i f_i^\top + (b_i - \mu(x))^2 f_i f_i^\top.
\]
Taking conditional expectation and using independence of $b_i$ from $(r_i,f_i)$ (given $x$), the cross term vanishes:
\[
    \E[(r_i - \mu(x))(b_i - \mu(x)) f_i f_i^\top \mid x]
    = \E[b_i - \mu(x) \mid x]\E[(r_i - \mu(x)) f_i f_i^\top \mid x] = 0.
\]
The last term factorizes as $\E[(b_i - \mu(x))^2 f_i f_i^\top \mid x] = \Var(b_i\mid x)\, \E[f_i f_i^\top\mid x]$, which yields
\eqref{eq:rloo-second-moment}.
\end{proof}


\section{Optimality of \texorpdfstring{$\gamma = 1.0$}{gamma = 1.0}}
\label{app:gamma-one}

\begin{proposition}[Optimality of $\gamma = 1.0$]
\label{prop:gamma-one}
When the extrinsic reward is sparse (assigned only at the final token), $\gamma = 1.0$ is the unique discount factor that preserves unbiasedness of the REINFORCE gradient estimator for the undiscounted objective $J(\pitheta) = \E[R(x,y)]$.
\end{proposition}

\begin{proof}
For the undiscounted objective $J(\pitheta)=\E[R(x,y)]$, the REINFORCE identity gives
\[
    \nabla_\theta J(\pitheta)
    = \E\!\Big[R(x,y)\sum_{t=1}^{T}\nabla_\theta \log \pitheta(y_t \mid c_t)\Big].
\]
If one instead uses a discounted return with $\gamma<1$ in a sparse terminal-reward setting, the resulting estimator corresponds to
\[
    \E\!\Big[R(x,y)\sum_{t=1}^{T}\gamma^{T-t}\nabla_\theta \log \pitheta(y_t \mid c_t)\Big],
\]
whose difference from $\nabla_\theta J(\pitheta)$ is
\[
    \E\!\Big[R(x,y)\sum_{t=1}^{T-1}\big(\gamma^{T-t}-1\big)\nabla_\theta \log \pitheta(y_t \mid c_t)\Big].
\]
This term is not identically zero in general (e.g., when early-token probabilities affect the terminal reward), hence the estimator is
biased for any $\gamma<1$. The unique choice that matches the undiscounted objective is $\gamma=1$.
\end{proof}


\section{Proof of Theorem~\ref{thm:prefix-tighter}}
\label{app:prefix-proof}

We provide a complete proof under the definitions in \Cref{sec:prefix-is}. For convenience, write the log-threshold interval as
\[
    [\log\lambda, \log\Lambda] = [-\tau_-, \tau_+], \qquad \tau_+ := \log\Lambda,\;\; \tau_- := -\log\lambda,
\]
so that $w_t \in [\lambda,\Lambda]$ is equivalent to $\ell_t \in [-\tau_-,\tau_+]$.

\paragraph{Mask definitions.} The three masks compared in \Cref{thm:prefix-tighter} are:
\begin{itemize}
    \item \textbf{Token-level (ICEPOP):} $M_t^{\mathrm{token}} = \mathbb{I}[w_t \in [\lambda,\Lambda]] = \mathbb{I}[\ell_t \in [-\tau_-,\tau_+]]$.
    \item \textbf{Sequence-level (seq-mask-tis):} $M_t^{\mathrm{seq}} = \mathbb{I}[\bar{\ell} \in [-\tau_-,\tau_+]]$ for all $t$,
    where $\bar{\ell} := \frac{1}{P_T}\sum_{s=1}^{T}\ell_s m_s$ is the sequence-level average log-ratio.
    \item \textbf{Prefix cumulative:} $M_t^{\mathrm{prefix}} = \mathbb{I}[L_t/P_t \in [-\tau_-,\tau_+]]$, where
    $L_t = \sum_{s\le t}\ell_s m_s$ and $P_t=\sum_{s\le t}m_s$.
\end{itemize}

\begin{proof}[Proof of \Cref{thm:prefix-tighter}]
\textbf{Part (a).} Assume $w_1 \notin [\lambda,\Lambda]$ and $|\ell_t| \le \epsilon$ for all $t>1$, with
$0 \le \epsilon \le \min(\tau_+,\tau_-)$. Then for every $t>1$,
\[
    \ell_t \in [-\epsilon,\epsilon] \subseteq [-\tau_-,\tau_+],
\]
so $M_t^{\mathrm{token}}=1$ for all $t>1$, while $M_1^{\mathrm{token}}=0$ (since $w_1 \notin [\lambda,\Lambda]$). Hence token-level
ICEPOP masks only position $t=1$.

For prefix IS, write (assuming $m_1=1$ for the first action token)
\[
    L_t = \ell_1 + \sum_{s=2}^{t}\ell_s m_s, \qquad P_t = 1 + \sum_{s=2}^{t}m_s.
\]
If $\ell_1>\tau_+$, then $\ell_s m_s \ge -\epsilon m_s$ for all $s\ge2$, so
\[
    L_t \ge \ell_1 - \epsilon(P_t-1).
\]
Therefore, if $P_t < (\ell_1+\epsilon)/(\tau_+ + \epsilon)$ (the condition in \Cref{eq:early-upper-sufficient}), then
\[
    \frac{L_t}{P_t} \ge \frac{\ell_1-\epsilon(P_t-1)}{P_t} > \tau_+,
\]
so $M_t^{\mathrm{prefix}}=0$. The lower-violation case $\ell_1<-\tau_-$ is symmetric: since $\ell_s m_s \le \epsilon m_s$ for $s\ge2$,
we have $L_t \le \ell_1 + \epsilon(P_t-1)$, and the condition in \Cref{eq:early-lower-sufficient} implies $L_t/P_t < -\tau_-$, hence
$M_t^{\mathrm{prefix}}=0$. In particular, prefix IS always masks at least position $t=1$, and under the sufficient conditions masks a
longer prefix.

\textbf{Part (b).} Assume there exists $t^*\in\{1,\ldots,T\}$ such that $|\ell_t|\le \epsilon$ for all $t\neq t^*$, with
$0 \le \epsilon \le \min(\tau_+,\tau_-)$. For any $t<t^*$, all active log-ratios in the prefix satisfy $\ell_s \in [-\tau_-,\tau_+]$.
Since $L_t/P_t$ is a weighted average of these in-bound values (with weights $m_s/P_t$), it follows that $L_t/P_t \in [-\tau_-,\tau_+]$,
and hence $M_t^{\mathrm{prefix}}=1$ for all $t<t^*$. Therefore prefix masking can only occur for positions $t\ge t^*$, which implies
\[
    \big|\{t : M_t^{\mathrm{prefix}} = 0\}\big| \le T - t^* + 1.
\]
The seq-mask-tis mask $M_t^{\mathrm{seq}}$ is constant across positions by definition, so it either rejects all tokens or rejects none.

For the claimed regime where prefix rejects but sequence-level accepts, consider the pure single-spike case $\ell_{t^*}=L$ and $\ell_t=0$
for $t\neq t^*$. Then $\bar{\ell} = L/P_T$ and $L_{t^*}/P_{t^*} = L/P_{t^*}$. Since $P_{t^*} \le P_T$, we have $L/P_{t^*} \ge L/P_T$.
Thus whenever
\[
    \frac{L}{P_{t^*}} > \tau_+
    \quad\text{and}\quad
    \frac{L}{P_T} \le \tau_+,
\]
we obtain $M_{t^*}^{\mathrm{prefix}}=0$ while $M_t^{\mathrm{seq}}=1$ for all $t$. (The lower-threshold case is analogous with $-\tau_-$.)

\textbf{Part (c).} If $\ell_s > \tau_+$ for all $s$, then for every $t$ the prefix average satisfies $L_t/P_t > \tau_+$, since it is an
average of values each strictly larger than $\tau_+$. Hence $M_t^{\mathrm{prefix}}=0$ for all $t$. The sequence-level average
$\bar{\ell}$ is also strictly larger than $\tau_+$, so $M_t^{\mathrm{seq}}=0$ for all $t$. The lower-violation case $\ell_s < -\tau_-$
is symmetric.
\end{proof}


\section{Numerical Stability}
\label{app:numerical}

The adaptive normalization (\Cref{sec:adaptive-norm}) includes several safeguards:

\begin{enumerate}
    \item \textbf{Sum threshold:} If $|S^+| < \epsilon_{\mathrm{num}}$ or $|S^-| < \epsilon_{\mathrm{num}}$ (default $\epsilon_{\mathrm{num}} = 10^{-8}$), normalization is skipped and the original advantages are returned.

    \item \textbf{Scale clamping:} $\alpha$ and $\beta$ are clamped to $[\epsilon_{\mathrm{num}}, s_{\max}]$ (default $s_{\max} = 10$) to prevent extreme scaling.

    \item \textbf{Intermediate overflow protection:} The quantity $(S^+/S^-)^2 Q^-$ is clamped to $10^8$ before computing $\alpha$.

    \item \textbf{Finiteness check:} If $\alpha$ or $\beta$ is NaN or $\pm\infty$ after computation, normalization is skipped.
\end{enumerate}

These safeguards may cause the normalized advantages to deviate slightly from the exact empirical constraints in
\Cref{def:adaptive-norm}, but in practice the deviation is negligible ($|\text{mean}| < 0.001$, $|\text{var} - 1| < 0.01$). These
thresholds are heuristic defaults; formal validation is deferred to the experimental evaluation.

\textbf{Fallback conditions.} Normalization is skipped entirely when:
\begin{itemize}
    \item All advantages have the same sign ($\mathcal{P} = \emptyset$ or $\mathcal{N} = \emptyset$), indicating the group has uniform quality and the leave-one-out-shaped advantages are already meaningful.
    \item The uniform scale mode (\Cref{app:uniform-scale}) is active and the group has uniform rewards.
\end{itemize}


\section{Uniform Scale Mode}
\label{app:uniform-scale}

The uniform scale mode is an optional technique to improve sample utilization. When all $n$ samples for a prompt have identical rewards (all correct or all incorrect), the leave-one-out baseline yields $\tilde{r}_i = 0$ for all $i$, eliminating the gradient signal. Uniform scale mode recovers this signal.

\begin{definition}[Uniform Scale]
For prompt groups where $\mathrm{std}(\{r_1, \ldots, r_n\}) < \epsilon_{\mathrm{tol}}$ (uniform-reward groups), the shaped reward is set to $\tilde{r}_i = r_i / n$, and the adaptive normalization (\Cref{sec:adaptive-norm}) is skipped.
\end{definition}

\begin{proposition}
Under uniform scale mode, the gradient for uniform-reward groups is proportional to $r_i \sum_t \nabla_\theta \log \pitheta(y_{i,t}|c_{i,t})$, which pushes the policy toward all sampled responses when $r_i > 0$ (all correct) and away from them when $r_i < 0$ (all incorrect), preserving gradient signal that the leave-one-out baseline would otherwise eliminate.
\end{proposition}

\begin{proof}
With $\tilde{r}_i = r_i/n$ and no normalization, the sample-$i$ contribution to the REINFORCE gradient is proportional to
\[
    \frac{r_i}{n} \sum_t \nabla_\theta \log \pitheta(y_{i,t}\mid c_{i,t}).
\]
Hence $\sign(r_i)$ determines the update direction: when $r_i > 0$ the policy increases the log-probability of the sampled tokens, and
when $r_i < 0$ it decreases them.
\end{proof}
