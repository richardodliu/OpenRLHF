\documentclass[11pt]{article}

\input{env}
\input{math}

\title{REINFORCE Pro Max: Variance-Reduced Advantage Estimation\\
with Causal Off-Policy Correction for LLM-RL}

\author{Anonymous}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Critic-free reinforcement learning methods for Large Language Models (LLMs), including REINFORCE++, GRPO, and RLOO, eliminate the need for
a value network and save significant GPU memory. However, they face two intertwined challenges: high-variance advantage estimation and
off-policy mismatch between inference engines and training frameworks. We present REINFORCE Pro Max, a unified framework addressing both
challenges with theoretical analysis. The first component, REINFORCE Max, combines a leave-one-out (RLOO) baseline---which provides
statistical independence between each sample and its baseline, enabling clean second-moment analysis---with an adaptive asymmetric
normalization that preserves gradient direction while enforcing zero empirical mean and unit empirical variance on non-zero tokens. The
second component, REINFORCE Pro, introduces prefix cumulative importance sampling correction that preserves the causal structure of
autoregressive generation. We show that, under explicit sufficient conditions on the per-token log-ratios stated in
Theorem~\ref{thm:prefix-tighter}, prefix cumulative IS can provide tighter masking (i.e., mask at least as many off-policy tokens) than
both token-level methods (which ignore causal dependencies) and sequence-level methods (which lose positional information) in three
analyzed deviation patterns (early deviation, late deviation, and monotone drift). We relate prefix cumulative IS to the per-position
structure of the Adaptive bound in trust region analyses: the KL chain rule yields an expectation identity for the cumulative log-ratio,
and thresholding the sample-level prefix log-ratio provides a practical, per-position proxy for trust-region style filtering. Together,
the two components target the two factors in the surrogate objective error decomposition: normalization stabilizes the advantage magnitude,
while prefix masking filters out tokens with large cumulative $\piold/\piroll$ mismatch.
\end{abstract}


%==============================================================================
% INTRODUCTION
%==============================================================================
\input{main/1-intro}


%==============================================================================
% PRELIMINARIES
%==============================================================================
\input{main/3-preliminaries}


%==============================================================================
% METHOD: REINFORCE MAX + REINFORCE PRO
%==============================================================================
\input{main/4-method}


%==============================================================================
% UNIFIED FRAMEWORK
%==============================================================================
\input{main/5-theory}


%==============================================================================
% RELATED WORK
%==============================================================================
\input{main/2-related}


%==============================================================================
% EXPERIMENTS
%==============================================================================
\input{main/6-experiment}


%==============================================================================
% CONCLUSION
%==============================================================================
\input{main/7-conclusion}


%==============================================================================
% REFERENCES
%==============================================================================
\bibliographystyle{plainnat}
\bibliography{main/reference}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{main/appendix}


\end{document}
