\documentclass[11pt]{article}

\input{env}
\input{math}

\title{REINFORCE Pro Max: Variance-Reduced Advantage Estimation\\
with Causal Off-Policy Correction for LLM-RL}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Critic-free reinforcement learning methods for Large Language Models---such as REINFORCE++, GRPO, and RLOO---eliminate the need for a value network, saving significant GPU memory. However, they face two intertwined challenges: high-variance advantage estimation and off-policy mismatch between inference engines and training frameworks. We present REINFORCE Pro Max, a unified framework addressing both challenges with theoretical analysis. The first component, REINFORCE Max, combines a leave-one-out (RLOO) baseline---which provides statistical independence between each sample and its baseline, enabling clean second-moment factorization---with an adaptive asymmetric normalization that preserves gradient direction while ensuring zero mean and unit variance. The second component, REINFORCE Pro, introduces prefix cumulative importance sampling correction that preserves the causal structure of autoregressive generation. We show that, under explicit sufficient conditions on the per-token log-ratios (Theorem~\ref{thm:prefix-tighter}), prefix cumulative IS can provide tighter masking than both token-level methods (which ignore causal dependencies) and sequence-level methods (which lose positional information) in three analyzed deviation patterns (early deviation, late deviation, and monotone drift). We establish a connection between prefix cumulative IS and the Adaptive bound from the trust region literature, showing that our method acts as a practical, sample-level approximation of the per-position error bound in that framework. Together, the two components address both factors in the surrogate objective error decomposition: the advantage magnitude is controlled via normalization, and the context distribution shift is controlled via sample-level filtering on retained trajectories.
\end{abstract}


%==============================================================================
% INTRODUCTION
%==============================================================================
\input{main/1-intro}


%==============================================================================
% PRELIMINARIES
%==============================================================================
\input{main/3-preliminaries}


%==============================================================================
% METHOD: REINFORCE MAX + REINFORCE PRO
%==============================================================================
\input{main/4-method}


%==============================================================================
% UNIFIED FRAMEWORK
%==============================================================================
\input{main/5-theory}


%==============================================================================
% RELATED WORK
%==============================================================================
\input{main/2-related}


%==============================================================================
% EXPERIMENTS
%==============================================================================
\input{main/6-experiment}


%==============================================================================
% CONCLUSION
%==============================================================================
\input{main/7-conclusion}


%==============================================================================
% REFERENCES
%==============================================================================
\bibliographystyle{plainnat}
\bibliography{main/reference}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{main/appendix}


\end{document}
