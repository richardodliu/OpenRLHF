\subsection{Results}
\label{subsec:eval-res}

Table~\ref{tab:main_results} provides a comprehensive comparison of \model{} against leading thinking models. The following sections provide a detailed analysis of its performance across different aspects:


\paragraph{Mathematical Reasoning} 
\model{} demonstrates leading mathematical reasoning capabilities, as evidenced by its performance on challenging benchmarks. By relying solely on natural language reasoning, it achieves 93.40\% on AIME 25 and 86.72\% on HMMT 25, securing the second-highest rank overall and leading all open-weights models. Furthermore, the model delivers competitive results across specialized mathematical domains, scoring 82.63\% on Omni-MATH and 88.54\% on CNMO 2024. These results highlight a particular proficiency in complex, Olympiad-style problem-solving. This demonstrates that a stable and efficient RL training recipe, together with a diverse and high-quality math training dataset, drives superior mathematical reasoning across competition-level benchmarks. 

Moreover, we evaluate the mathematical reasoning capabilities of \model{} on the IMO 2025. Specifically, \model{} is integrated into the multi-agent framework AWorld~\citep{yu2025aworld} and tasked with solving the problems through pure natural language reasoning, without relying on code generation or external symbolic solvers. The model successfully solved Problems 1, 3, 4, and 5 on its first attempt, a performance corresponding to the IMO silver medal level. On its third attempt, it generated a nearly complete geometric proof for Problem 2. For the most challenging Problem 6, which no AI participant solved correctly during IMO 2025, \model{} converged to the same incorrect answer (4048) as Gemini 2.5 Pro, whereas the correct answer is 2112. A detailed case study is available in Appendix~\ref{sec:imo}. We believe the outstanding natural language reasoning ability will generalize to a broader range of tasks, paving the way for enhanced overall performance.


\paragraph{Coding Capabilities} 
As the results show, \model{} demonstrates exceptional performance in programming tasks that demand iterative refinement and deep logical reasoning, establishing a leading position among both open-weights and closed-weights models. On LiveCodeBench-v6 (2408-2505), it achieves a top score of 78.30\%, outperforming DeepSeek-V3.1 by 2.97 points and Qwen3-235B-A22B-Thinking-2507 by 2.58 percentage points. Furthermore, on CodeForces (rating), \model{} attains a score of 2088, which is the highest score among all models and exceeds the performance of both open-source competitors and closed-source APIs. It indicates that our carefully synthesized dataset shapes \model{}'s robust performance on programming applications, which forms a strong foundation for future endeavors on agentic applications. 

\paragraph{Logical Reasoning} 
\model{} demonstrates promising capabilities in other logical reasoning tasks. Powered by sourcing carefully selected logical games from multiple domains, \model achieves a score of 55.94\% on the challenging ARC-AGI-1 benchmark, ranking second overall. This performance places it only behind GPT-5-Thinking (65.70\%) and represents a substantial improvement of +15.32 percentage points over DeepSeek-V3.1 (40.62\%) and +7.82 points over Qwen3-235B-A22B-Thinking-2507 (48.12\%).

\paragraph{Human Alignment} In addition to the reasoning RL training, we also leverage a general RL training stage to equip the reasoning model with strong performance on general tasks. From Table~\ref{tab:main_results}, \model{} achieves strong alignment with human preferences in complex scenarios. On the ArenaHard v2 benchmark, it attains an 81.59\% win-rate, ranking second overall and trailing GPT-5-Thinking by only 1.32 percentage points. It also leads all models with an Elo rating of 84.52. In Creative Writing v3, \model{} scores 85.40\%, performing within 0.1 percentage points of the leading open-source model. These results confirm \model{}'s effectiveness in balancing human preference alignment with broad capabilitiesâ€”a critical advantage for real-world deployment.

\paragraph{Healthcare Capabilities} 
On HealthBench, \model{} attains a score of 57.93\%, ranking second overall and leading the field of open-source models. This performance indicates proficient clinical knowledge integration and suggests the model's viability for complex healthcare tasks.
