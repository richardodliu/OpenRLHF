\subsection{Evaluation Settings}
All thinking models are evaluated under a standardized pipeline for a fair comparison. Benchmarks including AIME 2025, LiveCodeBench-v6, ARC-AGI-1, CodeForces, HMMT 2025, ZebraLogic, HLE, and BFCL v3 are assessed with a 128K context window, extended via YaRN \citep{peng2023yarn} for models with insufficient native context. Benchmarks including Aider, CNMO 2024, and BBEH use a 64K context window. Other benchmarks use a 32K context window. For baseline models, we use their official hyperparameters for open-weights models, while using vendor-recommended settings for proprietary APIs. For baselines with officially reported results, we report the higher score between our reproduction and the official release. 

\label{subsec:eval-set}

