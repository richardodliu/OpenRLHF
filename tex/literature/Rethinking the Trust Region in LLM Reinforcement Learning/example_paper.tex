%%%%%%%% ICML 2026 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2026} with \usepackage[nohyperref]{icml2026} above.
\usepackage{hyperref}

% \usepackage{amsmath, amssymb, graphicx, algorithm, algpseudocode}  

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2026}

% For preprint, use
\usepackage[preprint]{icml2026}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2026}
\usepackage{multirow}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage[dvipsnames]{xcolor}
\usepackage[absolute,overlay]{textpos}
\usepackage{fontawesome}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
% \usepackage{listings}
% \lstset{
% basicstyle=\small\ttfamily,
% columns=flexible,
% breaklines=true
% }
% \lstset{escapeinside={<@}{@>}}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% AI box new 
\usepackage[most]{tcolorbox}
\tcbset{
  aibox/.style={
    enhanced,
    breakable,
    width=\linewidth,
    top=7pt,
    bottom=2pt,
    colback=blue!6!white,
    colframe=black,
    colbacktitle=black,
    coltitle=white,
    attach boxed title to top left={yshift=-0.1in,xshift=0.15in},
    boxed title style={boxrule=0pt, colframe=white},
  },
  aiboxgrey/.style={
    enhanced,
    breakable,
    width=\linewidth,
    top=7pt,
    bottom=2pt,
    colback=gray!5!white, % Changed from RGB to grayscale specification
    colframe=black,
    colbacktitle=black,
    coltitle=white,
    attach boxed title to top left={yshift=-0.1in,xshift=0.15in},
    boxed title style={boxrule=0pt, colframe=white},
  }
}

\newtcolorbox{AIbox}[2][]{aibox, title={#2}, #1}
\newtcolorbox{AIboxgrey}[2][]{aiboxgrey, title={#2}, #1}

%%%%%%%%%% for listings %%%%%%%%%%%%%%
\usepackage{listings}
\lstset{
    basicstyle = \ttfamily\small, % 等宽字体，小号字
    % basicstyle = \ttfamily, % 等宽字体
    breaklines = true,            % 启用自动换行
    breakatwhitespace = true,    % 允许在任意位置换行（而不仅限空格处）
    breakindent=0pt,
    xleftmargin = 0pt,    % Key: removes left indentation of the listing block
    % frame = single,               % 添加边框
    % backgroundcolor = \color{gray!10}, % 背景色
    % numbers = left,               % 显示行号
    % numberstyle = \tiny\color{gray}, % 行号样式
    % postbreak=\raisebox{0pt}[0pt][0pt]{\ensuremath{\color{blue}\hookrightarrow\space}}, % 在换行处添加一个红色右箭头并保留一个空格
    % escapeinside = {@(}{)}        % 允许在代码中嵌入 LaTeX 命令（如数学公式）
    escapeinside={<@}{@>}
}
%%%%%%%%%% for listings %%%%%%%%%%%%%%




\DeclareMathOperator*{\argmax}{arg\,max}  
\DeclareMathOperator*{\rolloutpi}{{\textcolor{black}{\mu}}}
\DeclareMathOperator*{\trainerpi}{{\textcolor{black}{\pi}}}
\DeclareMathOperator*{\redmu}{{\textcolor{red}{\mu}}}
\DeclareMathOperator*{\bluepi}{{\textcolor{red}{\pi}}}


%%%%%% add by xiangxin for highlighting comment 
% \newcommand{\xiangxin}[1]{{{\textcolor{red}{[Xiangxin: #1]}}}}





%%%%%%% added by xiangxin for compressing spaces
% Spacing adjustments
% \setlength{\textfloatsep}{5pt}
% \setlength\floatsep{3pt}
% \setlength\intextsep{1pt}
% \setlength{\abovecaptionskip}{0.1em}
% \setlength{\belowcaptionskip}{0.1em}
% \setlength{\parskip}{0.1em}
% \usepackage[compact]{titlesec}
% \usepackage{sidecap}
% \titlespacing*{\section}{0pt}{*0.1}{*0.1}
% \titlespacing*{\subsection}{0pt}{*0.1}{*0.1}
% \titlespacing*{\subsubsection}{0pt}{*0.1}{*0.1}
% \allowdisplaybreaks







% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Rethinking the Trust Region in LLM Reinforcement Learning}

\begin{document}

\twocolumn[
  \icmltitle{Rethinking the Trust Region in LLM Reinforcement Learning}

  % It is OKAY to include author information, even for blind submissions: the
  % style file will automatically remove it for you unless you've provided
  % the [accepted] option to the icml2026 package.

  % List of affiliations: The first argument should be a (short) identifier you
  % will use later to specify author affiliations Academic affiliations
  % should list Department, University, City, Region, Country Industry
  % affiliations should list Company, City, Region, Country

  % You can specify symbols, otherwise they are numbered in order. Ideally, you
  % should not use this facility. Affiliations will be numbered in order of
  % appearance and this is the preferred way.
  \icmlsetsymbol{equal}{*}
  \icmlsetsymbol{prior}{$\dagger$}

  \begin{icmlauthorlist}
    \icmlauthor{Penghui Qi}{equal,sea,nus}
    \icmlauthor{Xiangxin Zhou}{equal,sea}
    \icmlauthor{Zichen Liu}{nus}
    \icmlauthor{Tianyu Pang}{sea}
    \icmlauthor{Chao Du}{sea}
    \icmlauthor{Min Lin}{sea}
    \icmlauthor{Wee Sun Lee}{nus}
    %\icmlauthor{}{sch}
    % \icmlauthor{Firstname8 Lastname8}{sch}
    % \icmlauthor{Firstname8 Lastname8}{yyy,comp}
    %\icmlauthor{}{sch}
    %\icmlauthor{}{sch}
  \end{icmlauthorlist}

  \icmlaffiliation{sea}{Sea AI Lab, Singapore}
  \icmlaffiliation{nus}{School of Computing, National University of Singapore}
  % \icmlaffiliation{prior}{Work done in Sea AI Lab}
  
  % \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

  \icmlcorrespondingauthor{Wee Sun Lee}{leews@comp.nus.edu.sg}
  \icmlcorrespondingauthor{Penghui Qi}{penghuiq@comp.nus.edu.sg}

  % You may provide any keywords that you find helpful for describing your
  % paper; these are used to populate the "keywords" metadata in the PDF but
  % will not be shown in the document
  \icmlkeywords{LLMs, Reinforcement Learning, Trust Region, Training Stability, Training Efficiency}

  \vskip 0.3in

  \noindent\begin{minipage}{\textwidth}
  \vspace{8.4cm}
  \end{minipage}
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column listing the
% affiliations and the copyright notice. The command takes one argument, which
% is text to display at the start of the footnote. The \icmlEqualContribution
% command is standard text for equal contribution. Remove it (just {}) if you
% do not need this facility.

% Use ONE of the following lines. DO NOT remove the command.
% If you have no special notice, KEEP empty braces:
% \printAffiliationsAndNotice{}  % no special notice (required even if empty)
% Or, if applicable, use the standard equal contribution text:
\printAffiliationsAndNotice{\icmlEqualContribution}

\begin{textblock*}{17cm}(2.1cm,6cm)
    \centering
    \vspace{-0.6cm}
    \begin{center}
        \faGithub~\url{https://github.com/sail-sg/Stable-RL}
    \end{center}
    \vspace{-0.1cm}
    \includegraphics[width=0.85\linewidth]{figs/ppo_vs_dppo.pdf}
    \captionof{figure}{Comparison of \textcolor{red}{PPO} and the proposed \textcolor{blue}{DPPO} (the Binary-TV variant in \Cref{sec:method_binary}). (\textbf{Left}) The surrogate objective and corresponding masks for PPO and DPPO. PPO (and variants like GRPO) employs a heuristic mask based on the probability ratio, which \textbf{over-penalizes low-probability tokens} and \textbf{under-penalizes high-probability ones} (\Cref{sec:method_limitations}). In contrast, DPPO utilizes a more principled mask based on a direct approximation of policy divergence (e.g., Total Variation), ensuring updates stay within a theoretically grounded trust region (\Cref{sec:llm_tr}). (\textbf{Right}) Experimental results on the AIME24 using Qwen3-30B-A3B-Base. DPPO significantly outperforms GRPO baselines, achieving superior training efficiency and stability even without rollout routing replay (R3) (\Cref{sec:scaling_exp}).}
    \label{fig:ppo_vs_dppo}
\end{textblock*}


\begin{abstract}  
Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose \textbf{Divergence Proximal Policy Optimization (DPPO)}, which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training \textbf{stability} and \textbf{efficiency} compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning. 
\end{abstract}

\input{paper/intro}

\input{paper/background}

\input{paper/llm_bound}

\input{paper/method}

\input{paper/stability}

\input{paper/efficiency}

\input{paper/scaling_and_benchmark}


% \vspace{-0.2cm}
\section{Conclusion}


In this work, we have presented a comprehensive rethinking of the trust region framework within the context of LLM fine-tuning. We derived policy improvement bounds specifically tailored to the finite-horizon, undiscounted setting of LLM generation, establishing a rigorous theoretical foundation for future trust-region research. Furthermore, through extensive empirical analysis, we investigated the trade-offs between training stability and efficiency, providing practical guidelines to optimize both.

Central to our contribution is the introduction of Divergence Proximal Policy Optimization (DPPO). We identified and addressed a critical structural flaw in the standard PPO algorithm: it over-constrains updates to low-probability tokens while under-constraining potentially catastrophic shifts in high-probability tokens. 
This implicit bias results in a sub-optimal training dynamic, particularly for the expansive, long-tailed vocabularies inherent to LLMs. 
By substituting heuristic ratio clipping with a more principled policy divergence, DPPO significantly enhances both efficiency and stability. To avoid huge memory footprint for computing an exact policy divergence, we introduced Binary and Top-K approximations, which capture essential divergence with negligible overhead. Our evaluations demonstrate that DPPO consistently outperforms existing methods like GRPO in both training efficiency and stability, offering a more robust foundation for the RL-based LLM fine-tuning.



\clearpage


\section*{Impact Statement}

This paper presents work whose goal is to advance the field of Machine
Learning. There are many potential societal consequences of our work, none
which we feel must be specifically highlighted here.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2026}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\input{paper/app}

\end{document}

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
