% \vspace{-0.2cm}
\section{Scaling Experiments}
\label{sec:scaling_exp}


\textbf{Experimental Setting:} We conduct large-scale experiments to further validate our methods. We train on a filtered subset of DAPO-Math~\citep{yu2025dapo}, containing approximately 13k samples. Five model configurations (different base models and training techniques) are evaluated: (1) \textbf{MoE Base}: Qwen3-30B-A3B-Base~\citep{yang2025qwen3}; (2) \textbf{MoE Base w/ R3}: Qwen3-30B-A3B-Base with rollout router replay (R3)~\citep{ma2025stabilizing}; (3) \textbf{MoE Thinking}: Qwen3-30B-A3B; (4) \textbf{Dense Base}: Qwen3-8B-Base; (5) \textbf{MoE Base w/ LoRA}: Qwen3-30B-A3B-Base with LoRA~\citep{hu2022lora}. Baseline methods include \textbf{GRPO-ClipHigher}\citep{shao2024deepseekmath, liu2025understanding, yu2025dapo} and \textbf{CISPO}\citep{chen2025minimax, khatri2025art}. 
All methods use the behavior policy ($\rolloutpi_{\theta'}$) instead of recomputed policy distribution ($\trainerpi_{\theta'}$) to construct the trust region (i.e., for clipping or masking). We compare our proposed methods, \textbf{DPPO-Binary-KL} and \textbf{DDPO-Binary-TV}, against these baselines. More details are provided in Appendix~\ref{appendix:detailed_experimental_settings}.

\textbf{Main Results.}
We present online evaluation results on AIME24 and AIME25 \citep{AIME} during RL training in the following figures: \Cref{fig:main_base} (MoE Base with and without R3) and \Cref{fig:main_a3b_and_8b} (MoE Thinking and Dense Base). Results for MoE Base with LoRA are provided in Appendix~\ref{appendix:extended_main_results}.

Our proposed method consistently demonstrates superior \textbf{stability} and \textbf{efficiency} across all five large-scale experiments. Specifically, DPPO optimizes rewards at a significantly faster speed than the GRPO-ClipHigher baseline and achieves better converged performance, providing empirical validation for the motivations discussed in \Cref{sec:method_limitations}. While all baseline methods frequently exhibit training instability or catastrophic collapse (e.g., CISPO in MoE~Base without R3 and GRPO-ClipHigher in MoE~Thinking), our approach maintains a remarkably stable training process.

% Rollout router replay (R3)~\citep{ma2025stabilizing,zheng2025stabilizing, liu2025deepseek} is deemed to be a necessary technique for stabilizing RL training in MoE models. However, as shown in \Cref{fig:main_base}, both of our DPPO variants even \textit{without} R3 consistently outperform baselines \textit{with} R3, highlighting the effectiveness of DPPO in both training efficiency and stability. Additional detailed results and discussions are available in Appendix~\ref{appendix:extended_main_results}.

Rollout router replay (R3) is widely considered a necessary technique for stabilizing RL training in MoE models~\citep{ma2025stabilizing, zheng2025stabilizing, liu2025deepseek}. However, as illustrated in \Cref{fig:main_base}, our DPPO variants (\textit{without} R3) even consistently \textbf{outperform the R3-enhanced baselines}, which underscores the superior training efficiency and inherent stability of the DPPO framework. We provide additional detailed results and extended discussions in Appendix~\ref{appendix:extended_main_results}.


\textbf{Ablation on TV/KL Approximation.}
In the above scaling experiments, DPPO is implemented using the binary TV/KL approximation (Equations~\ref{eq:binary_tv} and \ref{eq:binary_kl}). To assess the impact of this simplification, we compare it against DPPO with the top-K (K=20) TV/KL (Equations~\ref{eq:topk_tv} and \ref{eq:topk_kl}) under the same setting as MoE Base. The results, presented in \Cref{fig:main_topk}, show that both approximations perform similarly and significantly outperform the baselines. This finding indicates that the easy-to-implement binary approximation is a sufficient and computationally efficient choice for scalable RL. We provide more detailed results in Appendix~\ref{appendix:ablation_topk_approximation}.


% \begin{figure}[t]
%     \centering  
%     \includegraphics[width=1.0\linewidth]{figs/main-lora.pdf}  
%     \caption{Evolution of AIME24 and AIME25 scores during RL training with LoRA using Qwen3-30B-A3B-Base.}  
%     \label{fig:main_lora}  
%     % \vspace{-1.4em}
% \end{figure}



\textbf{Generalization to Other Model Families and Tasks.}
We also conduct experiments on models from the Llama family \citep{touvron2023llama,wang2025octothinker} and on tasks beyond math reasoning \citep{liu2025gem}. The results, which are presented in Appendix ~\ref{appendix:extended_models_tasks}, show DPPO outperforms the baseline across most settings, highlighting its broad applicability.



\begin{figure}[t]
    \centering  
    \includegraphics[width=1.0\linewidth]{figs/main-topk.pdf}  
    \caption{Evolution of AIME24 and AIME25 scores for baselines and DPPO with binary/Top-K (K=20) TV/KL approximation under the same setting as MoE Base w/o R3.}  
    \label{fig:main_topk}  
    \vspace{-1.4em}
\end{figure}
