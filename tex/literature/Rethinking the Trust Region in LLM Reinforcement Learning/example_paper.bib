@article{guo2025deepseekr1,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{schulman2017equivalence,
  title={Equivalence between policy gradients and soft q-learning},
  author={Schulman, John and Chen, Xi and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1704.06440},
  year={2017}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@article{jaques2019way,
  title={Way off-policy batch deep reinforcement learning of implicit human preferences in dialog},
  author={Jaques, Natasha and Ghandeharioun, Asma and Shen, Judy Hanwen and Ferguson, Craig and Lapedriza, Agata and Jones, Noah and Gu, Shixiang and Picard, Rosalind},
  journal={arXiv preprint arXiv:1907.00456},
  year={2019}
}

@book{sutton2018rlbook,
  author = {Sutton, Richard S. and Barto, Andrew G.},
  edition = {Second},
  publisher = {The MIT Press},
  title = {Reinforcement Learning: An Introduction},
  year = {2018 }
}

@misc{OpenReasonerZero2025,
  title={Open-Reasoner-Zero: An Open Source Approach to Scaling Reinforcement Learning on the Base Model},
  author={Jingcheng Hu and Yinmin Zhang and Qi Han and Daxin Jiang and Xiangyu Zhang, Heung-Yeung Shum},
  year={2025},
  howpublished={\url{https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero}},
}

@article{lambert2024tulu3,
  title={T$\backslash$" ulu 3: Pushing frontiers in open language model post-training},
  author={Lambert, Nathan and Morrison, Jacob and Pyatkin, Valentina and Huang, Shengyi and Ivison, Hamish and Brahman, Faeze and Miranda, Lester James V and Liu, Alisa and Dziri, Nouha and Lyu, Shane and others},
  journal={arXiv preprint arXiv:2411.15124},
  year={2024}
}

@misc{tinyzero,
author       = {Jiayi Pan and Junjie Zhang and Xingyao Wang and Lifan Yuan and Hao Peng and Alane Suhr},
title        = {TinyZero},
howpublished = {https://github.com/Jiayi-Pan/TinyZero},
note         = {Accessed: 2025-01-24},
year         = {2025}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{schulman2015high,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1506.02438},
  year={2015}
}

@article{kazemnejad2024vineppo,
  title={Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment},
  author={Kazemnejad, Amirhossein and Aghajohari, Milad and Portelance, Eva and Sordoni, Alessandro and Reddy, Siva and Courville, Aaron and Roux, Nicolas Le},
  journal={arXiv preprint arXiv:2410.01679},
  year={2024}
}

@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}

@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}



@article{li2024numinamath,
  title={Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions},
  author={Li, Jia and Beeching, Edward and Tunstall, Lewis and Lipkin, Ben and Soletskyi, Roman and Huang, Shengyi and Rasul, Kashif and Yu, Longhui and Jiang, Albert Q and Shen, Ziju and others},
  journal={Hugging Face repository},
  volume={13},
  pages={9},
  year={2024}
}

@article{lewkowycz2022solving,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3843--3857},
  year={2022}
}

@article{he2024olympiadbench,
  title={Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems},
  author={He, Chaoqun and Luo, Renjie and Bai, Yuzhuo and Hu, Shengding and Thai, Zhen Leng and Shen, Junhao and Hu, Jinyi and Han, Xu and Huang, Yujie and Zhang, Yuxiang and others},
  journal={arXiv preprint arXiv:2402.14008},
  year={2024}
}

@article{sheng2024hybridflow,
  title={Hybridflow: A flexible and efficient rlhf framework},
  author={Sheng, Guangming and Zhang, Chi and Ye, Zilingfeng and Wu, Xibin and Zhang, Wang and Zhang, Ru and Peng, Yanghua and Lin, Haibin and Wu, Chuan},
  journal={arXiv preprint arXiv:2409.19256},
  year={2024}
}

@article{hu2024openrlhf,
  title={OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework},
  author={Jian Hu and Xibin Wu and Zilin Zhu and Xianyu and Weixun Wang and Dehao Zhang and Yu Cao},
  journal={arXiv preprint arXiv:2405.11143},
  year={2024}
}

@misc{vonwerra2022trl,
  author = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallouédec},
  title = {TRL: Transformer Reinforcement Learning},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/trl}}
}

@article{yang2024qwen25math,
  title={Qwen2.5-math technical report: Toward mathematical expert model via self-improvement},
  author={Yang, An and Zhang, Beichen and Hui, Binyuan and Gao, Bofei and Yu, Bowen and Li, Chengpeng and Liu, Dayiheng and Tu, Jianhong and Zhou, Jingren and Lin, Junyang and others},
  journal={arXiv preprint arXiv:2409.12122},
  year={2024}
}

@article{yang2024qwen25,
  title={Qwen2.5 technical report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{grattafiori2024llama,
  title={The llama 3 herd of models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@misc{liu2025oatzero,
  title={There May Not be Aha Moment in R1-Zero-like Training — A Pilot Study},
  author={Zichen Liu and Changyu Chen and Wenjun Li and Tianyu Pang and Chao Du and Min Lin},
  year={2025},
  howpublished={\url{https://oatllm.notion.site/oat-zero}},
  note={Notion Blog},
}

@article{yeo2025demystifying,
  title={Demystifying Long Chain-of-Thought Reasoning in LLMs},
  author={Yeo, Edward and Tong, Yuxuan and Niu, Morry and Neubig, Graham and Yue, Xiang},
  journal={arXiv preprint arXiv:2502.03373},
  year={2025}
}

@inproceedings{andrychowicz2021matters,
  title={What matters for on-policy deep actor-critic methods? a large-scale study},
  author={Andrychowicz, Marcin and Raichuk, Anton and Sta{\'n}czyk, Piotr and Orsini, Manu and Girgin, Sertan and Marinier, Rapha{\"e}l and Hussenot, Leonard and Geist, Matthieu and Pietquin, Olivier and Michalski, Marcin and others},
  booktitle={International conference on learning representations},
  year={2021}
}

@misc{liu2025oat,
author       = {Zichen Liu and Changyu Chen and Chao Du and Wee Sun Lee and Min Lin},
title        = {OAT: A research-friendly framework for LLM online alignment},
howpublished = {\url{https://github.com/sail-sg/oat}},
year         = {2025}
}

@article{chen2024not,
  title={Do not think that much for 2+ 3=? on the overthinking of o1-like llms},
  author={Chen, Xingyu and Xu, Jiahao and Liang, Tian and He, Zhiwei and Pang, Jianhui and Yu, Dian and Song, Linfeng and Liu, Qiuzhi and Zhou, Mengfei and Zhang, Zhuosheng and others},
  journal={arXiv preprint arXiv:2412.21187},
  year={2024}
}

@misc{deepscaler2025,
  title={DeepScaleR: Surpassing O1-Preview with a 1.5B Model by Scaling RL},
  author={Michael Luo and Sijun Tan and Justin Wong and Xiaoxiang Shi and William Y. Tang and Manan Roongta and Colin Cai and Jeffrey Luo and Tianjun Zhang and Li Erran Li and Raluca Ada Popa and Ion Stoica},
  year={2025},
  howpublished={\url{https://github.com/agentica-project/deepscaler}},
}

@article{cui2025process,
  title={Process reinforcement through implicit rewards},
  author={Cui, Ganqu and Yuan, Lifan and Wang, Zefan and Wang, Hanbin and Li, Wendi and He, Bingxiang and Fan, Yuchen and Yu, Tianyu and Xu, Qixin and Chen, Weize and others},
  journal={arXiv preprint arXiv:2502.01456},
  year={2025}
}

@article{allal2025smollm2,
  title={SmolLM2: When Smol Goes Big--Data-Centric Training of a Small Language Model},
  author={Allal, Loubna Ben and Lozhkov, Anton and Bakouch, Elie and Bl{\'a}zquez, Gabriel Mart{\'\i}n and Penedo, Guilherme and Tunstall, Lewis and Marafioti, Andr{\'e}s and Kydl{\'\i}{\v{c}}ek, Hynek and Lajar{\'\i}n, Agust{\'\i}n Piqueres and Srivastav, Vaibhav and others},
  journal={arXiv preprint arXiv:2502.02737},
  year={2025}
}

@misc{numina_math_datasets,
  author = {Jia LI and Edward Beeching and Lewis Tunstall and Ben Lipkin and Roman Soletskyi and Shengyi Costa Huang and Kashif Rasul and Longhui Yu and Albert Jiang and Ziju Shen and Zihan Qin and Bin Dong and Li Zhou and Yann Fleureau and Guillaume Lample and Stanislas Polu},
  title = {NuminaMath},
  year = {2024},
  publisher = {Numina},
  journal = {Hugging Face repository},
  howpublished = {\url{[https://huggingface.co/AI-MO/NuminaMath-1.5](https://github.com/project-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf)}}
}

@article{ahmadian2024back,
  title={Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms},
  author={Ahmadian, Arash and Cremer, Chris and Gall{\'e}, Matthias and Fadaee, Marzieh and Kreutzer, Julia and Pietquin, Olivier and {\"U}st{\"u}n, Ahmet and Hooker, Sara},
  journal={arXiv preprint arXiv:2402.14740},
  year={2024}
}

@misc{kool2019buy,
  title={Buy 4 reinforce samples, get a baseline for free!},
  author={Kool, Wouter and van Hoof, Herke and Welling, Max},
  year={2019}
}

@inproceedings{li2024flexattention,
  title={Flexattention for efficient high-resolution vision-language models},
  author={Li, Junyan and Chen, Delin and Cai, Tianle and Chen, Peihao and Hong, Yining and Chen, Zhenfang and Shen, Yikang and Gan, Chuang},
  booktitle={European Conference on Computer Vision},
  pages={286--302},
  year={2024},
  organization={Springer}
}

@article{zeng2025simplerl,
  title={Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild},
  author={Zeng, Weihao and Huang, Yuzhen and Liu, Qian and Liu, Wei and He, Keqing and Ma, Zejun and He, Junxian},
  journal={arXiv preprint arXiv:2503.18892},
  year={2025}
}

@article{liu2025understanding,
  title={Understanding r1-zero-like training: A critical perspective},
  author={Liu, Zichen and Chen, Changyu and Li, Wenjun and Qi, Penghui and Pang, Tianyu and Du, Chao and Lee, Wee Sun and Lin, Min},
  journal={arXiv preprint arXiv:2503.20783},
  year={2025}
}

@misc{o1,
  title={Learning to reason with LLMs},
  author={OpenAI},
  year={2024},
  url = {https://openai.com/index/learning-to-reason-with-llms/}
}

@misc{gemini,
  title={Gemini 2.5: Our most intelligent AI model},
  author={Google},
  year={2025},
  url = {https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@inproceedings{vllm,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@article{dao2023flashattention,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{lambert2024t,
  title={T$\backslash$" ulu 3: Pushing frontiers in open language model post-training},
  author={Lambert, Nathan and Morrison, Jacob and Pyatkin, Valentina and Huang, Shengyi and Ivison, Hamish and Brahman, Faeze and Miranda, Lester James V and Liu, Alisa and Dziri, Nouha and Lyu, Shane and others},
  journal={arXiv preprint arXiv:2411.15124},
  year={2024}
}

@article{pignatelli2023survey,
  title={A survey of temporal credit assignment in deep reinforcement learning},
  author={Pignatelli, Eduardo and Ferret, Johan and Geist, Matthieu and Mesnard, Thomas and van Hasselt, Hado and Pietquin, Olivier and Toni, Laura},
  journal={ransactions on Machine Learning Research},
  year={2024}
}

@article{qu2025optimizing,
  title={Optimizing test-time compute via meta reinforcement fine-tuning},
  author={Qu, Yuxiao and Yang, Matthew YR and Setlur, Amrith and Tunstall, Lewis and Beeching, Edward Emanuel and Salakhutdinov, Ruslan and Kumar, Aviral},
  journal={arXiv preprint arXiv:2503.07572},
  year={2025}
}

@incollection{zilberstein1995approximate,
  title={Approximate reasoning using anytime algorithms},
  author={Zilberstein, Shlomo and Russell, Stuart},
  booktitle={Imprecise and approximate computation},
  pages={43--62},
  year={1995},
  publisher={Springer}
}

@inproceedings{dean1988analysis,
  title={An Analysis of Time-Dependent Planning.},
  author={Dean, Thomas L and Boddy, Mark S},
  booktitle={AAAI},
  volume={88},
  pages={49--54},
  year={1988}
}


@article{ma2025reasoning,
  title={Reasoning Models Can Be Effective Without Thinking},
  author={Ma, Wenjie and He, Jingxuan and Snell, Charlie and Griggs, Tyler and Min, Sewon and Zaharia, Matei},
  journal={arXiv preprint arXiv:2504.09858},
  year={2025}
}

@article{muennighoff2025s1,
  title={s1: Simple test-time scaling},
  author={Muennighoff, Niklas and Yang, Zitong and Shi, Weijia and Li, Xiang Lisa and Fei-Fei, Li and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Liang, Percy and Cand{\`e}s, Emmanuel and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2501.19393},
  year={2025}
}

@article{metacot,
  title={Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Though},
  author={Xiang, Violet and Snell, Charlie and Gandhi, Kanishk and Albalak, Alon and Singh, Anikait and Blagden, Chase and Phung, Duy and Rafailov, Rafael and Lile, Nathan and Mahan, Dakota and others},
  journal={arXiv preprint arXiv:2501.04682},
  year={2025}
}

@article{munkhbat2025self0training,
  title   = {Self-Training Elicits Concise Reasoning in Large Language Models},
  author  = {Tergel Munkhbat and Namgyu Ho and Seo Hyun Kim and Yongjin Yang and Yujin Kim and Se-Young Yun},
  year    = {2025},
  journal = {arXiv preprint arXiv: 2502.20122}
}

@article{lee2025well,
  title   = {How Well do LLMs Compress Their Own Chain-of-Thought? A Token Complexity Approach},
  author  = {Ayeong Lee and Ethan Che and Tianyi Peng},
  year    = {2025},
  journal = {arXiv preprint arXiv: 2503.01141}
}

@inproceedings{jin2024impact,
  title={The Impact of Reasoning Step Length on Large Language Models},
  author={Jin, Mingyu and Yu, Qinkai and Shu, Dong and Zhao, Haiyan and Hua, Wenyue and Meng, Yanda and Zhang, Yongfeng and Du, Mengnan},
  booktitle={ACL (Findings)},
  year={2024}
}

@article{nayab2024concise,
  title   = {Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost},
  author  = {Sania Nayab and Giulio Rossolini and Marco Simoni and Andrea Saracino and Giorgio Buttazzo and Nicolamaria Manes and Fabrizio Giacomelli},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2407.19825}
}


@article{yang2025dynamic,
  title   = {Dynamic Early Exit in Reasoning Models},
  author  = {Chenxu Yang and Qingyi Si and Yongjie Duan and Zheliang Zhu and Chenyu Zhu and Zheng Lin and Li Cao and Weiping Wang},
  year    = {2025},
  journal = {arXiv preprint arXiv: 2504.15895}
}



@article{arora2025training,
  title   = {Training Language Models to Reason Efficiently},
  author  = {Daman Arora and Andrea Zanette},
  year    = {2025},
  journal = {arXiv preprint arXiv: 2502.04463}
}

@article{aggarwal2025l10,
  title   = {L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning},
  author  = {Pranjal Aggarwal and Sean Welleck},
  year    = {2025},
  journal = {arXiv preprint arXiv: 2503.04697}
}

@article{beck2023survey,
  title={A survey of meta-reinforcement learning},
  author={Beck, Jacob and Vuorio, Risto and Liu, Evan Zheran and Xiong, Zheng and Zintgraf, Luisa and Finn, Chelsea and Whiteson, Shimon},
  journal={arXiv preprint arXiv:2301.08028},
  year={2023}
}

@article{han2024token,
  title={Token-budget-aware llm reasoning},
  author={Han, Tingxu and Wang, Zhenting and Fang, Chunrong and Zhao, Shiyu and Ma, Shiqing and Chen, Zhenyu},
  journal={arXiv preprint arXiv:2412.18547},
  year={2024}
}

@article{li2023remax,
  title={Remax: A simple, effective, and efficient reinforcement learning method for aligning large language models},
  author={Li, Ziniu and Xu, Tian and Zhang, Yushun and Lin, Zhihang and Yu, Yang and Sun, Ruoyu and Luo, Zhi-Quan},
  journal={arXiv preprint arXiv:2310.10505},
  year={2023}
}

@article{comanici2025gemini,
  title={Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities},
  author={Comanici, Gheorghe and Bieber, Eric and Schaekermann, Mike and Pasupat, Ice and Sachdeva, Noveen and Dhillon, Inderjit and Blistein, Marcel and Ram, Ori and Zhang, Dan and Rosen, Evan and others},
  journal={arXiv preprint arXiv:2507.06261},
  year={2025}
}

@article{yu2025dapo,
  title={Dapo: An open-source llm reinforcement learning system at scale},
  author={Yu, Qiying and Zhang, Zheng and Zhu, Ruofei and Yuan, Yufeng and Zuo, Xiaochen and Yue, Yu and Dai, Weinan and Fan, Tiantian and Liu, Gaohong and Liu, Lingjun and others},
  journal={arXiv preprint arXiv:2503.14476},
  year={2025}
}

@article{cui2025entropy,
  title={The entropy mechanism of reinforcement learning for reasoning language models},
  author={Cui, Ganqu and Zhang, Yuchen and Chen, Jiacheng and Yuan, Lifan and Wang, Zhi and Zuo, Yuxin and Li, Haozhan and Fan, Yuchen and Chen, Huayu and Chen, Weize and others},
  journal={arXiv preprint arXiv:2505.22617},
  year={2025}
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}

@inproceedings{wang2020truly,
  title={Truly proximal policy optimization},
  author={Wang, Yuhui and He, Hao and Tan, Xiaoyang},
  booktitle={Uncertainty in artificial intelligence},
  pages={113--122},
  year={2020},
  organization={PMLR}
}

@article{xie2024simple,
  title={Simple policy optimization},
  author={Xie, Zhengpeng and Zhang, Qiang and Yang, Fan and Hutter, Marco and Xu, Renjing},
  journal={arXiv preprint arXiv:2401.16025},
  year={2024}
}

@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={Proceedings of the nineteenth international conference on machine learning},
  pages={267--274},
  year={2002}
}

@inproceedings{achiam2017constrained,
  title={Constrained policy optimization},
  author={Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  booktitle={International conference on machine learning},
  pages={22--31},
  year={2017},
  organization={PMLR}
}

@article{hunter2004tutorial,
  title={A tutorial on MM algorithms},
  author={Hunter, David R and Lange, Kenneth},
  journal={The American Statistician},
  volume={58},
  number={1},
  pages={30--37},
  year={2004},
  publisher={Taylor \& Francis}
}

@article{zheng2025stabilizing,
  title={Stabilizing Reinforcement Learning with LLMs: Formulation and Practices},
  author={Zheng, Chujie and Dang, Kai and Yu, Bowen and Li, Mingze and Jiang, Huiqiang and Lin, Junrong and Liu, Yuqiong and Lin, Hao and Wu, Chencan and Hu, Feng and others},
  journal={arXiv preprint arXiv:2512.01374},
  year={2025}
}

@article{wang2019trust,
  title={Trust region-guided proximal policy optimization},
  author={Wang, Yuhui and He, Hao and Tan, Xiaoyang and Gan, Yaozhong},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{chen2025minimax,
  title={MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention},
  author={Chen, Aili and Li, Aonian and Gong, Bangwei and Jiang, Binyang and Fei, Bo and Yang, Bo and Shan, Boji and Yu, Changqing and Wang, Chao and Zhu, Cheng and others},
  journal={arXiv preprint arXiv:2506.13585},
  year={2025}
}


@misc{liu-li-2025,
  title = {When Speed Kills Stability: Demystifying RL Collapse from the Inference-Training Mismatch},
  note =  {https://yingru.notion.site/When-Speed-Kills-Stability-Demystifying-RL-Collapse-from-the-Inference-Training-Mismatch-271211a558b7808d8b12d403fd15edda},
  author = {Jiacai Liu and Yingru Li and Yuqian Fu and Jiawei Wang and Qian Liu and Yu Shen},
  year = {2025},
}


@misc{yao2025offpolicy,
  title = {Your Efficient RL Framework Secretly Brings You Off-Policy RL Training},
  note = {https://fengyao.notion.site/off-policy-rl},
  author = {Yao, Feng and Liu, Liyuan and Zhang, Dinghuai and Dong, Chengyu and Shang, Jingbo and Gao, Jianfeng},
  journal = {Feng Yao's Notion},
  year = {2025},
  month = aug,
}

@article{qi2025defeating,
  title={Defeating the training-inference mismatch via fp16},
  author={Qi, Penghui and Liu, Zichen and Zhou, Xiangxin and Pang, Tianyu and Du, Chao and Lee, Wee Sun and Lin, Min},
  journal={arXiv preprint arXiv:2510.26788},
  year={2025}
}

@misc{slime_github,
  author       = {Zilin Zhu and Chengxing Xie and Xin Lv and slime Contributors},
  title        = {slime: An LLM post-training framework for RL Scaling},
  year         = {2025},
  howpublished = {\url{https://github.com/THUDM/slime}},
  note         = {GitHub repository. Corresponding author: Xin Lv},
  urldate      = {2025-06-19}
}

@article{hilton2022batch,
  title={Batch size-invariance for policy optimization},
  author={Hilton, Jacob and Cobbe, Karl and Schulman, John},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17086--17098},
  year={2022}
}

@inproceedings{qi2024zero,
  title={Zero bubble (almost) pipeline parallelism},
  author={Qi, Penghui and Wan, Xinyi and Huang, Guangxing and Lin, Min},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{liu2025deepseek,
  title={Deepseek-v3. 2: Pushing the frontier of open large language models},
  author={Liu, Aixin and Mei, Aoxue and Lin, Bangcai and Xue, Bing and Wang, Bingxuan and Xu, Bingzheng and Wu, Bochao and Zhang, Bowei and Lin, Chaofan and Dong, Chen and others},
  journal={arXiv preprint arXiv:2512.02556},
  year={2025}
}


@article{Team2025EveryAM,
  title = {Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning},
  author = {Ling Team and Han, Bin and Tang, Caizhi and Liang, Chen and Zhang, Donghao and Yuan, Fan and Zhu, Feng and Gao, Jie and Hu, Jingyu and Li, Longfei and Li, Meng and Zhang, Mingyang and Jiang, Peijie and Jiao, Peng and Zhao, Qian and Yang, Qingyuan and Shen, Wenbo and Yang, Xinxing and Zhang, Yalin and Ren, Yankun and Zhao, Yao and Cao, Yibo and Sun, Yixuan and Zhang, Yue and Fang, Yuchen and Lin, Zibin and Cheng, Zixuan and Zhou, Jun},
  journal = {arXiv preprint arXiv:2510.19338},
  year = {2025},
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@inproceedings{
  ren2025learning_dynamics_LLM,
  title={Learning Dynamics of {LLM} Finetuning},
  author={Yi Ren and Danica J. Sutherland},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
}

@article{yang2025qwen3,
  title={Qwen3 technical report},
  author={Yang, An and Li, Anfeng and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Gao, Chang and Huang, Chengen and Lv, Chenxu and others},
  journal={arXiv preprint arXiv:2505.09388},
  year={2025}
}


@article{he2025nondeterminism,
  author = {Horace He},
  title = {Defeating Nondeterminism in LLM Inference},
  journal = {Thinking Machines Lab: Connectionism},
  year = {2025},
  note = {https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/},
  doi = {10.64434/tml.20250910}
}

@article{ma2025stabilizing,
  title={Stabilizing moe reinforcement learning by aligning training and inference routers},
  author={Ma, Wenhan and Zhang, Hailin and Zhao, Liang and Song, Yifan and Wang, Yudong and Sui, Zhifang and Luo, Fuli},
  journal={arXiv preprint arXiv:2510.11370},
  year={2025}
}

@InProceedings{pmlr-v235-tajwar24a,
  title = 	 {Preference Fine-Tuning of {LLM}s Should Leverage Suboptimal, On-Policy Data},
  author =       {Tajwar, Fahim and Singh, Anikait and Sharma, Archit and Rafailov, Rafael and Schneider, Jeff and Xie, Tengyang and Ermon, Stefano and Finn, Chelsea and Kumar, Aviral},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {47441--47474},
  year = 	 {2024},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
}

@inproceedings{
hu2022lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nZeVKeeFYf9}
}

@article{wang2025octothinker,
  title={OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling},
  author={Wang, Zengzhi and Zhou, Fan and Li, Xuefeng and Liu, Pengfei},
  year={2025},
  journal={arXiv preprint arXiv:2506.20512},
  note={Preprint}
}

@article{liu2025gem,
  title={GEM: A Gym for Agentic LLMs},
  author={Liu, Zichen and Sims, Anya and Duan, Keyu and Chen, Changyu and Yu, Simon and Zhou, Xiangxin and Xu, Haotian and Xiong, Shaopan and Liu, Bo and Tan, Chenmien and others},
  journal={arXiv preprint arXiv:2510.01051},
  year={2025}
}

@article{schulman2025lora,
  author = {John Schulman and Thinking Machines Lab},
  title = {LoRA Without Regret},
  journal = {Thinking Machines Lab: Connectionism},
  year = {2025},
  note = {https://thinkingmachines.ai/blog/lora/},
  doi = {10.64434/tml.20250929},
}

@misc{AIME,
    title = {American Invitational Mathematics Examination - AIME},
    author = {MAA},
    howpublished={\url{https://maa.org/}},
    year = {2025}
}

@article{wang2025beyond,
  title={Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning},
  author={Wang, Shenzhi and Yu, Le and Gao, Chang and Zheng, Chujie and Liu, Shixuan and Lu, Rui and Dang, Kai and Chen, Xionghui and Yang, Jianxin and Zhang, Zhenru and others},
  journal={arXiv preprint arXiv:2506.01939},
  year={2025}
}

@article{team2025every,
  title={Every step evolves: Scaling reinforcement learning for trillion-scale thinking model},
  author={Team, Ling and Shen, Anqi and Li, Baihui and Hu, Bin and Jing, Bin and Chen, Cai and Huang, Chao and Zhang, Chao and Yang, Chaokun and Lin, Cheng and others},
  journal={arXiv preprint arXiv:2510.18855},
  year={2025}
}


@article{qi2025optimizing,
  title={Optimizing anytime reasoning via budget relative policy optimization},
  author={Qi, Penghui and Liu, Zichen and Pang, Tianyu and Du, Chao and Lee, Wee Sun and Lin, Min},
  journal={arXiv preprint arXiv:2505.13438},
  year={2025}
}

@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={53728--53741},
  year={2023}
}

@article{khatri2025art,
  title={The art of scaling reinforcement learning compute for llms},
  author={Khatri, Devvrit and Madaan, Lovish and Tiwari, Rishabh and Bansal, Rachit and Duvvuri, Sai Surya and Zaheer, Manzil and Dhillon, Inderjit S and Brandfonbrener, David and Agarwal, Rishabh},
  journal={arXiv preprint arXiv:2510.13786},
  year={2025}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{zhang2025deterministic,
  title={Deterministic inference across tensor parallel sizes that eliminates training-inference mismatch},
  author={Zhang, Ziyang and Ding, Xinheng and Yuan, Jiayi and Liu, Rixin and Mao, Huizi and Xing, Jiarong and Liu, Zirui},
  journal={arXiv preprint arXiv:2511.17826},
  year={2025}
}

@article{wan2026revisiting,
  title={Revisiting Parameter Server in LLM Post-Training},
  author={Wan, Xinyi and Qi, Penghui and Huang, Guangxing and Ruan, Chaoyi and Lin, Min and Li, Jialin},
  journal={arXiv preprint arXiv:2601.19362},
  year={2026}
}

