@String(PAMI  = {IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)})
@string(ICML = {International Conference on Machine Learning (ICML)})
@string(ICMLW = {International Conference on Machine Learning (ICML) Workshop})
@String(IJCV  = {International Journal of Computer Vision (IJCV)})
@String(TOG   = {ACM Transactions on Graphics (TOG)})
@String(TIP   = {IEEE Transactions on Image Processing (TIP)})
@String(TVCG  = {IEEE Transactions on Visualization and Computer Graphics (TVCG)})
@String(TCSVT = {IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)})
@String(TMM   =	{IEEE Transactions on Multimedia (TMM)})
@String(PR = {Pattern Recognition (PR)})
@String(JMLR = {Journal of Machine Learning Research (JMLR)})

@String(CVPR  = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)})
@String(ICCV  = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)})
@String(ECCV  = {Proceedings of the European Conference on Computer Vision (ECCV)})
@String(NIPS  = {Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)})
@String(ICPR  = {Proceedings of the International Conference on Pattern Recognition (ICPR)})
@String(BMVC  =	{Proceedings of the British Machine Vision Conference (BMVC)})
@String(ACMMM = {Proceedings of the ACM International Conference on Multimedia (ACM MM)})
@String(ICME  =	{Proceedings of the IEEE International Conference on Multimedia & Expo (ICME)})
@String(ICASSP=	{Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP)})
@String(ICIP  = {Proceedings of the IEEE International Conference on Image Processing (ICIP)})
@String(ACCV  = {Proceedings of the Asian Conference on Computer Vision (ACCV)})
@String(WACV  = {Proceedings of the Winter Conference on Applications of Computer Vision (WACV)})
@String(ICLR  = {Proceedings of the International Conference on Learning Representations (ICLR)})
@String(ICLRW  = {Proceedings of the International Conference on Learning Representations Workshop (ICLRW)})
@String(IJCAI = {Proceedings of the International Joint Conferences on Artificial Intelligence (IJCAI)})
@String(AAAI = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)})
@String(CVPRW= {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW})
@String(ICCVW= {Proceedings of the IEEE International Conference on Computer Vision Workshops (ICCVW)})
@String(SIGGRAPH= {Proceedings ACM SIGGRAPH conference (SIGGRAPH)})
@string(EMNLP={Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)})
@string(EMNLPW={Proceedings of the Conference on Empirical Methods in Natural Language Processing Workshop (EMNLPW)})


@inproceedings{evalplus,
  title={Is your code generated by {ChatGPT} really correct? Rigorous evaluation of large language models for code generation},
  author={Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
  booktitle={Proceedings of the 37th International Conference on Neural Information Processing Systems},
  pages={21558--21572},
  year={2023}
}


@misc{si2024llmsgeneratenovelresearch,
      title={Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers}, 
      author={Chenglei Si and Diyi Yang and Tatsunori Hashimoto},
      year={2024},
      eprint={2409.04109},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@InProceedings{liu2023visual,
      title={Visual Instruction Tuning}, 
      author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
      year={2023},
      eprint={2304.08485},
      booktitle={arXiv},
      primaryClass={cs.CV}
}

@misc{openai_system_card,
  author       = {OpenAI},
  title        = {O3 and O4 Mini System Card},
  howpublished = {\url{https://openai.com/index/o3-o4-mini-system-card/}},
  year = {2025},
  note         = {Accessed: 2025-06-15}
}

@misc{xu2025theagentcompanybenchmarkingllmagents,
      title={TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks}, 
      author={Frank F. Xu and Yufan Song and Boxuan Li and Yuxuan Tang and Kritanjali Jain and Mengxue Bao and Zora Z. Wang and Xuhui Zhou and Zhitong Guo and Murong Cao and Mingyang Yang and Hao Yang Lu and Amaad Martin and Zhe Su and Leander Maben and Raj Mehta and Wayne Chi and Lawrence Jang and Yiqing Xie and Shuyan Zhou and Graham Neubig},
      year={2025},
      eprint={2412.14161},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@InProceedings{radford2021learning,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      booktitle={arXiv},
      primaryClass={cs.CV}
}


@article{qin2023linearized,
  title={Linearized Relative Positional Encoding},
  author={Qin, Zhen and Sun, Weixuan and Lu, Kaiyue and Deng, Hui and Li, Dongxu and Han, Xiaodong and Dai, Yuchao and Kong, Lingpeng and Zhong, Yiran},
  journal={Transactions on Machine Learning Research},
  year={2023}
}

@misc{2210.15424,
Author = {Teven Le Scao and Thomas Wang and Daniel Hesslow and Lucile Saulnier and Stas Bekman and M Saiful Bari and Stella Biderman and Hady Elsahar and Niklas Muennighoff and Jason Phang and Ofir Press and Colin Raffel and Victor Sanh and Sheng Shen and Lintang Sutawika and Jaesung Tae and Zheng Xin Yong and Julien Launay and Iz Beltagy},
Title = {What Language Model to Train if You Have One Million {GPU} Hours?},
Year = {2022},
Eprint = {arXiv:2210.15424},
}
@article{kalamkar2019study,
  title={A study of BFLOAT16 for deep learning training},
  author={Kalamkar, Dhiraj and Mudigere, Dheevatsa and Mellempudi, Naveen and Das, Dipankar and Banerjee, Kunal and Avancha, Sasikanth and Vooturi, Dharma Teja and Jammalamadaka, Nataraj and Huang, Jianyu and Yuen, Hector and others},
  journal={arXiv preprint arXiv:1905.12322},
  year={2019}
}

@article{micikevicius2017mixed,
  title={Mixed precision training},
  author={Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and others},
  journal={arXiv preprint arXiv:1710.03740},
  year={2017}
}

@article{shoeybi2019megatron,
  title={{Megatron-LM}: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{zhao2023pytorch,
  title={{PyTorch FSDP}: experiences on scaling fully sharded data parallel},
  author={Zhao, Yanli and Gu, Andrew and Varma, Rohan and Luo, Liang and Huang, Chien-Chin and Xu, Min and Wright, Less and Shojanazeri, Hamid and Ott, Myle and Shleifer, Sam and others},
  journal={arXiv preprint arXiv:2304.11277},
  year={2023}
}

@inproceedings{swintransformer,
  title={{Swin Transformer}: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10012--10022},
  year={2021}
}

@inproceedings{wmt,
  title={Findings of the 2014 workshop on statistical machine translation},
  author={Bojar, Ond{\v{r}}ej and Buck, Christian and Federmann, Christian and Haddow, Barry and Koehn, Philipp and Leveling, Johannes and Monz, Christof and Pecina, Pavel and Post, Matt and Saint-Amand, Herve and others},
  booktitle={Proceedings of the ninth workshop on statistical machine translation},
  pages={12--58},
  year={2014}
}

@inproceedings{vivit,
  title={{ViViT}: A video vision transformer},
  author={Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\v{c}}i{\'c}, Mario and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6836--6846},
  year={2021}
}


@article{vit,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@misc{qwen2025qwen25technicalreport,
      title={Qwen2.5 Technical Report}, 
      author={Qwen and : and An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tianyi Tang and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
      year={2025},
      eprint={2412.15115},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@inproceedings{huang-etal-2020-improve,
    title = "Improve Transformer Models with Better Relative Position Embeddings",
    author = "Huang, Zhiheng  and
      Liang, Davis  and
      Xu, Peng  and
      Xiang, Bing",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.298",
    doi = "10.18653/v1/2020.findings-emnlp.298",
    pages = "3327--3335",
    abstract = "The transformer model has demonstrated superior results on NLP tasks including machine translation and question answering. In this paper, we argue that the position information is not fully utilized in existing work. For example, the initial proposal of a sinusoid embedding is fixed and not learnable. In this paper, we first review the absolute position embeddings and existing relative position embedding methods. We then propose new methods to encourage increased interaction between query, key and relative position embeddings in the self-attention mechanism. Our most promising approach is a generalization of the absolute position embedding. Our method results in increased accuracy compared to previous approaches in absolute and relative position embeddings on the SQuAD1.1 dataset. In addition, we address the inductive property of whether a position embedding can be robust enough to handle long sequences. We demonstrate empirically that our relative embedding method can be reasonably generalized to and is robust in the inductive perspective. Finally, we show that our proposed method can be effectively and efficiently adopted as a near drop-in replacement for improving the accuracy of large models with little computational overhead.",
}

@inproceedings{loshchilovdecoupled,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={International Conference on Learning Representations},
  year={2019},
}

@inproceedings{mao-2022-fine,
    title = "Fine-Tuning Pre-trained Transformers into Decaying Fast Weights",
    author = "Mao, Huanru Henry",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.697",
    pages = "10236--10242",
    abstract = "Autoregressive Transformers are strong language models but incur O(T) complexity during per-token generation due to the self-attention mechanism. Recent work proposes kernel-based methods to approximate causal self-attention by replacing it with recurrent formulations with various update rules and feature maps to achieve O(1) time and memory complexity. We explore these approaches and find that they are unnecessarily complex, and propose a simple alternative - decaying fast weights - that runs fast on GPU, outperforms prior methods, and retains 99{\%} of attention{'}s performance for GPT-2. We also show competitive performance on WikiText-103 against more complex attention substitutes.",
}

@inproceedings{
    zhen2022cosformer,
    title={{cosFormer}: Rethinking Softmax In Attention},
    author={Zhen Qin and Weixuan Sun and Hui Deng and Dongxu Li and Yunshen Wei and Baohong Lv and Junjie Yan and Lingpeng Kong and Yiran Zhong},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=Bl8CQrx2Up4}
}

@inproceedings{jimenez2024swebenchlanguagemodelsresolve,
      title={{SWE}-bench: Can Language Models Resolve Real-World GitHub Issues?}, 
      author={Carlos E. Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik Narasimhan},
      booktitle={International Conference on Learning Representations},
      year={2024},
      url={https://openreview.net/forum?id=VTF8yNQM66}
}

@book{Advanced.Algebra,
    title={Advanced Algebra},
    author={Musheng Yao and Advanced Algebra},
    year={2015},
    publisher={Fudan University Press}
}

@article{chen2021permuteformer,
  title={{PermuteFormer}: Efficient relative position encoding for long sequences},
  author={Chen, Peng},
  journal={arXiv preprint arXiv:2109.02377},
  year={2021}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{katharopoulos2020transformers,
  title={Transformers are {RNNs}: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International Conference on Machine Learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}

@inproceedings{
    choromanski2021rethinking,
    title={Rethinking Attention with {Performers}},
    author={Krzysztof Marcin Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Quincy Davis and Afroz Mohiuddin and Lukasz Kaiser and David Benjamin Belanger and Lucy J Colwell and Adrian Weller},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=Ua6zuk0WRH}
}

@InProceedings{hao2023improving,
      title={Improving Audio-Visual Segmentation with Bidirectional Generation}, 
      author={Dawei Hao and Yuxin Mao and Bowen He and Xiaodong Han and Yuchao Dai and Yiran Zhong},
      year={2024},
      booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence}
}


@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}


@inproceedings{shaw-etal-2018-self,
    title = "Self-Attention with Relative Position Representations",
    author = "Shaw, Peter  and
      Uszkoreit, Jakob  and
      Vaswani, Ashish",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2074",
    doi = "10.18653/v1/N18-2074",
    pages = "464--468",
    abstract = "Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.",
}


@inproceedings{dai-etal-2019-transformer,
    title = "Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context",
    author = "Dai, Zihang  and
      Yang, Zhilin  and
      Yang, Yiming  and
      Carbonell, Jaime  and
      Le, Quoc  and
      Salakhutdinov, Ruslan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1285",
    doi = "10.18653/v1/P19-1285",
    pages = "2978--2988",
    abstract = "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80{\%} longer than RNNs and 450{\%} longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
}

@misc{chen2023extending,
      title={Extending Context Window of Large Language Models via Positional Interpolation}, 
      author={Shouyuan Chen and Sherman Wong and Liangjian Chen and Yuandong Tian},
      year={2023},
      eprint={2306.15595},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xiao2023efficient,
      title={Efficient Streaming Language Models with Attention Sinks}, 
      author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
      year={2023},
      eprint={2309.17453},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{su2021roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Wen, Bo and Liu, Yunfeng},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}

@misc{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  howpublished={\url{https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf}},
  year={2018}
}

@misc{
    liu2020roberta,
    title={Ro{\{}BERT{\}}a: A Robustly Optimized {\{}BERT{\}} Pretraining Approach},
    author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
    year={2020},
    url={https://openreview.net/forum?id=SyxS0T4tvS}
}

@inproceedings{wang2018glue,
  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={353--355},
  year={2018}
}

@book{horn2012matrix,
  title={Matrix analysis},
  author={Horn, Roger A and Johnson, Charles R},
  year={2012},
  publisher={Cambridge university press}
}

@book{golub2013matrix,
  title={Matrix computations},
  author={Golub, Gene H and Van Loan, Charles F},
  year={2013},
  publisher={JHU press}
}

@book{bracewell1986fourier,
  title={The Fourier transform and its applications},
  author={Bracewell, Ronald Newbold and Bracewell, Ronald N},
  volume={31999},
  year={1986},
  publisher={McGraw-hill New York}
}


@inproceedings{hua2022transformer,
  title={Transformer quality in linear time},
  author={Hua, Weizhe and Dai, Zihang and Liu, Hanxiao and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={9099--9117},
  year={2022},
  organization={PMLR}
}

@article{ott2019fairseq,
  title={fairseq: A fast, extensible toolkit for sequence modeling},
  author={Ott, Myle and Edunov, Sergey and Baevski, Alexei and Fan, Angela and Gross, Sam and Ng, Nathan and Grangier, David and Auli, Michael},
  journal={arXiv preprint arXiv:1904.01038},
  year={2019}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{merity2017pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={5th International Conference on Learning Representations, {ICLR},
               Toulon, France},
  year={2017}
}

@inproceedings{post-2018-call,
    title = "A Call for Clarity in Reporting {BLEU} Scores",
    author = "Post, Matt",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6319",
    doi = "10.18653/v1/W18-6319",
    pages = "186--191",
    abstract = "The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to {``}the{''} BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide a new tool, SACREBLEU, to facilitate this.",
}


@inproceedings{papineni2002bleu,
  title={{BLEU}: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@inproceedings{liutkus2021relative,
  title={Relative positional encoding for transformers with linear complexity},
  author={Liutkus, Antoine and Cífka, Ondřej and Wu, Shih-Lun and Simsekli, Umut and Yang, Yi-Hsuan and Richard, Gael},
  booktitle={International Conference on Machine Learning},
  pages={7067--7079},
  year={2021},
  organization={PMLR}
}

@article{horn2021translational,
  title={Translational equivariance in kernelizable attention},
  author={Horn, Max and Shridhar, Kumar and Groenewald, Elrich and Baumann, Philipp FM},
  journal={arXiv preprint arXiv:2102.07680},
  year={2021}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{
    dosovitskiy2021an,
    title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
    author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{gulati20_interspeech,
  author={Anmol Gulati and James Qin and Chung-Cheng Chiu and Niki Parmar and Yu Zhang and Jiahui Yu and Wei Han and Shibo Wang and Zhengdong Zhang and Yonghui Wu and Ruoming Pang},
  title={{Conformer: Convolution-augmented Transformer for Speech Recognition}},
  year=2020,
  booktitle={Proc. Interspeech 2020},
  pages={5036--5040},
  doi={10.21437/Interspeech.2020-3015}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@inproceedings{cho-etal-2014-learning,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
}


@inproceedings{gehring2017convolutional,
  title={Convolutional sequence to sequence learning},
  author={Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
  booktitle={International Conference on Machine Learning},
  pages={1243--1252},
  year={2017},
  organization={PMLR}
}

@inproceedings{
    ke2021rethinking,
    title={Rethinking Positional Encoding in Language Pre-training},
    author={Guolin Ke and Di He and Tie-Yan Liu},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=09-528y2Fgf}
}

@article{raffel2019exploring, 
    title={Exploring the limits of transfer learning with a unified text-to-text transformer}, 
    author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J}, 
    journal={arXiv preprint arXiv:1910.10683}, 
    year={2019} 
}

@article{roy2021efficient,
  title={Efficient content-based sparse attention with routing transformers},
  author={Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={53--68},
  year={2021},
  publisher={MIT Press}
}

@article{sukhbaatar2015end,
  title={End-to-end memory networks},
  author={Sukhbaatar, Sainbayar and Weston, Jason and Fergus, Rob and others},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}


@article{vyas2020fast,
  title={Fast transformers with clustered attention},
  author={Vyas, Apoorv and Katharopoulos, Angelos and Fleuret, Fran{\c{c}}ois},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21665--21674},
  year={2020}
}

@inproceedings{xiong2021nystromformer,
  title={Nystr{\"o}mformer: A Nystr{\"o}m-based Algorithm for Approximating Self-Attention},
  author={Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
  booktitle={AAAI},
  year={2021}
}

@article{islam2020much,
  title={How much position information do convolutional neural networks encode?},
  author={Islam, Md Amirul and Jia, Sen and Bruce, Neil DB},
  journal={arXiv preprint arXiv:2001.08248},
  year={2020}
}

@inproceedings{metaformer,
  author       = {Weihao Yu and
                  Mi Luo and
                  Pan Zhou and
                  Chenyang Si and
                  Yichen Zhou and
                  Xinchao Wang and
                  Jiashi Feng and
                  Shuicheng Yan},
  title        = {MetaFormer is Actually What You Need for Vision},
  booktitle    = {{IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
                  {CVPR} 2022, New Orleans, LA, USA, June 18-24, 2022},
  pages        = {10809--10819},
  publisher    = {{IEEE}},
  year         = {2022},
  url          = {https://doi.org/10.1109/CVPR52688.2022.01055},
  doi          = {10.1109/CVPR52688.2022.01055},
  timestamp    = {Wed, 05 Oct 2022 16:31:19 +0200},
  biburl       = {https://dblp.org/rec/conf/cvpr/YuLZSZWFY22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{lru,
  author       = {Antonio Orvieto and
                  Samuel L. Smith and
                  Albert Gu and
                  Anushan Fernando and
                  {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
                  Razvan Pascanu and
                  Soham De},
  title        = {Resurrecting Recurrent Neural Networks for Long Sequences},
  journal      = {CoRR},
  volume       = {abs/2303.06349},
  year         = {2023},
  doi          = {10.48550/arXiv.2303.06349},
  eprinttype    = {arXiv},
  eprint       = {2303.06349},
  timestamp    = {Thu, 16 Mar 2023 16:04:57 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2303-06349.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{iclr18,
  author       = {Eric Martin and
                  Chris Cundy},
  title        = {Parallelizing Linear Recurrent Neural Nets Over Sequence Length},
  booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018,
                  Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher    = {OpenReview.net},
  year         = {2018},
  url          = {https://openreview.net/forum?id=HyUNwulC-},
  timestamp    = {Thu, 25 Jul 2019 14:25:41 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/MartinC18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{s4,
  author       = {Albert Gu and
                  Karan Goel and
                  Christopher R{\'{e}}},
  title        = {Efficiently Modeling Long Sequences with Structured State Spaces},
  booktitle    = {The Tenth International Conference on Learning Representations, {ICLR}
                  2022, Virtual Event, April 25-29, 2022},
  publisher    = {OpenReview.net},
  year         = {2022},
  url          = {https://openreview.net/forum?id=uYLFoz1vlAC},
  timestamp    = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/GuGR22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{unified,
  author       = {Hongyu He and
                  Marko Kabic},
  title        = {A Unified View of Long-Sequence Models towards Modeling Million-Scale
                  Dependencies},
  journal      = {CoRR},
  volume       = {abs/2302.06218},
  year         = {2023},
  doi          = {10.48550/arXiv.2302.06218},
  eprinttype    = {arXiv},
  eprint       = {2302.06218},
  timestamp    = {Mon, 20 Feb 2023 14:27:28 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2302-06218.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{hyena,
  author       = {Michael Poli and
                  Stefano Massaroli and
                  Eric Nguyen and
                  Daniel Y. Fu and
                  Tri Dao and
                  Stephen Baccus and
                  Yoshua Bengio and
                  Stefano Ermon and
                  Christopher R{\'{e}}},
  title        = {Hyena Hierarchy: Towards Larger Convolutional Language Models},
  journal      = {CoRR},
  volume       = {abs/2302.10866},
  year         = {2023},
  doi          = {10.48550/arXiv.2302.10866},
  eprinttype    = {arXiv},
  eprint       = {2302.10866},
  timestamp    = {Fri, 24 Feb 2023 11:55:23 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2302-10866.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{h3,
  author       = {Tri Dao and
                  Daniel Y. Fu and
                  Khaled Kamal Saab and
                  Armin W. Thomas and
                  Atri Rudra and
                  Christopher R{\'{e}}},
  title        = {Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
  journal      = {CoRR},
  volume       = {abs/2212.14052},
  year         = {2022},
  doi          = {10.48550/arXiv.2212.14052},
  eprinttype    = {arXiv},
  eprint       = {2212.14052},
  timestamp    = {Sun, 08 Jan 2023 14:16:55 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2212-14052.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{glu,
  author       = {Noam Shazeer},
  title        = {{GLU} Variants Improve Transformer},
  journal      = {CoRR},
  volume       = {abs/2002.05202},
  year         = {2020},
  eprinttype    = {arXiv},
  eprint       = {2002.05202},
  timestamp    = {Fri, 14 Feb 2020 12:07:41 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2002-05202.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gu-improving,
  author       = {Albert Gu and
                  {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
                  Thomas Paine and
                  Matt Hoffman and
                  Razvan Pascanu},
  title        = {Improving the Gating Mechanism of Recurrent Neural Networks},
  booktitle    = {Proceedings of the 37th International Conference on Machine Learning,
                  {ICML} 2020, 13-18 July 2020, Virtual Event},
  series       = {Proceedings of Machine Learning Research},
  volume       = {119},
  pages        = {3800--3809},
  publisher    = {{PMLR}},
  year         = {2020},
  url          = {http://proceedings.mlr.press/v119/gu20a.html},
  timestamp    = {Tue, 15 Dec 2020 17:40:18 +0100},
  biburl       = {https://dblp.org/rec/conf/icml/GuGP0P20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{dao2022flashattention,
  title={{FlashAttention}: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}


@misc{gu2022parameterization,
      title={On the Parameterization and Initialization of Diagonal State Space Models}, 
      author={Albert Gu and Ankit Gupta and Karan Goel and Christopher Ré},
      year={2022},
      eprint={2206.11893},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Elfwing2017SigmoidWeightedLU,
  title={Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning},
  author={Stefan Elfwing and Eiji Uchibe and Kenji Doya},
  journal={Neural networks : the official journal of the International Neural Network Society},
  year={2017},
  volume={107},
  pages={
          3-11
        }
}

@article{Zhou2016MinimalGU,
  title={Minimal gated unit for recurrent neural networks},
  author={Guoxiang Zhou and Jianxin Wu and Chen-Lin Zhang and Zhi-Hua Zhou},
  journal={International Journal of Automation and Computing},
  year={2016},
  volume={13},
  pages={226-234}
}
@article{Greff2015LSTMAS,
  title={LSTM: A Search Space Odyssey},
  author={Klaus Greff and Rupesh Kumar Srivastava and Jan Koutn{\'i}k and Bas R. Steunebrink and J{\"u}rgen Schmidhuber},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2015},
  volume={28},
  pages={2222-2232}
}

@inproceedings{lei-etal-2018-simple,
    title = "Simple Recurrent Units for Highly Parallelizable Recurrence",
    author = "Lei, Tao  and
      Zhang, Yu  and
      Wang, Sida I.  and
      Dai, Hui  and
      Artzi, Yoav",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1477",
    doi = "10.18653/v1/D18-1477",
    pages = "4470--4481",
    abstract = "Common recurrent neural architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU), a light recurrent unit that balances model capacity and scalability. SRU is designed to provide expressive recurrence, enable highly parallelized implementation, and comes with careful initialization to facilitate training of deep models. We demonstrate the effectiveness of SRU on multiple NLP tasks. SRU achieves 5{---}9x speed-up over cuDNN-optimized LSTM on classification and question answering datasets, and delivers stronger results than LSTM and convolutional models. We also obtain an average of 0.7 BLEU improvement over the Transformer model (Vaswani et al., 2017) on translation by incorporating SRU into the architecture.",
}

@inproceedings{zhou2021informer,
  title={Informer: Beyond efficient transformer for long sequence time-series forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={12},
  pages={11106--11115},
  year={2021}
}

@misc{wang2020linformer,
      title={Linformer: Self-Attention with Linear Complexity}, 
      author={Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},
      year={2020},
      eprint={2006.04768},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{s4aslinearrnn,
  author       = {Ankit Gupta and
                  Harsh Mehta and
                  Jonathan Berant},
  title        = {Simplifying and Understanding State Space Models with Diagonal Linear
                  RNNs},
  journal      = {CoRR},
  volume       = {abs/2212.00768},
  year         = {2022},
  doi          = {10.48550/arXiv.2212.00768},
  eprinttype    = {arXiv},
  eprint       = {2212.00768},
  timestamp    = {Thu, 08 Dec 2022 15:26:56 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2212-00768.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{s44mt,
  author       = {Ali Vardasbi and
                  Telmo Pessoa Pires and
                  Robin M. Schmidt and
                  Stephan Peitz},
  title        = {State Spaces Aren't Enough: Machine Translation Needs Attention},
  journal      = {CoRR},
  volume       = {abs/2304.12776},
  year         = {2023},
  doi          = {10.48550/arXiv.2304.12776},
  eprinttype    = {arXiv},
  eprint       = {2304.12776},
  timestamp    = {Wed, 03 May 2023 14:12:58 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2304-12776.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{peng2023rwkv,
      title={RWKV: Reinventing RNNs for the Transformer Era}, 
      author={Bo Peng and Eric Alcaide and Quentin Anthony and Alon Albalak and Samuel Arcadinho and Huanqi Cao and Xin Cheng and Michael Chung and Matteo Grella and Kranthi Kiran GV and Xuzheng He and Haowen Hou and Przemyslaw Kazienko and Jan Kocon and Jiaming Kong and Bartlomiej Koptyra and Hayden Lau and Krishna Sri Ipsit Mantri and Ferdinand Mom and Atsushi Saito and Xiangru Tang and Bolun Wang and Johan S. Wind and Stansilaw Wozniak and Ruichong Zhang and Zhenyuan Zhang and Qihang Zhao and Peng Zhou and Jian Zhu and Rui-Jie Zhu},
      year={2023},
      booktitle={arXiv 2305.13048}
}
@inproceedings{lra,
  author       = {Yi Tay and
                  Mostafa Dehghani and
                  Samira Abnar and
                  Yikang Shen and
                  Dara Bahri and
                  Philip Pham and
                  Jinfeng Rao and
                  Liu Yang and
                  Sebastian Ruder and
                  Donald Metzler},
  title        = {Long Range Arena : {A} Benchmark for Efficient Transformers},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=qVyeW-grC2k},
  timestamp    = {Wed, 23 Jun 2021 17:36:40 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/Tay0ASBPRYRM21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{tcn,
  author       = {Colin Lea and
                  Michael D. Flynn and
                  Ren{\'{e}} Vidal and
                  Austin Reiter and
                  Gregory D. Hager},
  title        = {Temporal Convolutional Networks for Action Segmentation and Detection},
  booktitle    = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition,
                  {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017},
  pages        = {1003--1012},
  publisher    = {{IEEE} Computer Society},
  year         = {2017},
  url          = {https://doi.org/10.1109/CVPR.2017.113},
  doi          = {10.1109/CVPR.2017.113},
  timestamp    = {Fri, 24 Mar 2023 00:02:57 +0100},
  biburl       = {https://dblp.org/rec/conf/cvpr/LeaFVRH17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wavenet,
  author       = {A{\"{a}}ron van den Oord and
                  Sander Dieleman and
                  Heiga Zen and
                  Karen Simonyan and
                  Oriol Vinyals and
                  Alex Graves and
                  Nal Kalchbrenner and
                  Andrew W. Senior and
                  Koray Kavukcuoglu},
  title        = {WaveNet: {A} Generative Model for Raw Audio},
  booktitle    = {The 9th {ISCA} Speech Synthesis Workshop, Sunnyvale, CA, USA, 13-15
                  September 2016},
  pages        = {125},
  publisher    = {{ISCA}},
  year         = {2016},
  url          = {http://www.isca-speech.org/archive/SSW\_2016/abstracts/ssw9\_DS-4\_van\_den\_Oord.html},
  timestamp    = {Tue, 16 Nov 2021 11:36:20 +0100},
  biburl       = {https://dblp.org/rec/conf/ssw/OordDZSVGKSK16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{qrnn,
  author       = {James Bradbury and
                  Stephen Merity and
                  Caiming Xiong and
                  Richard Socher},
  title        = {Quasi-Recurrent Neural Networks},
  booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017,
                  Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher    = {OpenReview.net},
  year         = {2017},
  url          = {https://openreview.net/forum?id=H1zJ-v5xl},
  timestamp    = {Thu, 25 Jul 2019 14:25:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/0002MXS17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{strongtypedrnn,
  author       = {David Balduzzi and
                  Muhammad Ghifary},
  editor       = {Maria{-}Florina Balcan and
                  Kilian Q. Weinberger},
  title        = {Strongly-Typed Recurrent Neural Networks},
  booktitle    = {Proceedings of the 33nd International Conference on Machine Learning,
                  {ICML} 2016, New York City, NY, USA, June 19-24, 2016},
  series       = {{JMLR} Workshop and Conference Proceedings},
  volume       = {48},
  pages        = {1292--1300},
  publisher    = {JMLR.org},
  year         = {2016},
  url          = {http://proceedings.mlr.press/v48/balduzzi16.html},
  timestamp    = {Wed, 29 May 2019 08:41:46 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/BalduzziG16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{mega,
  author       = {Xuezhe Ma and
                  Chunting Zhou and
                  Xiang Kong and
                  Junxian He and
                  Liangke Gui and
                  Graham Neubig and
                  Jonathan May and
                  Luke Zettlemoyer},
  title        = {Mega: Moving Average Equipped Gated Attention},
  journal      = {CoRR},
  volume       = {abs/2209.10655},
  year         = {2022},
  doi          = {10.48550/arXiv.2209.10655},
  eprinttype    = {arXiv},
  eprint       = {2209.10655},
  timestamp    = {Wed, 28 Sep 2022 15:17:28 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2209-10655.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{pretrainingwoattn,
  author       = {Junxiong Wang and
                  Jing Nathan Yan and
                  Albert Gu and
                  Alexander M. Rush},
  title        = {Pretraining Without Attention},
  journal      = {CoRR},
  volume       = {abs/2212.10544},
  year         = {2022},
  doi          = {10.48550/arXiv.2212.10544},
  eprinttype    = {arXiv},
  eprint       = {2212.10544},
  timestamp    = {Wed, 04 Jan 2023 16:01:37 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2212-10544.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{zaheer2020big,
  title={Big {B}ird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={17283--17297},
  year={2020}
}

@inproceedings{kitaev2020reformer,
  title={Reformer: The Efficient Transformer},
  author={Kitaev, Nikita and Kaiser, Lukasz and Levskaya, Anselm},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
 
@inproceedings{gupta2022DSS,
  author       = {Ankit Gupta and
                  Albert Gu and
                  Jonathan Berant},
  title        = {Diagonal State Spaces are as Effective as Structured State Spaces},
  booktitle    = {NeurIPS},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/9156b0f6dfa9bbd18c79cc459ef5d61c-Abstract-Conference.html},
  timestamp    = {Fri, 05 May 2023 16:00:57 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/0001GB22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{gupta2022diagonal,
      title={Diagonal State Spaces are as Effective as Structured State Spaces}, 
      author={Ankit Gupta and Albert Gu and Jonathan Berant},
      year={2022},
      eprint={2203.14343},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Choromanski2020RethinkingAW,
  title={Rethinking Attention with Performers},
  author={Krzysztof Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tam{\'a}s Sarl{\'o}s and Peter Hawkins and Jared Davis and Afroz Mohiuddin and Lukasz Kaiser and David Belanger and Lucy J. Colwell and Adrian Weller},
  journal={ArXiv},
  year={2020},
  volume={abs/2009.14794}
}

@article{Li2022WhatMC,
  title={What Makes Convolutional Models Great on Long Sequence Modeling?},
  author={Yuhong Li and Tianle Cai and Yi Zhang and De-huai Chen and Debadeepta Dey},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.09298}
}

@inproceedings{Schlag2021LinearTA,
  title={Linear Transformers Are Secretly Fast Weight Programmers},
  author={Imanol Schlag and Kazuki Irie and J{\"u}rgen Schmidhuber},
  booktitle={International Conference on Machine Learning},
  year={2021}
}

@article{Schmidhuber1992LearningTC,
  title={Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks},
  author={J{\"u}rgen Schmidhuber},
  journal={Neural Computation},
  year={1992},
  volume={4},
  pages={131-139}
}

@inproceedings{rfa,
  author       = {Hao Peng and
                  Nikolaos Pappas and
                  Dani Yogatama and
                  Roy Schwartz and
                  Noah A. Smith and
                  Lingpeng Kong},
  title        = {Random Feature Attention},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=QtTKTdVrFBB},
  timestamp    = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/Peng0Y0SK21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ckconv,
  author       = {David W. Romero and
                  Anna Kuzina and
                  Erik J. Bekkers and
                  Jakub Mikolaj Tomczak and
                  Mark Hoogendoorn},
  title        = {CKConv: Continuous Kernel Convolution For Sequential Data},
  booktitle    = {The Tenth International Conference on Learning Representations, {ICLR}
                  2022, Virtual Event, April 25-29, 2022},
  publisher    = {OpenReview.net},
  year         = {2022},
  url          = {https://openreview.net/forum?id=8FhxBtXSl0},
  timestamp    = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/RomeroKBTH22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
qin2023toeplitz,
title={Toeplitz Neural Network for Sequence Modeling},
author={Zhen Qin and Xiaodong Han and Weixuan Sun and Bowen He and Dong Li and Dongxu Li and Yuchao Dai and Lingpeng Kong and Yiran Zhong},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=IxmWsm4xrua}
}

@inproceedings{swintransformer,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10012--10022},
  year={2021}
}

@inproceedings{wmt,
  title={Findings of the 2014 workshop on statistical machine translation},
  author={Bojar, Ond{\v{r}}ej and Buck, Christian and Federmann, Christian and Haddow, Barry and Koehn, Philipp and Leveling, Johannes and Monz, Christof and Pecina, Pavel and Post, Matt and Saint-Amand, Herve and others},
  booktitle={Proceedings of the ninth workshop on statistical machine translation},
  pages={12--58},
  year={2014}
}




@inproceedings{huang-etal-2020-improve,
    title = "Improve Transformer Models with Better Relative Position Embeddings",
    author = "Huang, Zhiheng  and
      Liang, Davis  and
      Xu, Peng  and
      Xiang, Bing",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.298",
    doi = "10.18653/v1/2020.findings-emnlp.298",
    pages = "3327--3335",
    abstract = "The transformer model has demonstrated superior results on NLP tasks including machine translation and question answering. In this paper, we argue that the position information is not fully utilized in existing work. For example, the initial proposal of a sinusoid embedding is fixed and not learnable. In this paper, we first review the absolute position embeddings and existing relative position embedding methods. We then propose new methods to encourage increased interaction between query, key and relative position embeddings in the self-attention mechanism. Our most promising approach is a generalization of the absolute position embedding. Our method results in increased accuracy compared to previous approaches in absolute and relative position embeddings on the SQuAD1.1 dataset. In addition, we address the inductive property of whether a position embedding can be robust enough to handle long sequences. We demonstrate empirically that our relative embedding method can be reasonably generalized to and is robust in the inductive perspective. Finally, we show that our proposed method can be effectively and efficiently adopted as a near drop-in replacement for improving the accuracy of large models with little computational overhead.",
}



@book{Advanced.Algebra,
    title={Advanced Algebra},
    author={Musheng Yao and Advanced Algebra},
    year={2015},
    publisher={Fudan University Press}
}

@article{chen2021permuteformer,
  title={Permuteformer: Efficient relative position encoding for long sequences},
  author={Chen, Peng},
  journal={arXiv preprint arXiv:2109.02377},
  year={2021}
}





@article{s5,
  author       = {Jimmy T. H. Smith and
                  Andrew Warrington and
                  Scott W. Linderman},
  title        = {Simplified State Space Layers for Sequence Modeling},
  journal      = {CoRR},
  volume       = {abs/2208.04933},
  year         = {2022},
  doi          = {10.48550/arXiv.2208.04933},
  eprinttype    = {arXiv},
  eprint       = {2208.04933},
  timestamp    = {Tue, 16 Aug 2022 16:44:57 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2208-04933.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{dai-etal-2019-transformer,
    title = "Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context",
    author = "Dai, Zihang  and
      Yang, Zhilin  and
      Yang, Yiming  and
      Carbonell, Jaime  and
      Le, Quoc  and
      Salakhutdinov, Ruslan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1285",
    doi = "10.18653/v1/P19-1285",
    pages = "2978--2988",
    abstract = "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80{\%} longer than RNNs and 450{\%} longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
}



@misc{
    liu2020roberta,
    title={Ro{\{}BERT{\}}a: A Robustly Optimized {\{}BERT{\}} Pretraining Approach},
    author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
    year={2020},
    url={https://openreview.net/forum?id=SyxS0T4tvS}
}

@inproceedings{wang2018glue,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={353--355},
  year={2018}
}

@book{horn2012matrix,
  title={Matrix analysis},
  author={Horn, Roger A and Johnson, Charles R},
  year={2012},
  publisher={Cambridge university press}
}

@book{golub2013matrix,
  title={Matrix computations},
  author={Golub, Gene H and Van Loan, Charles F},
  year={2013},
  publisher={JHU press}
}

@book{bracewell1986fourier,
  title={The Fourier transform and its applications},
  author={Bracewell, Ronald Newbold and Bracewell, Ronald N},
  volume={31999},
  year={1986},
  publisher={McGraw-hill New York}
}



@article{ott2019fairseq,
  title={fairseq: A fast, extensible toolkit for sequence modeling},
  author={Ott, Myle and Edunov, Sergey and Baevski, Alexei and Fan, Angela and Gross, Sam and Ng, Nathan and Grangier, David and Auli, Michael},
  journal={arXiv preprint arXiv:1904.01038},
  year={2019}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}



@inproceedings{post-2018-call,
    title = "A Call for Clarity in Reporting {BLEU} Scores",
    author = "Post, Matt",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6319",
    doi = "10.18653/v1/W18-6319",
    pages = "186--191",
    abstract = "The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to {``}the{''} BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide a new tool, SACREBLEU, to facilitate this.",
}


@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@inproceedings{liutkus2021relative,
  title={Relative positional encoding for transformers with linear complexity},
  author={Liutkus, Antoine and Cífka, Ondřej and Wu, Shih-Lun and Simsekli, Umut and Yang, Yi-Hsuan and Richard, Gael},
  booktitle={International Conference on Machine Learning},
  pages={7067--7079},
  year={2021},
  organization={PMLR}
}

@article{horn2021translational,
  title={Translational equivariance in kernelizable attention},
  author={Horn, Max and Shridhar, Kumar and Groenewald, Elrich and Baumann, Philipp FM},
  journal={arXiv preprint arXiv:2102.07680},
  year={2021}
}


@inproceedings{
    dosovitskiy2021an,
    title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
    author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{gulati20_interspeech,
  author={Anmol Gulati and James Qin and Chung-Cheng Chiu and Niki Parmar and Yu Zhang and Jiahui Yu and Wei Han and Shibo Wang and Zhengdong Zhang and Yonghui Wu and Ruoming Pang},
  title={{Conformer: Convolution-augmented Transformer for Speech Recognition}},
  year=2020,
  booktitle={Proc. Interspeech 2020},
  pages={5036--5040},
  doi={10.21437/Interspeech.2020-3015}
}

@inproceedings{
    peng2021random,
    title={Random Feature Attention},
    author={Hao Peng and Nikolaos Pappas and Dani Yogatama and Roy Schwartz and Noah Smith and Lingpeng Kong},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=QtTKTdVrFBB}
}



@inproceedings{gehring2017convolutional,
  title={Convolutional sequence to sequence learning},
  author={Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
  booktitle={International Conference on Machine Learning},
  pages={1243--1252},
  year={2017},
  organization={PMLR}
}



@article{raffel2019exploring, 
    title={Exploring the limits of transfer learning with a unified text-to-text transformer}, 
    author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J}, 
    journal={arXiv preprint arXiv:1910.10683}, 
    year={2019} 
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}



@inproceedings{tay2020sparse,
  title={Sparse sinkhorn attention},
  author={Tay, Yi and Bahri, Dara and Yang, Liu and Metzler, Donald and Juan, Da-Cheng},
  booktitle={International Conference on Machine Learning},
  pages={9438--9447},
  year={2020},
  organization={PMLR}
}

@article{roy2021efficient,
  title={Efficient content-based sparse attention with routing transformers},
  author={Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={53--68},
  year={2021},
  publisher={MIT Press}
}

@article{sukhbaatar2015end,
  title={End-to-end memory networks},
  author={Sukhbaatar, Sainbayar and Weston, Jason and Fergus, Rob and others},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{vyas2020fast,
  title={Fast transformers with clustered attention},
  author={Vyas, Apoorv and Katharopoulos, Angelos and Fleuret, Fran{\c{c}}ois},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21665--21674},
  year={2020}
}


@article{islam2020much,
  title={How much position information do convolutional neural networks encode?},
  author={Islam, Md Amirul and Jia, Sen and Bruce, Neil DB},
  journal={arXiv preprint arXiv:2001.08248},
  year={2020}
}

@inproceedings{
zhu2021longshort,
title={Long-Short Transformer: Efficient Transformers for Language and Vision},
author={Chen Zhu and Wei Ping and Chaowei Xiao and Mohammad Shoeybi and Tom Goldstein and Anima Anandkumar and Bryan Catanzaro},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=M_lkFOwVdYc}
}

@article{choromanski2020rethinking,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020}
}

@article{liu2021pay, title={Pay attention to mlps}, author={Liu, Hanxiao and Dai, Zihang and So, David and Le, Quoc V}, journal={Advances in Neural Information Processing Systems}, volume={34}, pages={9204--9215}, year={2021} }

@inproceedings{tay2021synthesizer,
  title={Synthesizer: Rethinking self-attention for transformer models},
  author={Tay, Yi and Bahri, Dara and Metzler, Donald and Juan, Da-Cheng and Zhao, Zhe and Zheng, Che},
  booktitle={International conference on machine learning},
  pages={10183--10192},
  year={2021},
  organization={PMLR}
}

@article{simplelongconv,
  author       = {Daniel Y. Fu and
                  Elliot L. Epstein and
                  Eric Nguyen and
                  Armin W. Thomas and
                  Michael Zhang and
                  Tri Dao and
                  Atri Rudra and
                  Christopher R{\'{e}}},
  title        = {Simple Hardware-Efficient Long Convolutions for Sequence Modeling},
  journal      = {CoRR},
  volume       = {abs/2302.06646},
  year         = {2023},
  doi          = {10.48550/arXiv.2302.06646},
  eprinttype    = {arXiv},
  eprint       = {2302.06646},
  timestamp    = {Mon, 20 Feb 2023 14:27:28 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2302-06646.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{onlstm,
  author       = {Yikang Shen and
                  Shawn Tan and
                  Alessandro Sordoni and
                  Aaron C. Courville},
  title        = {Ordered Neurons: Integrating Tree Structures into Recurrent Neural
                  Networks},
  booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019,
                  New Orleans, LA, USA, May 6-9, 2019},
  publisher    = {OpenReview.net},
  year         = {2019},
  url          = {https://openreview.net/forum?id=B1l6qiR5F7},
  timestamp    = {Thu, 25 Jul 2019 13:03:16 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/ShenTSC19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{rae-razavi-2020-transformers,
    title = "Do Transformers Need Deep Long-Range Memory?",
    author = "Rae, Jack  and
      Razavi, Ali",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.672",
    doi = "10.18653/v1/2020.acl-main.672",
    pages = "7524--7529",
    abstract = "Deep attention models have advanced the modelling of sequential data across many domains. For language modelling in particular, the Transformer-XL {---} a Transformer augmented with a long-range memory of past activations {---} has been shown to be state-of-the-art across a variety of well-studied benchmarks. The Transformer-XL incorporates a long-range memory at every layer of the network, which renders its state to be thousands of times larger than RNN predecessors. However it is unclear whether this is necessary. We perform a set of interventions to show that comparable performance can be obtained with 6X fewer long range memories and better performance can be obtained by limiting the range of attention in lower layers of the network.",
}


@inproceedings{swin,
  author       = {Ze Liu and
                  Yutong Lin and
                  Yue Cao and
                  Han Hu and
                  Yixuan Wei and
                  Zheng Zhang and
                  Stephen Lin and
                  Baining Guo},
  title        = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  booktitle    = {2021 {IEEE/CVF} International Conference on Computer Vision, {ICCV}
                  2021, Montreal, QC, Canada, October 10-17, 2021},
  pages        = {9992--10002},
  publisher    = {{IEEE}},
  year         = {2021},
  url          = {https://doi.org/10.1109/ICCV48922.2021.00986},
  doi          = {10.1109/ICCV48922.2021.00986},
  timestamp    = {Thu, 19 May 2022 16:00:58 +0200},
  biburl       = {https://dblp.org/rec/conf/iccv/LiuL00W0LG21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lee-thorp-etal-2022-fnet,
    title = "{FN}et: Mixing Tokens with {F}ourier Transforms",
    author = "Lee-Thorp, James  and
      Ainslie, Joshua  and
      Eckstein, Ilya  and
      Ontanon, Santiago",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.319",
    doi = "10.18653/v1/2022.naacl-main.319",
    pages = "4296--4313",
    abstract = "We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that {``}mix{''} input tokens. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97{\%} of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80{\%} faster on GPUs and 70{\%} faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the {``}efficient Transformers{''} on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.",
}

@inproceedings{rao2021global,
  title={Global Filter Networks for Image Classification},
  author={Rao, Yongming and Zhao, Wenliang and Zhu, Zheng and Lu, Jiwen and Zhou, Jie},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2021}
}

@inproceedings{guibas2021efficient,
  title={Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators},
  author={Guibas, John and Mardani, Morteza and Li, Zongyi and Tao, Andrew and Anandkumar, Anima and Catanzaro, Bryan},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{qin2023exploring,
      title={Exploring Transformer Extrapolation}, 
      author={Zhen Qin and Yiran Zhong and Hui Deng},
      booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
      year={2024}
}

@inproceedings{qin2023accelerating,
    title = "Accelerating Toeplitz Neural Network with Constant-time Inference Complexity",
    author = "Qin, Zhen  and
      Zhong, Yiran",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    publisher = "Association for Computational Linguistics",
}

@ARTICLE{10149455,
  author={Sun, Weixuan and Qin, Zhen and Deng, Hui and Wang, Jianyuan and Zhang, Yi and Zhang, Kaihao and Barnes, Nick and Birchfield, Stan and Kong, Lingpeng and Zhong, Yiran},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Vicinity Vision Transformer}, 
  year={2023},
  volume={45},
  number={10},
  pages={12635-12649},
  doi={10.1109/TPAMI.2023.3285569}}


@misc{zhou2023audiovisual,
      title={Audio-Visual Segmentation with Semantics}, 
      author={Jinxing Zhou and Xuyang Shen and Jianyuan Wang and Jiayi Zhang and Weixuan Sun and Jing Zhang and Stan Birchfield and Dan Guo and Lingpeng Kong and Meng Wang and Yiran Zhong},
      year={2023},
      eprint={2301.13190},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{Shen_2023_CVPR,
    author    = {Shen, Xuyang and Li, Dong and Zhou, Jinxing and Qin, Zhen and He, Bowen and Han, Xiaodong and Li, Aixuan and Dai, Yuchao and Kong, Lingpeng and Wang, Meng and Qiao, Yu and Zhong, Yiran},
    title     = {Fine-Grained Audible Video Description},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {10585-10596}
}

@misc{lu2022linear,
      title={Linear Video Transformer with Feature Fixation}, 
      author={Kaiyue Lu and Zexiang Liu and Jianyuan Wang and Weixuan Sun and Zhen Qin and Dong Li and Xuyang Shen and Hui Deng and Xiaodong Han and Yuchao Dai and Yiran Zhong},
      year={2022},
      eprint={2210.08164},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{Mao_2023_ICCV,
    author    = {Mao, Yuxin and Zhang, Jing and Xiang, Mochu and Zhong, Yiran and Dai, Yuchao},
    title     = {Multimodal Variational Auto-encoder based Audio-Visual Segmentation},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {954-965}
}

@article{qin2023scaling,
  title={Scaling transnormer to 175 billion parameters},
  author={Qin, Zhen and Li, Dong and Sun, Weigao and Sun, Weixuan and Shen, Xuyang and Han, Xiaodong and Wei, Yunshen and Lv, Baohong and Yuan, Fei and Luo, Xiao and others},
  journal={arXiv preprint arXiv:2307.14995},
  year={2023}
}

@inproceedings{gu2022efficiently,
  title={Efficiently Modeling Long Sequences with Structured State Spaces},
  author={Gu, Albert and Goel, Karan and R\'e, Christopher},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2022}
}

@misc{dss,
Author = {Ankit Gupta and Albert Gu and Jonathan Berant},
Title = {Diagonal State Spaces are as Effective as Structured State Spaces},
Year = {2022},
Eprint = {arXiv:2203.14343},
}

@inproceedings{tay2020long,
  title={Long Range Arena: A Benchmark for Efficient Transformers},
  author={Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}


@inproceedings{Skyformer,
    title={Skyformer: Remodel Self-Attention with Gaussian Kernel and Nystr\"om Method}, 
    author={Yifan Chen and 
            Qi Zeng and 
            Heng Ji and 
            Yun Yang},
    booktitle={Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December
               6-14, 2021, virtual},
    year={2021}
}

@inproceedings{DBLP:conf/iclr/LiHEL21,
  author       = {Zhong Li and
                  Jiequn Han and
                  Weinan E and
                  Qianxiao Li},
  title        = {On the Curse of Memory in Recurrent Neural Networks: Approximation
                  and Optimization Analysis},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=8Sqhl-nF50},
  timestamp    = {Wed, 23 Jun 2021 17:36:40 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/LiHEL21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{indrnn,
  author       = {Shuai Li and
                  Wanqing Li and
                  Chris Cook and
                  Ce Zhu and
                  Yanbo Gao},
  title        = {Independently Recurrent Neural Network (IndRNN): Building a Longer
                  and Deeper {RNN}},
  booktitle    = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition,
                  {CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018},
  pages        = {5457--5466},
  publisher    = {Computer Vision Foundation / {IEEE} Computer Society},
  year         = {2018},
  url          = {http://openaccess.thecvf.com/content\_cvpr\_2018/html/Li\_Independently\_Recurrent\_Neural\_CVPR\_2018\_paper.html},
  doi          = {10.1109/CVPR.2018.00572},
  timestamp    = {Fri, 24 Mar 2023 00:02:54 +0100},
  biburl       = {https://dblp.org/rec/conf/cvpr/0005LCZG18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{
huang2023encoding,
title={Encoding Recurrence into Transformers},
author={Feiqing Huang and Kexin Lu and Yuxi CAI and Zhen Qin and Yanwen Fang and Guangjian Tian and Guodong Li},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=7YfHla7IxBJ}
}

@inproceedings{linearxfmrsparallelscan,
  author       = {Valerii Likhosherstov and
                  Krzysztof Marcin Choromanski and
                  Jared Quincy Davis and
                  Xingyou Song and
                  Adrian Weller},
  editor       = {Marc'Aurelio Ranzato and
                  Alina Beygelzimer and
                  Yann N. Dauphin and
                  Percy Liang and
                  Jennifer Wortman Vaughan},
  title        = {Sub-Linear Memory: How to Make Performers SLiM},
  booktitle    = {Advances in Neural Information Processing Systems 34: Annual Conference
                  on Neural Information Processing Systems 2021, NeurIPS 2021, December
                  6-14, 2021, virtual},
  pages        = {6707--6719},
  year         = {2021},
  url          = {https://proceedings.neurips.cc/paper/2021/hash/35309226eb45ec366ca86a4329a2b7c3-Abstract.html},
  timestamp    = {Tue, 03 May 2022 16:20:47 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/LikhosherstovCD21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{xfmrsarernns,
  author       = {Angelos Katharopoulos and
                  Apoorv Vyas and
                  Nikolaos Pappas and
                  Fran{\c{c}}ois Fleuret},
  title        = {Transformers are RNNs: Fast Autoregressive Transformers with Linear
                  Attention},
  booktitle    = {Proceedings of the 37th International Conference on Machine Learning,
                  {ICML} 2020, 13-18 July 2020, Virtual Event},
  series       = {Proceedings of Machine Learning Research},
  volume       = {119},
  pages        = {5156--5165},
  publisher    = {{PMLR}},
  year         = {2020},
  url          = {http://proceedings.mlr.press/v119/katharopoulos20a.html},
  timestamp    = {Tue, 15 Dec 2020 17:40:19 +0100},
  biburl       = {https://dblp.org/rec/conf/icml/KatharopoulosV020.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{pmlr-v139-touvron21a,
  title =     {Training data-efficient image transformers \&amp; distillation through attention},
  author =    {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
  booktitle = {International Conference on Machine Learning},
  pages =     {10347--10357},
  year =      {2021},
  volume =    {139},
  month =     {July}
}

@inproceedings{DBLP:conf/iclr/MahtoVTH21,
  author       = {Shivangi Mahto and
                  Vy Ai Vo and
                  Javier S. Turek and
                  Alexander Huth},
  title        = {Multi-timescale Representation Learning in {LSTM} Language Models},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=9ITXiTrAoT},
  timestamp    = {Wed, 23 Jun 2021 17:36:40 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/MahtoVTH21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/NeilPL16,
  author       = {Daniel Neil and
                  Michael Pfeiffer and
                  Shih{-}Chii Liu},
  editor       = {Daniel D. Lee and
                  Masashi Sugiyama and
                  Ulrike von Luxburg and
                  Isabelle Guyon and
                  Roman Garnett},
  title        = {Phased {LSTM:} Accelerating Recurrent Network Training for Long or
                  Event-based Sequences},
  booktitle    = {Advances in Neural Information Processing Systems 29: Annual Conference
                  on Neural Information Processing Systems 2016, December 5-10, 2016,
                  Barcelona, Spain},
  pages        = {3882--3890},
  year         = {2016},
  url          = {https://proceedings.neurips.cc/paper/2016/hash/5bce843dd76db8c939d5323dd3e54ec9-Abstract.html},
  timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/NeilPL16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/iclr/TallecO18a,
  author       = {Corentin Tallec and
                  Yann Ollivier},
  title        = {Can recurrent neural networks warp time?},
  booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018,
                  Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher    = {OpenReview.net},
  year         = {2018},
  url          = {https://openreview.net/forum?id=SJcKhk-Ab},
  timestamp    = {Thu, 25 Jul 2019 14:25:42 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/TallecO18a.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{kim-2014-convolutional,
    title = "Convolutional Neural Networks for Sentence Classification",
    author = "Kim, Yoon",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1181",
    doi = "10.3115/v1/D14-1181",
    pages = "1746--1751",
}

@article{forgetgate,
  author       = {Jos van der Westhuizen and
                  Joan Lasenby},
  title        = {The unreasonable effectiveness of the forget gate},
  journal      = {CoRR},
  volume       = {abs/1804.04849},
  year         = {2018},
  eprinttype    = {arXiv},
  eprint       = {1804.04849},
  timestamp    = {Mon, 13 Aug 2018 16:49:00 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1804-04849.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{gray2006toeplitz,
  title={Toeplitz and circulant matrices: A review},
  author={Gray, Robert M and others},
  journal={Foundations and Trends{\textregistered} in Communications and Information Theory},
  volume={2},
  number={3},
  pages={155--239},
  year={2006},
  publisher={Now Publishers, Inc.}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer.},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
  journal={J. Mach. Learn. Res.},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@inproceedings{yu2022metaformer,
  title={Metaformer is actually what you need for vision},
  author={Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10819--10829},
  year={2022}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{
alibi,
title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
author={Ofir Press and Noah Smith and Mike Lewis},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=R8sQPpGCv0}
}

@misc{chi2022kerple,
      title={KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation}, 
      author={Ta-Chung Chi and Ting-Han Fan and Peter J. Ramadge and Alexander I. Rudnicky},
      year={2022},
      eprint={2205.09921},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chi2023dissecting,
      title={Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis}, 
      author={Ta-Chung Chi and Ting-Han Fan and Alexander I. Rudnicky and Peter J. Ramadge},
      year={2023},
      eprint={2212.10356},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{titsias2016one,
  title={One-vs-each approximation to softmax for scalable estimation of probabilities},
  author={Titsias, Michalis K},
  journal={arXiv preprint arXiv:1609.07410},
  year={2016}
}

@article{jang2016categorical,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal={arXiv preprint arXiv:1611.01144},
  year={2016}
}

@article{parmar2024reuse,
  title={Reuse, Don't Retrain: A Recipe for Continued Pretraining of Language Models},
  author={Parmar, Jupinder and Satheesh, Sanjev and Patwary, Mostofa and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2407.07263},
  year={2024}
}

@inproceedings{goldman2024really,
    title = "Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context {NLP}",
    author = "Goldman, Omer  and
      Jacovi, Alon  and
      Slobodkin, Aviv  and
      Maimon, Aviya  and
      Dagan, Ido  and
      Tsarfaty, Reut",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.924/",
    doi = "10.18653/v1/2024.emnlp-main.924",
    pages = "16576--16586",
    abstract = "Improvements in language models' capabilities have pushed their applications towards longer contexts, making long-context evaluation and development an active research area. However, many disparate use-cases are grouped together under the umbrella term of {\textquotedblleft}long-context{\textquotedblright}, defined simply by the total length of the model`s input, including - for example - Needle-in-a-Haystack tasks, book summarization, and information aggregation. Given their varied difficulty, in this position paper we argue that conflating different tasks by their context length is unproductive. As a community, we require a more precise vocabulary to understand what makes long-context tasks similar or different. We propose to unpack the taxonomy of long-context based on the properties that make them more difficult with longer contexts. We propose two orthogonal axes of difficulty: (I) Diffusion: How hard is it to find the necessary information in the context? (II) Scope: How much necessary information is there to find? We survey the literature on long-context, provide justification for this taxonomy as an informative descriptor, and situate the literature with respect to it. We conclude that the most difficult and interesting settings, whose necessary information is very long and highly diffused within the input, is severely under-explored. By using a descriptive vocabulary and discussing the relevant properties of difficulty in long-context, we can implement more informed research in this area. We call for a careful design of tasks and benchmarks with distinctly long context, taking into account the characteristics that make it qualitatively different from shorter context."
}


@inproceedings{dong2022survey,
  title={A survey on in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Ma, Jingyuan and Li, Rui and Xia, Heming and Xu, Jingjing and Wu, Zhiyong and Chang, Baobao and others},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={1107--1128},
  year={2024}
}


@article{agarwal2024many,
  title={Many-shot in-context learning},
  author={Agarwal, Rishabh and Singh, Avi and Zhang, Lei M and Bohnet, Bernd and Rosias, Luis and Chan, Stephanie and Zhang, Biao and Anand, Ankesh and Abbas, Zaheer and Nova, Azade and others},
  journal={arXiv preprint arXiv:2404.11018},
  year={2024}
}


@article{hsieh2024ruler,
  title={{RULER}: What's the Real Context Size of Your Long-Context Language Models?},
  author={Hsieh, Cheng-Ping and Sun, Simeng and Kriman, Samuel and Acharya, Shantanu and Rekesh, Dima and Jia, Fei and Zhang, Yang and Ginsburg, Boris},
  journal={arXiv preprint arXiv:2404.06654},
  year={2024}
}


@article{gao2017properties,
  title={On the properties of the softmax function with application in game theory and reinforcement learning},
  author={Gao, Bolin and Pavel, Lacra},
  journal={arXiv preprint arXiv:1704.00805},
  year={2017}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{lecun1989backpropagation,
  title={Backpropagation applied to handwritten zip code recognition},
  author={LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  journal={Neural computation},
  volume={1},
  number={4},
  pages={541--551},
  year={1989},
  publisher={MIT Press}
}

@article{tolstikhin2021mlp,
  title={Mlp-mixer: An all-mlp architecture for vision},
  author={Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24261--24272},
  year={2021}
}

@article{Laurent2016ARN,
  title={A recurrent neural network without chaos},
  author={Thomas Laurent and James H. von Brecht},
  journal={ArXiv},
  year={2016},
  volume={abs/1612.06212}
}

@article{minimalrnn,
  author       = {Minmin Chen},
  title        = {MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural
                  Networks},
  journal      = {CoRR},
  volume       = {abs/1711.06788},
  year         = {2017},
  eprinttype    = {arXiv},
  eprint       = {1711.06788},
  timestamp    = {Mon, 13 Aug 2018 16:46:27 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1711-06788.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{shazeer2020glu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@article{mildenhall2021nerf,
  title={Nerf: Representing scenes as neural radiance fields for view synthesis},
  author={Mildenhall, Ben and Srinivasan, Pratul P and Tancik, Matthew and Barron, Jonathan T and Ramamoorthi, Ravi and Ng, Ren},
  journal={Communications of the ACM},
  volume={65},
  number={1},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{mehta2022long,
  title={Long range language modeling via gated state spaces},
  author={Mehta, Harsh and Gupta, Ankit and Cutkosky, Ashok and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2206.13947},
  year={2022}
}



@inproceedings{salman2015weather,
  title={Weather forecasting using deep learning techniques},
  author={Salman, Afan Galih and Kanigoro, Bayu and Heryadi, Yaya},
  booktitle={2015 international conference on advanced computer science and information systems (ICACSIS)},
  pages={281--285},
  year={2015},
  organization={Ieee}
}

@inproceedings{selvin2017stock,
  title={Stock price prediction using LSTM, RNN and CNN-sliding window model},
  author={Selvin, Sreelekshmy and Vinayakumar, R and Gopalakrishnan, EA and Menon, Vijay Krishna and Soman, KP},
  booktitle={2017 international conference on advances in computing, communications and informatics (icacci)},
  pages={1643--1647},
  year={2017},
  organization={IEEE}
}

@inproceedings{miao2015eesen,
  title={EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding},
  author={Miao, Yajie and Gowayyed, Mohammad and Metze, Florian},
  booktitle={2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)},
  pages={167--174},
  year={2015},
  organization={IEEE}
}

@inproceedings{
martin2018parallelizing,
title={Parallelizing Linear Recurrent Neural Nets Over Sequence Length},
author={Eric Martin and Chris Cundy},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=HyUNwulC-},
}

@inproceedings{gong21b_interspeech,
  author={Yuan Gong and Yu-An Chung and James Glass},
  title={{AST: Audio Spectrogram Transformer}},
  year=2021,
  booktitle={Proc. Interspeech 2021},
  pages={571--575},
  doi={10.21437/Interspeech.2021-698}
}

@article{akbari2021vatt,
  title={Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text},
  author={Akbari, Hassan and Yuan, Liangzhe and Qian, Rui and Chuang, Wei-Hong and Chang, Shih-Fu and Cui, Yin and Gong, Boqing},
  journal={arXiv preprint arXiv:2104.11178},
  year={2021}
}

@misc{lewis2019bart,
      title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}, 
      author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
      year={2019},
      eprint={1910.13461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{rae2022scaling,
      title={Scaling Language Models: Methods, Analysis \& Insights from Training Gopher}, 
      author={Jack W. Rae and Sebastian Borgeaud and Trevor Cai and Katie Millican and Jordan Hoffmann and Francis Song and John Aslanides and Sarah Henderson and Roman Ring and Susannah Young and Eliza Rutherford and Tom Hennigan and Jacob Menick and Albin Cassirer and Richard Powell and George van den Driessche and Lisa Anne Hendricks and Maribeth Rauh and Po-Sen Huang and Amelia Glaese and Johannes Welbl and Sumanth Dathathri and Saffron Huang and Jonathan Uesato and John Mellor and Irina Higgins and Antonia Creswell and Nat McAleese and Amy Wu and Erich Elsen and Siddhant Jayakumar and Elena Buchatskaya and David Budden and Esme Sutherland and Karen Simonyan and Michela Paganini and Laurent Sifre and Lena Martens and Xiang Lorraine Li and Adhiguna Kuncoro and Aida Nematzadeh and Elena Gribovskaya and Domenic Donato and Angeliki Lazaridou and Arthur Mensch and Jean-Baptiste Lespiau and Maria Tsimpoukelli and Nikolai Grigorev and Doug Fritz and Thibault Sottiaux and Mantas Pajarskas and Toby Pohlen and Zhitao Gong and Daniel Toyama and Cyprien de Masson d'Autume and Yujia Li and Tayfun Terzi and Vladimir Mikulik and Igor Babuschkin and Aidan Clark and Diego de Las Casas and Aurelia Guy and Chris Jones and James Bradbury and Matthew Johnson and Blake Hechtman and Laura Weidinger and Iason Gabriel and William Isaac and Ed Lockhart and Simon Osindero and Laura Rimell and Chris Dyer and Oriol Vinyals and Kareem Ayoub and Jeff Stanway and Lorrayne Bennett and Demis Hassabis and Koray Kavukcuoglu and Geoffrey Irving},
      year={2022},
      eprint={2112.11446},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{du2022glm,
      title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling}, 
      author={Zhengxiao Du and Yujie Qian and Xiao Liu and Ming Ding and Jiezhong Qiu and Zhilin Yang and Jie Tang},
      year={2022},
      eprint={2103.10360},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{taylor2022galactica,
      title={Galactica: A Large Language Model for Science}, 
      author={Ross Taylor and Marcin Kardas and Guillem Cucurull and Thomas Scialom and Anthony Hartshorn and Elvis Saravia and Andrew Poulton and Viktor Kerkez and Robert Stojnic},
      year={2022},
      eprint={2211.09085},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{li-etal-2023-map,
    title = "{MAP}: Low-data Regime Multimodal Learning with Adapter-based Pre-training and Prompting",
    author = "Li, Wenyan  and
      Li, Dong  and
      Li, Wanjing  and
      Wang, Yuanjie  and
      Jie, Hai  and
      Zhong, Yiran",
    editor = "Breitholtz, Ellen  and
      Lappin, Shalom  and
      Loaiciga, Sharid  and
      Ilinykh, Nikolai  and
      Dobnik, Simon",
    booktitle = "Proceedings of the 2023 CLASP Conference on Learning with Small Data (LSD)",
    month = sep,
    year = "2023",
    address = "Gothenburg, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.clasp-1.19",
    pages = "185--190",
    abstract = "Pretrained vision-language (VL) models have shown impressive results on various multi-modal downstream tasks recently. Many of the benchmark models build on pretrained causal language models (LMs), leveraging the original few-shot learning and generalization capability of the LMs trained with large text corpora. However, these models are often gigantic and require large-scale image and text data with high computational cost to train. This paper introduces a moderate-size model called MAP for efficient VL transfer learning through adapter-based pretraining and prompting. We aim to answer the question of how much we can complete through VL pretraining within the low-data regime while maximizing efficiency in transferring knowledge of a moderate-size frozen LM. Our experiments demonstrate that MAP achieves substantially better zero-shot and few-shot performance on downstream VL tasks with only 10{\%} the size of pretraining data and a 30x lighter pretrained LM backbone compared to Frozen. MAP also outperforms fully trained models of comparable size at retaining its transfer learning ability when the amount of training data reduces.",
}


@misc{zhang2022opt,
      title={OPT: Open Pre-trained Transformer Language Models}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{workshop2023bloom,
      title={BLOOM: A 176B-Parameter Open-Access Multilingual Language Model}, 
      author={BigScience Workshop and : and Teven Le Scao and Angela Fan and Christopher Akiki and Ellie Pavlick and Suzana Ilić and Daniel Hesslow and Roman Castagné and Alexandra Sasha Luccioni and François Yvon and Matthias Gallé and Jonathan Tow and Alexander M. Rush and Stella Biderman and Albert Webson and Pawan Sasanka Ammanamanchi and Thomas Wang and Benoît Sagot and Niklas Muennighoff and Albert Villanova del Moral and Olatunji Ruwase and Rachel Bawden and Stas Bekman and Angelina McMillan-Major and Iz Beltagy and Huu Nguyen and Lucile Saulnier and Samson Tan and Pedro Ortiz Suarez and Victor Sanh and Hugo Laurençon and Yacine Jernite and Julien Launay and Margaret Mitchell and Colin Raffel and Aaron Gokaslan and Adi Simhi and Aitor Soroa and Alham Fikri Aji and Amit Alfassy and Anna Rogers and Ariel Kreisberg Nitzav and Canwen Xu and Chenghao Mou and Chris Emezue and Christopher Klamm and Colin Leong and Daniel van Strien and David Ifeoluwa Adelani and Dragomir Radev and Eduardo González Ponferrada and Efrat Levkovizh and Ethan Kim and Eyal Bar Natan and Francesco De Toni and Gérard Dupont and Germán Kruszewski and Giada Pistilli and Hady Elsahar and Hamza Benyamina and Hieu Tran and Ian Yu and Idris Abdulmumin and Isaac Johnson and Itziar Gonzalez-Dios and Javier de la Rosa and Jenny Chim and Jesse Dodge and Jian Zhu and Jonathan Chang and Jörg Frohberg and Joseph Tobing and Joydeep Bhattacharjee and Khalid Almubarak and Kimbo Chen and Kyle Lo and Leandro Von Werra and Leon Weber and Long Phan and Loubna Ben allal and Ludovic Tanguy and Manan Dey and Manuel Romero Muñoz and Maraim Masoud and María Grandury and Mario Šaško and Max Huang and Maximin Coavoux and Mayank Singh and Mike Tian-Jian Jiang and Minh Chien Vu and Mohammad A. Jauhar and Mustafa Ghaleb and Nishant Subramani and Nora Kassner and Nurulaqilla Khamis and Olivier Nguyen and Omar Espejel and Ona de Gibert and Paulo Villegas and Peter Henderson and Pierre Colombo and Priscilla Amuok and Quentin Lhoest and Rheza Harliman and Rishi Bommasani and Roberto Luis López and Rui Ribeiro and Salomey Osei and Sampo Pyysalo and Sebastian Nagel and Shamik Bose and Shamsuddeen Hassan Muhammad and Shanya Sharma and Shayne Longpre and Somaieh Nikpoor and Stanislav Silberberg and Suhas Pai and Sydney Zink and Tiago Timponi Torrent and Timo Schick and Tristan Thrush and Valentin Danchev and Vassilina Nikoulina and Veronika Laippala and Violette Lepercq and Vrinda Prabhu and Zaid Alyafeai and Zeerak Talat and Arun Raja and Benjamin Heinzerling and Chenglei Si and Davut Emre Taşar and Elizabeth Salesky and Sabrina J. Mielke and Wilson Y. Lee and Abheesht Sharma and Andrea Santilli and Antoine Chaffin and Arnaud Stiegler and Debajyoti Datta and Eliza Szczechla and Gunjan Chhablani and Han Wang and Harshit Pandey and Hendrik Strobelt and Jason Alan Fries and Jos Rozen and Leo Gao and Lintang Sutawika and M Saiful Bari and Maged S. Al-shaibani and Matteo Manica and Nihal Nayak and Ryan Teehan and Samuel Albanie and Sheng Shen and Srulik Ben-David and Stephen H. Bach and Taewoon Kim and Tali Bers and Thibault Fevry and Trishala Neeraj and Urmish Thakker and Vikas Raunak and Xiangru Tang and Zheng-Xin Yong and Zhiqing Sun and Shaked Brody and Yallow Uri and Hadar Tojarieh and Adam Roberts and Hyung Won Chung and Jaesung Tae and Jason Phang and Ofir Press and Conglong Li and Deepak Narayanan and Hatim Bourfoune and Jared Casper and Jeff Rasley and Max Ryabinin and Mayank Mishra and Minjia Zhang and Mohammad Shoeybi and Myriam Peyrounette and Nicolas Patry and Nouamane Tazi and Omar Sanseviero and Patrick von Platen and Pierre Cornette and Pierre François Lavallée and Rémi Lacroix and Samyam Rajbhandari and Sanchit Gandhi and Shaden Smith and Stéphane Requena and Suraj Patil and Tim Dettmers and Ahmed Baruwa and Amanpreet Singh and Anastasia Cheveleva and Anne-Laure Ligozat and Arjun Subramonian and Aurélie Névéol and Charles Lovering and Dan Garrette and Deepak Tunuguntla and Ehud Reiter and Ekaterina Taktasheva and Ekaterina Voloshina and Eli Bogdanov and Genta Indra Winata and Hailey Schoelkopf and Jan-Christoph Kalo and Jekaterina Novikova and Jessica Zosa Forde and Jordan Clive and Jungo Kasai and Ken Kawamura and Liam Hazan and Marine Carpuat and Miruna Clinciu and Najoung Kim and Newton Cheng and Oleg Serikov and Omer Antverg and Oskar van der Wal and Rui Zhang and Ruochen Zhang and Sebastian Gehrmann and Shachar Mirkin and Shani Pais and Tatiana Shavrina and Thomas Scialom and Tian Yun and Tomasz Limisiewicz and Verena Rieser and Vitaly Protasov and Vladislav Mikhailov and Yada Pruksachatkun and Yonatan Belinkov and Zachary Bamberger and Zdeněk Kasner and Alice Rueda and Amanda Pestana and Amir Feizpour and Ammar Khan and Amy Faranak and Ana Santos and Anthony Hevia and Antigona Unldreaj and Arash Aghagol and Arezoo Abdollahi and Aycha Tammour and Azadeh HajiHosseini and Bahareh Behroozi and Benjamin Ajibade and Bharat Saxena and Carlos Muñoz Ferrandis and Daniel McDuff and Danish Contractor and David Lansky and Davis David and Douwe Kiela and Duong A. Nguyen and Edward Tan and Emi Baylor and Ezinwanne Ozoani and Fatima Mirza and Frankline Ononiwu and Habib Rezanejad and Hessie Jones and Indrani Bhattacharya and Irene Solaiman and Irina Sedenko and Isar Nejadgholi and Jesse Passmore and Josh Seltzer and Julio Bonis Sanz and Livia Dutra and Mairon Samagaio and Maraim Elbadri and Margot Mieskes and Marissa Gerchick and Martha Akinlolu and Michael McKenna and Mike Qiu and Muhammed Ghauri and Mykola Burynok and Nafis Abrar and Nazneen Rajani and Nour Elkott and Nour Fahmy and Olanrewaju Samuel and Ran An and Rasmus Kromann and Ryan Hao and Samira Alizadeh and Sarmad Shubber and Silas Wang and Sourav Roy and Sylvain Viguier and Thanh Le and Tobi Oyebade and Trieu Le and Yoyo Yang and Zach Nguyen and Abhinav Ramesh Kashyap and Alfredo Palasciano and Alison Callahan and Anima Shukla and Antonio Miranda-Escalada and Ayush Singh and Benjamin Beilharz and Bo Wang and Caio Brito and Chenxi Zhou and Chirag Jain and Chuxin Xu and Clémentine Fourrier and Daniel León Periñán and Daniel Molano and Dian Yu and Enrique Manjavacas and Fabio Barth and Florian Fuhrimann and Gabriel Altay and Giyaseddin Bayrak and Gully Burns and Helena U. Vrabec and Imane Bello and Ishani Dash and Jihyun Kang and John Giorgi and Jonas Golde and Jose David Posada and Karthik Rangasai Sivaraman and Lokesh Bulchandani and Lu Liu and Luisa Shinzato and Madeleine Hahn de Bykhovetz and Maiko Takeuchi and Marc Pàmies and Maria A Castillo and Marianna Nezhurina and Mario Sänger and Matthias Samwald and Michael Cullan and Michael Weinberg and Michiel De Wolf and Mina Mihaljcic and Minna Liu and Moritz Freidank and Myungsun Kang and Natasha Seelam and Nathan Dahlberg and Nicholas Michio Broad and Nikolaus Muellner and Pascale Fung and Patrick Haller and Ramya Chandrasekhar and Renata Eisenberg and Robert Martin and Rodrigo Canalli and Rosaline Su and Ruisi Su and Samuel Cahyawijaya and Samuele Garda and Shlok S Deshmukh and Shubhanshu Mishra and Sid Kiblawi and Simon Ott and Sinee Sang-aroonsiri and Srishti Kumar and Stefan Schweter and Sushil Bharati and Tanmay Laud and Théo Gigant and Tomoya Kainuma and Wojciech Kusa and Yanis Labrak and Yash Shailesh Bajaj and Yash Venkatraman and Yifan Xu and Yingxin Xu and Yu Xu and Zhe Tan and Zhongli Xie and Zifan Ye and Mathilde Bras and Younes Belkada and Thomas Wolf},
      year={2023},
      eprint={2211.05100},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{biderman2023pythia,
      title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling}, 
      author={Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
      year={2023},
      eprint={2304.01373},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
liu2024ringattention,
title={Ring Attention with Blockwise Transformers for Near-Infinite Context},
author={Hao Liu and Matei Zaharia and Pieter Abbeel},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=WsRHpHH4s0}
}

@inproceedings{zheng2022linear,
  title={Linear complexity randomized self-attention mechanism},
  author={Lin Zheng and Chong Wang and Lingpeng Kong},
  booktitle={International Conference on Machine Learning},
  pages={27011--27041},
  year={2022},
  organization={PMLR}
}

@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{Tillet2019TritonAI,
  title={Triton: an intermediate language and compiler for tiled neural network computations},
  author={Philippe Tillet and Hsiang-Tsung Kung and David D. Cox},
  journal={Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
  year={2019}
}

@inproceedings{zheng2023efficient,
  title={Efficient Attention via Control Variates},
  author={Lin Zheng and Jianbo Yuan and Chong Wang and Lingpeng Kong},
  booktitle={International Conference on Learning Representations},
  year={2023},
  url={https://openreview.net/forum?id=G-uNfHKrj46}
}

@misc{1904.10509,
Author = {Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
Title = {Generating Long Sequences with Sparse Transformers},
Year = {2019},
Eprint = {arXiv:1904.10509},
}

@misc{2008.07669,
Author = {Albert Gu and Tri Dao and Stefano Ermon and Atri Rudra and Christopher Re},
Title = {HiPPO: Recurrent Memory with Optimal Polynomial Projections},
Year = {2020},
Eprint = {arXiv:2008.07669},
}

@inproceedings{
dao2023flashattention2,
title={{FlashAttention-2}: Faster Attention with Better Parallelism and Work Partitioning},
author={Tri Dao},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=mZn2Xyh9Ec}
}


@article{liu2022neural,
  title={Neural architecture search on efficient transformers and beyond},
  author={Liu, Zexiang and Li, Dong and Lu, Kaiyue and Qin, Zhen and Sun, Weixuan and Xu, Jiacheng and Zhong, Yiran},
  journal={arXiv preprint arXiv:2207.13955},
  year={2022}
}

@misc{huang2023ceval,
      title={C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models}, 
      author={Yuzhen Huang and Yuzhuo Bai and Zhihao Zhu and Junlei Zhang and Jinghan Zhang and Tangjun Su and Junteng Liu and Chuancheng Lv and Yikai Zhang and Jiayi Lei and Yao Fu and Maosong Sun and Junxian He},
      year={2023},
      eprint={2305.08322},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{mihaylov2018suit,
  title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={2381--2391},
  year={2018}
}

@article{clark2018think,
  title={Think you have solved question answering? Try {ARC}, the {AI2} reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@inproceedings{clark2019boolq,
  title={{BoolQ}: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={2924--2936},
  year={2019}
}

@inproceedings{zellers2019hellaswag,
  title={Hella{S}wag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4791--4800},
  year={2019}
}

@article{sakaguchi2019winogrande,
  title={Wino{G}rande: An adversarial {W}inograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{sap2019social,
  title={Social {IQ}a: Commonsense Reasoning about Social Interactions},
  author={Sap, Maarten and Rashkin, Hannah and Chen, Derek and Le Bras, Ronan and Choi, Yejin},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={4463--4473},
  year={2019}
}

@article{leo2021evalharness,
  title={A framework for few-shot language model evaluation},
  author={Gao, Leo and Tow, Jonathan and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and McDonell, Kyle and Muennighoff, Niklas and others},
  journal={Version v0. 0.1. Sept},
  year={2021}
}

@article{gpt-neo,
  title={Gpt-neox-20b: An open-source autoregressive language model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  journal={arXiv preprint arXiv:2204.06745},
  year={2022}
}

@misc{wang2021gpt,
  title={GPT-J-6B: A 6 billion parameter autoregressive language model},
  author={Wang, Ben and Komatsuzaki, Aran},
  year={2021}
}


@article{mpt-7b,
  title={Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023},
  author={MosaicML NLP Team and others},
  journal={URL www. mosaicml. com/blog/mpt-7b. Accessed},
  pages={05--05},
  year={2023}
}

@article{falcon40b,
  title={Falcon-40B: an open large language model with state-of-the-art performance},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and others},
  year={2023},
  journal={Technical report, Technology Innovation Institute}
}

@misc{2307.09288,
Author = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
Title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
Year = {2023},
Eprint = {arXiv:2307.09288},
}

@article{openlm2023openllama,
  title={Openllama: An open reproduction of llama},
  author={Geng, Xinyang and Liu, Hao},
  journal={URL: https://github. com/openlm-research/open\_llama},
  year={2023}
}

@article{baichuan2023baichuan2,
  title={Baichuan 2: Open Large-scale Language Models},
  author={Baichuan},
  journal={arXiv preprint arXiv:2309.10305},
  year={2023}
}

@article{zeng2022glm,
  title={Glm-130b: An open bilingual pre-trained model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2210.02414},
  year={2022}
}


@misc{2305.13048,
Author = {Bo Peng and Eric Alcaide and Quentin Anthony and Alon Albalak and Samuel Arcadinho and Huanqi Cao and Xin Cheng and Michael Chung and Matteo Grella and Kranthi Kiran GV and Xuzheng He and Haowen Hou and Przemyslaw Kazienko and Jan Kocon and Jiaming Kong and Bartlomiej Koptyra and Hayden Lau and Krishna Sri Ipsit Mantri and Ferdinand Mom and Atsushi Saito and Xiangru Tang and Bolun Wang and Johan S. Wind and Stansilaw Wozniak and Ruichong Zhang and Zhenyuan Zhang and Qihang Zhao and Peng Zhou and Jian Zhu and Rui-Jie Zhu},
Title = {RWKV: Reinventing RNNs for the Transformer Era},
Year = {2023},
Eprint = {arXiv:2305.13048},
}

@misc{2303.06349,
Author = {Antonio Orvieto and Samuel L Smith and Albert Gu and Anushan Fernando and Caglar Gulcehre and Razvan Pascanu and Soham De},
Title = {Resurrecting Recurrent Neural Networks for Long Sequences},
Year = {2023},
Eprint = {arXiv:2303.06349},
}

@misc{1710.05941,
Author = {Prajit Ramachandran and Barret Zoph and Quoc V. Le},
Title = {Searching for Activation Functions},
Year = {2017},
Eprint = {arXiv:1710.05941},
}

@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 12,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.10256836},
  url          = {https://zenodo.org/records/10256836}
}

@inproceedings{peng-etal-2023-rwkv,
    title = "{RWKV}: Reinventing {RNN}s for the Transformer Era",
    author = "Peng, Bo  and
      Alcaide, Eric  and
      Anthony, Quentin  and
      Albalak, Alon  and
      Arcadinho, Samuel  and
      Biderman, Stella  and
      Cao, Huanqi  and
      Cheng, Xin  and
      Chung, Michael  and
      Derczynski, Leon  and
      Du, Xingjian  and
      Grella, Matteo  and
      Gv, Kranthi  and
      He, Xuzheng  and
      Hou, Haowen  and
      Kazienko, Przemyslaw  and
      Kocon, Jan  and
      Kong, Jiaming  and
      Koptyra, Bart{\l}omiej  and
      Lau, Hayden  and
      Lin, Jiaju  and
      Mantri, Krishna Sri Ipsit  and
      Mom, Ferdinand  and
      Saito, Atsushi  and
      Song, Guangyu  and
      Tang, Xiangru  and
      Wind, Johan  and
      Wo{\'z}niak, Stanis{\l}aw  and
      Zhang, Zhenyuan  and
      Zhou, Qinghua  and
      Zhu, Jian  and
      Zhu, Rui-Jie",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.936",
    doi = "10.18653/v1/2023.findings-emnlp.936",
    pages = "14048--14077",
    abstract = "Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.",
}

@misc{2307.08621,
Author = {Yutao Sun and Li Dong and Shaohan Huang and Shuming Ma and Yuqing Xia and Jilong Xue and Jianyong Wang and Furu Wei},
Title = {Retentive Network: A Successor to Transformer for Large Language Models},
Year = {2023},
Eprint = {arXiv:2307.08621},
}

@misc{yang2024gated,
      title={Gated Linear Attention Transformers with Hardware-Efficient Training}, 
      author={Songlin Yang and Bailin Wang and Yikang Shen and Rameswar Panda and Yoon Kim},
      year={2024},
      eprint={2312.06635},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{jiang2023mistral,
  title={Mistral 7{B}},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@misc{bahdanau2016neural,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{mao2022fine,
  title={Fine-tuning pre-trained transformers into decaying fast weights},
  author={Mao, Huanru Henry},
  journal={arXiv preprint arXiv:2210.04243},
  year={2022}
}

@inproceedings{schlag2017gated,
  title={Gated fast weights for on-the-fly neural program generation},
  author={Schlag, Imanol and Schmidhuber, J{\"u}rgen},
  booktitle={NIPS Metalearning Workshop},
  year={2017}
}

@article{de2016cheap,
  title={A cheap linear attention mechanism with fast lookups and fixed-size representations},
  author={de Br{\'e}bisson, Alexandre and Vincent, Pascal},
  journal={arXiv preprint arXiv:1609.05866},
  year={2016}
}


@article{GLA,
  author       = {Songlin Yang and
                  Bailin Wang and
                  Yikang Shen and
                  Rameswar Panda and
                  Yoon Kim},
  title        = {Gated Linear Attention Transformers with Hardware-Efficient Training},
  journal      = {CoRR},
  volume       = {abs/2312.06635},
  year         = {2023},
  doi          = {10.48550/ARXIV.2312.06635},
  eprinttype    = {arXiv},
  eprint       = {2312.06635},
  timestamp    = {Thu, 04 Jan 2024 15:12:49 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2312-06635.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{retnet,
  title={Retentive network: A successor to transformer for large language models},
  author={Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
  journal={arXiv preprint arXiv:2307.08621},
  year={2023}
}

@misc{2312.00752,
Author = {Albert Gu and Tri Dao},
Title = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
Year = {2023},
Eprint = {arXiv:2312.00752},
}

@misc{
katsch2024gateloop,
title={GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling},
author={Tobias Katsch},
year={2024},
url={https://openreview.net/forum?id=02Ug9N8DCI}
}

@misc{
schlag2018gated,
title={{GATED} {FAST} {WEIGHTS} {FOR} {ASSOCIATIVE} {RETRIEVAL}},
author={Imanol Schlag and Jürgen Schmidhuber},
year={2018},
url={https://openreview.net/forum?id=HJ8W1Q-0Z},
}

@misc{2210.04243,
Author = {Huanru Henry Mao},
Title = {Fine-Tuning Pre-trained Transformers into Decaying Fast Weights},
Year = {2022},
Eprint = {arXiv:2210.04243},
}


@inproceedings{qin2023hierarchically,
  title={Hierarchically gated recurrent neural network for sequence modeling},
  author={Qin, Zhen and Yang, Songlin and Zhong, Yiran},
  booktitle={Proceedings of the 37th International Conference on Neural Information Processing Systems},
  pages={33202--33221},
  year={2023}
}


@article{zoology,
  author       = {Simran Arora and
                  Sabri Eyuboglu and
                  Aman Timalsina and
                  Isys Johnson and
                  Michael Poli and
                  James Zou and
                  Atri Rudra and
                  Christopher R{\'{e}}},
  title        = {Zoology: Measuring and Improving Recall in Efficient Language Models},
  journal      = {CoRR},
  volume       = {abs/2312.04927},
  year         = {2023},
}

@misc{2102.11174,
Author = {Imanol Schlag and Kazuki Irie and Jürgen Schmidhuber},
Title = {Linear Transformers Are Secretly Fast Weight Programmers},
Year = {2021},
Eprint = {arXiv:2102.11174},
}

@article{fu2023flashfftconv,
  title={Flash{FFTC}onv: Efficient Convolutions for Long Sequences with Tensor Cores},
  author={Fu, Daniel Y. and Kumbong, Hermann and Nguyen, Eric and R{\'e}, Christopher},
  booktitle={arXiv preprint arXiv:2311.05908},
  year={2023}
}


@inproceedings{qin2024unlocking,
  title={Unlocking the Secrets of Linear Complexity Sequence Model from A Unified Perspective},
  author={Qin, Zhen and Shen, Xuyang and Sun, Weigao and Li, Dong and Birchfield, Stan and Hartley, Richard and Zhong, Yiran},
  booktitle={arXiv preprint arXiv:2405.17383},
  year={2024}
}


@inproceedings{qin2023accelerating,
  title={Accelerating Toeplitz Neural Network with Constant-time Inference Complexity},
  author={Qin, Zhen and Zhong, Yiran},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={12206--12215},
  year={2023}
}

@article{zhu_visionmamba_2024,
  title={Vision mamba: Efficient visual representation learning with bidirectional state space model},
  author={Zhu, Lianghui and Liao, Bencheng and Zhang, Qian and Wang, Xinlong and Liu, Wenyu and Wang, Xinggang},
  journal={arXiv preprint arXiv:2401.09417},
  year={2024}
}

@inproceedings{rombach_ldm_cvpr_2022,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle=cvpr,
  pages={10684--10695},
  year={2022}
}

@inproceedings{peebles_dit_iccv_2023,
  title={Scalable diffusion models with transformers},
  author={Peebles, William and Xie, Saining},
  booktitle=iccv,
  pages={4195--4205},
  year={2023}
}


@inproceedings{brock_iclr_2018_biggan,
  title={Large Scale GAN Training for High Fidelity Natural Image Synthesis},
  author={Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  booktitle={International Conference on Learning Representations},
  year={2018}
}


@inproceedings{sauer_stylegan_siggraph_2022,
  title={Stylegan-xl: Scaling stylegan to large diverse datasets},
  author={Sauer, Axel and Schwarz, Katja and Geiger, Andreas},
  booktitle=SIGGRAPH,
  pages={1--10},
  year={2022}
}

@article{dhariwal_improve_ddpm_nips_2021,
  title={Diffusion models beat gans on image synthesis},
  author={Dhariwal, Prafulla and Nichol, Alexander},
  journal=NIPS,
  volume={34},
  pages={8780--8794},
  year={2021}
}

@article{ho_CDM_JMLR_2022,
  title={Cascaded diffusion models for high fidelity image generation},
  author={Ho, Jonathan and Saharia, Chitwan and Chan, William and Fleet, David J and Norouzi, Mohammad and Salimans, Tim},
  journal=JMLR,
  volume={23},
  number={47},
  pages={1--33},
  year={2022}
}

@article{duan_visionRWKV_2024,
  title={Vision-rwkv: Efficient and scalable visual perception with rwkv-like architectures},
  author={Duan, Yuchen and Wang, Weiyun and Chen, Zhe and Zhu, Xizhou and Lu, Lewei and Lu, Tong and Qiao, Yu and Li, Hongsheng and Dai, Jifeng and Wang, Wenhai},
  journal={arXiv preprint arXiv:2403.02308},
  year={2024}
}

@article{fei_diffusionRWKV_2024,
  title={Diffusion-RWKV: Scaling RWKV-Like Architectures for Diffusion Models},
  author={Fei, Zhengcong and Fan, Mingyuan and Yu, Changqian and Li, Debang and Huang, Junshi},
  journal={arXiv preprint arXiv:2404.04478},
  year={2024}
}

@misc{openai_mrcr,
  author       = {OpenAI},
  title        = {OpenAI MRCR Dataset},
  year         = {2024},
  publisher    = {Hugging Face},
  howpublished = {\url{https://huggingface.co/datasets/openai/mrcr}},
  note         = {Accessed: 2025-06-15}
}

@article{fei_dis_2024,
  title={Scalable Diffusion Models with State Space Backbone},
  author={Fei, Zhengcong and Fan, Mingyuan and Yu, Changqian and Huang, Junshi},
  journal={arXiv preprint arXiv:2402.05608},
  year={2024}
}

@article{Yan_DiffusionSSM_2023,
  title={Diffusion Models Without Attention},
  author={Jing Nathan Yan and Jiatao Gu and Alexander M. Rush},
  journal={arXiv preprint arXiv:2311.18257},
  year={2023}
}

@inproceedings{gu_S4_ICLR_2021,
  title={Efficiently Modeling Long Sequences with Structured State Spaces},
  author={Gu, Albert and Goel, Karan and Re, Christopher},
  booktitle=ICLR,
  year={2021}
}

@inproceedings{fu_Hippos_iclr_2022,
  title={Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
  author={Fu, Daniel Y and Dao, Tri and Saab, Khaled Kamal and Thomas, Armin W and Rudra, Atri and Re, Christopher},
  booktitle=ICLR,
  year={2022}
}

@inproceedings{choromanski_performer_iclr_2020,
  title={Rethinking Attention with Performers},
  author={Choromanski, Krzysztof Marcin and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared Quincy and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  booktitle=iclr,
  year={2020}
}

@inproceedings{qin_cosformer_iclr_2021,
  title={cosFormer: Rethinking Softmax In Attention},
  author={Qin, Zhen and Sun, Weixuan and Deng, Hui and Li, Dongxu and Wei, Yunshen and Lv, Baohong and Yan, Junjie and Kong, Lingpeng and Zhong, Yiran},
  booktitle=iclr,
  year={2021}
}
@inproceedings{katharopoulos_icml_transformerrnn_icml_2020,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle=icml,
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}

@inproceedings{qin2022devil,
  title={The Devil in Linear Transformer},
  author={Qin, Zhen and Han, Xiaodong and Sun, Weixuan and Li, Dongxu and Kong, Lingpeng and Barnes, Nick and Zhong, Yiran},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={7025--7041},
  year={2022}
}

@article{yang_GLA_2023,
  title={Gated linear attention transformers with hardware-efficient training},
  author={Yang, Songlin and Wang, Bailin and Shen, Yikang and Panda, Rameswar and Kim, Yoon},
  journal={arXiv preprint arXiv:2312.06635},
  year={2023}
}

@article{qin_transnormerllm_2023,
  title={TransNormerLLM: A Faster and Better Large Language Model with Improved TransNormer},
  author={Qin, Zhen and Li, Dong and Sun, Weigao and Sun, Weixuan and Shen, Xuyang and Han, Xiaodong and Wei, Yunshen and Lv, Baohong and Luo, Xiao and Qiao, Yu and others},
  year={2023},
  journal={arXiv preprint arXiv:2307.14995}
}

@article{sun2023retentive,
  title={Retentive network: A successor to transformer for large language models},
  author={Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
  journal={arXiv preprint arXiv:2307.08621},
  year={2023}
}

@inproceedings{peng_rwkv_emnlp_2023,
  title={RWKV: Reinventing RNNs for the Transformer Era},
  author={Peng, Bo and Alcaide, Eric and Anthony, Quentin Gregory and Albalak, Alon and Arcadinho, Samuel and Biderman, Stella and Cao, Huanqi and Cheng, Xin and Chung, Michael Nguyen and Derczynski, Leon and others},
  booktitle=emnlp,
  year={2023}
}


@article{gu_S4D_nips_2022,
  title={On the parameterization and initialization of diagonal state space models},
  author={Gu, Albert and Goel, Karan and Gupta, Ankit and R{\'e}, Christopher},
  journal=NIPS,
  volume={35},
  pages={35971--35983},
  year={2022}
}

@article{hu2024zigma,
  title={Zigma: Zigzag mamba diffusion model},
  author={Hu, Vincent Tao and Baumann, Stefan Andreas and Gui, Ming and Grebenkova, Olga and Ma, Pingchuan and Fischer, Johannes and Ommer, Bjorn},
  journal={arXiv preprint arXiv:2403.13802},
  year={2024}
}

@article{sun_vvt_pami_2023,
  title={Vicinity vision transformer},
  author={Sun, Weixuan and Qin, Zhen and Deng, Hui and Wang, Jianyuan and Zhang, Yi and Zhang, Kaihao and Barnes, Nick and Birchfield, Stan and Kong, Lingpeng and Zhong, Yiran},
  journal=pami,
  year={2023},
  publisher={IEEE}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle=cvpr,
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{touvron_deit_icml_2021,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle=icml,
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}

@inproceedings{merity_Wikitext103_iclr_2016,
  title={Pointer Sentinel Mixture Models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  booktitle=iclr,
  year={2016}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{wang2018glue,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  booktitle=EMNLPW,
  pages={353--355},
  year={2018}
}

@article{peng_rwkv4_2024,
  title={Eagle and Finch: RWKV with matrix-valued states and dynamic recurrence},
  author={Peng, Bo and Goldstein, Daniel and Anthony, Quentin and Albalak, Alon and Alcaide, Eric and Biderman, Stella and Cheah, Eugene and Ferdinan, Teddy and Hou, Haowen and Kazienko, Przemys{\l}aw and others},
  journal={arXiv preprint arXiv:2404.05892},
  year={2024}
}

@article{qin2024hgrn2,
  title={{HGRN2}: Gated linear RNNs with state expansion},
  author={Qin, Zhen and Yang, Songlin and Sun, Weixuan and Shen, Xuyang and Li, Dong and Sun, Weigao and Zhong, Yiran},
  journal={arXiv preprint arXiv:2404.07904},
  year={2024}
}
@article{DBLP:journals/tacl/KwiatkowskiPRCP19,
  author       = {Tom Kwiatkowski and
                  Jennimaria Palomaki and
                  Olivia Redfield and
                  Michael Collins and
                  Ankur P. Parikh and
                  Chris Alberti and
                  Danielle Epstein and
                  Illia Polosukhin and
                  Jacob Devlin and
                  Kenton Lee and
                  Kristina Toutanova and
                  Llion Jones and
                  Matthew Kelcey and
                  Ming{-}Wei Chang and
                  Andrew M. Dai and
                  Jakob Uszkoreit and
                  Quoc Le and
                  Slav Petrov},
  title        = {Natural Questions: a Benchmark for Question Answering Research},
  journal      = {Trans. Assoc. Comput. Linguistics},
  volume       = {7},
  pages        = {452--466},
  year         = {2019},
  url          = {https://doi.org/10.1162/tacl\_a\_00276},
  doi          = {10.1162/TACL\_A\_00276},
  timestamp    = {Wed, 19 Jun 2024 17:28:03 +0200},
  biburl       = {https://dblp.org/rec/journals/tacl/KwiatkowskiPRCP19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{kwiatkowski2019natural,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{DBLP:conf/acl/JoshiCWZ17,
  author       = {Mandar Joshi and
                  Eunsol Choi and
                  Daniel S. Weld and
                  Luke Zettlemoyer},
  editor       = {Regina Barzilay and
                  Min{-}Yen Kan},
  title        = {TriviaQA: {A} Large Scale Distantly Supervised Challenge Dataset for
                  Reading Comprehension},
  booktitle    = {Proceedings of the 55th Annual Meeting of the Association for Computational
                  Linguistics, {ACL} 2017, Vancouver, Canada, July 30 - August 4, Volume
                  1: Long Papers},
  pages        = {1601--1611},
  publisher    = {Association for Computational Linguistics},
  year         = {2017},
  url          = {https://doi.org/10.18653/v1/P17-1147},
  doi          = {10.18653/V1/P17-1147},
  timestamp    = {Fri, 06 Aug 2021 00:40:58 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/JoshiCWZ17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{joshi2017triviaqa,
  title={TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1601--1611},
  year={2017}
}

@inproceedings{bisk2020piqa,
  title={{PIQA}: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  pages={7432--7439},
  year={2020}
}

@article{jared_scaling_law_openai_2020,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{danny_scaling_transfer_openai_2021,
  title={Scaling laws for transfer},
  author={Hernandez, Danny and Jared Kaplan and Tom Henighan and Sam McCandlish},
  journal={arXiv preprint arXiv:2102.01293},
  year={2021}
}

@article{jacob_scaling_law_rl_openai_2023,
  title={Scaling laws for single-agent reinforcement learning},
  author={Hilton, Jacob and Jie Tang and John Schulman},
  journal={arXiv preprint arXiv:2301.13442},
  year={2023}
}

@article{hui_unraveling_2024,
  title={Unraveling the Mystery of Scaling Laws: Part I},
  author={Su, Hui and Zhi Tian and Xiaoyu Shen and Xunliang Cai},
  journal={arXiv preprint arXiv:2403.06563},
  year={2024}
}

@article{xiong_temporal_scaling_2024,
  title={Temporal Scaling Law for Large Language Models},
  author={Xiong, Yizhe and Xiansheng Chen and Xin Ye and Hui Chen and Zijia Lin and Haoran Lian and Jianwei Niu and Guiguang Ding},
  journal={arXiv preprint arXiv:2404.17785},
  year={2024}
}

@inproceedings{clark_scaling_law_moe_icml_2022,
  title={Unified scaling laws for routed language models},
  author={Clark, Aidan and Diego de Las Casas and Aurelia Guy and Arthur Mensch and Michela Paganini and Jordan Hoffmann and Bogdan Damoc and Blake Hechtman and Trevor Cai and Sebastian Borgeaud and others},
  booktitle=icml,
  pages={4057--4086},
  year={2022},
  organization={PMLR}
}

@article{Henighan_scaling_2020,
  title={Scaling laws for autoregressive generative modeling},
  author={Henighan, Tom and Jared Kaplan and Mor Katz and Mark Chen and Christopher Hesse and Jacob Jackson and Heewoo Jun and Tom B. Brown and Prafulla Dhariwal and Scott Gray and others},
  journal={arXiv preprint arXiv:2010.14701},
  year={2020}
}

@article{vaswani_transformer_2017,
  title={Attention is all you need},
  author={Vaswani, Ashish and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Łukasz Kaiser and Illia Polosukhin},
  journal=NIPS,
  volume={30},
  year={2017}
}

@article{llama2_2023,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{qin2024lightning,
  title={Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models},
  author={Qin, Zhen and Sun, Weigao and Li, Dong and Shen, Xuyang and Sun, Weixuan and Zhong, Yiran},
  journal={arXiv preprint arXiv:2401.04658},
  year={2024}
}


@article{gu2021efficiently,
  title={Efficiently modeling long sequences with structured state spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2111.00396},
  year={2021}
}
@article{gu2020hippo,
  title={Hippo: Recurrent memory with optimal polynomial projections},
  author={Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1474--1487},
  year={2020}
}

@article{fu2022hungry,
  title={Hungry hungry hippos: Towards language modeling with state space models},
  author={Fu, Daniel Y and Dao, Tri and Saab, Khaled K and Thomas, Armin W and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2212.14052},
  year={2022}
}

@inproceedings{fu2023simple,
  title={Simple hardware-efficient long convolutions for sequence modeling},
  author={Fu, Daniel Y and Epstein, Elliot L and Nguyen, Eric and Thomas, Armin W and Zhang, Michael and Dao, Tri and Rudra, Atri and R{\'e}, Christopher},
  booktitle={International Conference on Machine Learning},
  pages={10373--10391},
  year={2023},
  organization={PMLR}
}

@inproceedings{orvieto2023resurrecting,
  title={Resurrecting recurrent neural networks for long sequences},
  author={Orvieto, Antonio and Smith, Samuel L and Gu, Albert and Fernando, Anushan and Gulcehre, Caglar and Pascanu, Razvan and De, Soham},
  booktitle={International Conference on Machine Learning},
  pages={26670--26698},
  year={2023},
  organization={PMLR}
}

@article{Bi2024deepseek,
  title={Deepseek llm: Scaling open-source language models with longtermism},
  author={Bi, Xiao and Deli Chen and Guanting Chen and Shanhuang Chen and Damai Dai and Chengqi Deng and Honghui Ding and Kai Dong and Qiushi Du},
  journal={arXiv preprint arXiv:2401.02954},
  year={2024}
}

@article{Isik2024downstream,
  title={Scaling Laws for Downstream Task Performance of Large Language Models},
  author={Isik, Berivan and Natalia Ponomareva and Hussein Hazimeh and Dimitris Paparas and Sergei Vassilvitskii and Sanmi Koyejo},
  journal={arXiv preprint arXiv:2402.04177},
  year={2024}
}

@article{gao2024sparse,
  title={Scaling and evaluating sparse autoencoders},
  author={Gao, Leo and Tom Dupré la Tour and Henk Tillman and Gabriel Goh and Rajan Troll and Alec Radford and Ilya Sutskever and Jan Leike and Jeffrey Wu},
  journal={arXiv preprint arXiv:2406.04093},
  year={2024}
}

@article{gpt4,
  title={Gpt-4 technical report},
  author={OpanAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}
@article{qin2024you,
  title={You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet},
  author={Qin, Zhen and Mao, Yuxin and Shen, Xuyang and Li, Dong and Zhang, Jing and Dai, Yuchao and Zhong, Yiran},
  journal={arXiv preprint arXiv:2405.21022},
  year={2024}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@article{qin2023transnormerllm,
  title={TransNormerLLM: A Faster and Better Large Language Model with Improved TransNormer},
  author={Qin, Zhen and Li, Dong and Sun, Weigao and Sun, Weixuan and Shen, Xuyang and Han, Xiaodong and Wei, Yunshen and Lv, Baohong and Luo, Xiao and Qiao, Yu and others},
  year={2023},
  journal={arXiv preprint arXiv:2307.14995},
}



@InProceedings{qin2024various,  title =  {Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention},
author =       {Qin, Zhen and Sun, Weigao and Li, Dong and Shen, Xuyang and Sun, Weixuan and Zhong, Yiran},
pages =  {41517--41535},
year =  {2024},
booktitle={International conference on machine learning},
  organization={PMLR}
}


@misc{tiktoken, 
title={Openai/tiktoken: Tiktoken is a fast BPE tokeniser for use with openai’s models.}, 
url={https://github.com/openai/tiktoken}, 
journal={GitHub},
author={Openai}} 

@misc{huang2023ceval,
      title={C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models}, 
      author={Yuzhen Huang and Yuzhuo Bai and Zhihao Zhu and Junlei Zhang and Jinghan Zhang and Tangjun Su and Junteng Liu and Chuancheng Lv and Yikai Zhang and Jiayi Lei and Yao Fu and Maosong Sun and Junxian He},
      year={2023},
      eprint={2305.08322},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{yang2024fla,
  title  = {FLA: A Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism},
  author = {Yang, Songlin and Zhang, Yu},
  journal    = {https://github.com/sustcsonglin/flash-linear-attention},
  month  = jan,
  year   = {2024}
}
@article{merity2016wikitext,
  title={The wikitext long term dependency language modeling dataset},
  author={Merity, Stephen},
  journal={Salesforce Metamind},
  volume={9},
  year={2016}
}
@article{paperno2016lambada,
  title={The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  journal={arXiv preprint arXiv:1606.06031},
  year={2016}
}

@inproceedings{shaham2022scrolls,
  title={{SCROLLS}: Standardized CompaRison Over Long Language Sequences},
  author={Shaham, Uri and Segal, Elad and Ivgi, Maor and Efrat, Avia and Yoran, Ori and Haviv, Adi and Gupta, Ankit and Xiong, Wenhan and Geva, Mor and Berant, Jonathan and others},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={12007--12021},
  year={2022}
}


@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}

@inproceedings{
mamba,
title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
author={Albert Gu and Tri Dao},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=tEYskw1VY2}
}


@article{mamba2,
  title={Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality},
  author={Dao, Tri and Gu, Albert},
  journal={arXiv preprint arXiv:2405.21060},
  year={2024}
}

@article{yang2023gated,
  title   = {Gated Linear Attention Transformers with Hardware-Efficient Training},
  author  = {Yang, Songlin and Wang, Bailin and Shen, Yikang and Panda, Rameswar and Kim, Yoon},
  journal = {arXiv preprint arXiv:2312.06635},
  year    = {2023}
}



@inproceedings{
hernandez2022scalinglawsinterpretabilitylearning,
title={Scaling Laws for Multilingual Language Models},
author={Anonymous},
booktitle={Submitted to The Thirteenth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=T2h2V7Rx7q},
note={under review}
}


@misc{xue2023repeatrepeatinsightsscaling,
      title={To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis}, 
      author={Fuzhao Xue and Yao Fu and Wangchunshu Zhou and Zangwei Zheng and Yang You},
      year={2023},
      eprint={2305.13230},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@inproceedings{lee2021deduplicating,
  title={Deduplicating Training Data Makes Language Models Better},
  author={Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={8424--8445},
  year={2022}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training {G}opher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@inproceedings{
penedo2023refinedweb,
title={The {R}efined{W}eb Dataset for {F}alcon {LLM}: Outperforming Curated Corpora with Web Data Only},
author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Hamza Alobeidli and Alessandro Cappelli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2023},
url={https://openreview.net/forum?id=kM5eGcdCzq}
}


@inproceedings{
  penedo2024fineweb,
  title={The {F}ine{W}eb Datasets: Decanting the Web for the Finest Text Data at Scale},
  author={Guilherme Penedo and Hynek Kydl{\'\i}{\v{c}}ek and Loubna Ben allal and Anton Lozhkov and Margaret Mitchell and Colin Raffel and Leandro Von Werra and Thomas Wolf},
  booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2024},
  url={https://openreview.net/forum?id=n6SCkn2QaG}
}



@article{gunasekar2023textbooks,
  title={Textbooks are all you need},
  author={Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
  journal={arXiv preprint arXiv:2306.11644},
  year={2023}
}


@article{abdin2024phi,
  title={Phi-4 technical report},
  author={Abdin, Marah and Aneja, Jyoti and Behl, Harkirat and Bubeck, S{\'e}bastien and Eldan, Ronen and Gunasekar, Suriya and Harrison, Michael and Hewett, Russell J and Javaheripi, Mojan and Kauffmann, Piero and others},
  journal={arXiv preprint arXiv:2412.08905},
  year={2024}
}


@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}


@article{bai2022constitutional,
  title={Constitutional {AI}: Harmlessness from {AI} feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}


@article{wang2022self,
  title={Self-instruct: Aligning language models with self-generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@inproceedings{rafailov2024direct,
  title={Direct preference optimization: your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  booktitle={Proceedings of the 37th International Conference on Neural Information Processing Systems},
  pages={53728--53741},
  year={2023}
}

@inproceedings{
DBLP:conf/iclr/LepikhinLXCFHKS21,
title={{GS}hard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
author={Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=qrwe7XHTmYb}
}

@misc{liu2024llavanext,
    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    month={January},
    year={2024}
}

@inproceedings{
DBLP:conf/iclr/DosovitskiyB0WZ21,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@article{schuhmann2021laion,
  title={{LAION-400M}: Open dataset of {CLIP}-filtered 400 million image-text pairs},
  author={Schuhmann, Christoph and Vencu, Richard and Beaumont, Romain and Kaczmarczyk, Robert and Mullis, Clayton and Katta, Aarush and Coombes, Theo and Jitsev, Jenia and Komatsuzaki, Aran},
  journal={arXiv preprint arXiv:2111.02114},
  year={2021}
}

@article{mccandlish2018empirical,
  title={An empirical model of large-batch training},
  author={McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Team, OpenAI Dota},
  journal={arXiv preprint arXiv:1812.06162},
  year={2018}
}

@inproceedings{li2022blip,
  title={{BLIP}: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={12888--12900},
  year={2022},
  organization={PMLR}
}

@inproceedings{li2023blip2,
  title={{BLIP}-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}

@article{
aria,
title={Aria: An open multimodal native mixture-of-experts model},
author={Li, Dongxu and Liu, Yudong and Wu, Haoning and Wang, Yue and Shen, Zhiqi and Qu,  Bowen and Niu, Xinyao and Wang, Guoyin and Chen, Bei and Li, Junnan},
journal={arXiv preprint arXiv:2410.05993 },
year={2024},
note={}
}
@article{
yu2022coca,
title={{CoCa}: Contrastive Captioners are Image-Text Foundation Models},
author={Yu, Jiahui and Wang, Zirui and Vasudevan,Vijay and Yeung, Legg and Seyedhosseini , Mojtaba and Wu, Yonghui},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=Ee277P3AYC},
note={}
}

@article{shibata1999byte,
  title={Byte pair encoding: A text compression scheme that accelerates pattern matching},
  author={Shibata, Yusuxke and Kida, Takuya and Fukamachi, Shuichi and Takeda, Masayuki and Shinohara, Ayumi and Shinohara, Takeshi and Arikawa, Setsuo},
  year=1999,
  journal={Technical Report DOI-TR-161, Department of Informatics, Kyushu University}
}



@article{hernandez2022scaling,
  title={Scaling laws and interpretability of learning from repeated data},
  author={Hernandez, Danny and Brown, Tom and Conerly, Tom and DasSarma, Nova and Drain, Dawn and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Henighan, Tom and Hume, Tristan and others},
  journal={arXiv preprint arXiv:2205.10487},
  year={2022}
}


@article{muennighoff2023scaling,
  title={Scaling data-constrained language models},
  author={Muennighoff, Niklas and Rush, Alexander and Barak, Boaz and Le Scao, Teven and Tazi, Nouamane and Piktus, Aleksandra and Pyysalo, Sampo and Wolf, Thomas and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={50358--50376},
  year={2023}
}

@article{wang2024deepnet,
  title={Deep{N}et: Scaling transformers to 1,000 layers},
  author={Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Zhang, Dongdong and Wei, Furu},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}


@inproceedings{
loshchilov2017decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}




@article{shao2024deepseekmath,
  title={{DeepSeekMath}: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{xiong2023effective,
  title={Effective long-context scaling of foundation models},
  author={Xiong, Wenhan and Liu, Jingyu and Molybog, Igor and Zhang, Hejia and Bhargava, Prajjwal and Hou, Rui and Martin, Louis and Rungta, Rashi and Sankararaman, Karthik Abinav and Oguz, Barlas and others},
  journal={arXiv preprint arXiv:2309.16039},
  year={2023}
}

@misc{penedo2024finewebdatasetsdecantingweb,
      title={The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale}, 
      author={Guilherme Penedo and Hynek Kydlíček and Loubna Ben allal and Anton Lozhkov and Margaret Mitchell and Colin Raffel and Leandro Von Werra and Thomas Wolf},
      year={2024},
      eprint={2406.17557},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{chen2024mindsearch,
  title={{MindSearch}: Mimicking Human Minds Elicits Deep AI Searcher},
  author={Chen, Zehui and Liu, Kuikun and Wang, Qiuchen and Liu, Jiangning and Zhang, Wenwei and Chen, Kai and Zhao, Feng},
  journal={arXiv preprint arXiv:2407.20183},
  year={2024}
}


@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@misc{jiang2024mixtralexperts,
      title={Mixtral of Experts}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and Lélio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Théophile Gervet and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2024},
      eprint={2401.04088},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@misc{deepseekai2024deepseekv3technicalreport,
      title={{DeepSeek-V3} Technical Report}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@inproceedings{ainslie2023gqa,
  title={{GQA}: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebron, Federico and Sanghai, Sumit},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={4895--4901},
  year={2023}
}


@article{dubey2024llama,
  title={The {L}lama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{yang2024qwen2,
  title={Qwen2.5 Technical Report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{jiang2024megascalescalinglargelanguage,
    author = {Ziheng Jiang and Haibin Lin and Yinmin Zhong and Qi Huang and Yangrui Chen and Zhi Zhang and Yanghua Peng and Xiang Li and Cong Xie and Shibiao Nong and Yulu Jia and Sun He and Hongmin Chen and Zhihao Bai and Qi Hou and Shipeng Yan and Ding Zhou and Yiyao Sheng and Zhuo Jiang and Haohan Xu and Haoran Wei and Zhang Zhang and Pengfei Nie and Leqi Zou and Sida Zhao and Liang Xiang and Zherui Liu and Zhe Li and Xiaoying Jia and Jianxi Ye and Xin Jin and Xin Liu},
    title = {MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs},
    journal = {arXiv preprint arXiv:2402.15627},
    year = {2024}
}

@misc{TransformerEngine, 
title={Transformer Engine}, 
url={https://github.com/NVIDIA/TransformerEngine}, 
year=2023,
author={{NVIDIA}}} 


@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}


@inproceedings{rajbhandari2020zeromemoryoptimizationstraining,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

% <Evaluations>

@inproceedings{
hendryckstest2021,
title={Measuring Massive Multitask Language Understanding},
author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=d7KBjmI3GmQ}
}



@inproceedings{
wang2024mmlu_pro,
title={{MMLU}-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark},
author={Yubo Wang and Xueguang Ma and Ge Zhang and Yuansheng Ni and Abhranil Chandra and Shiguang Guo and Weiming Ren and Aaran Arulraj and Xuan He and Ziyan Jiang and Tianle Li and Max Ku and Kai Wang and Alex Zhuang and Rongqi Fan and Xiang Yue and Wenhu Chen},
booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2024},
url={https://openreview.net/forum?id=y10DM6R2r3}
}


@article{wei2024simpleQA,
  title={Measuring short-form factuality in large language models},
  author={Wei, Jason and Karina, Nguyen and Chung, Hyung Won and Jiao, Yunxin Joy and Papay, Spencer and Glaese, Amelia and Schulman, John and Fedus, William},
  journal={arXiv preprint arXiv:2411.04368},
  year={2024}
}

@article{he2024c_simpleQA,
  title={Chinese simple{QA}: A chinese factuality evaluation for large language models},
  author={He, Yancheng and Li, Shilong and Liu, Jiaheng and Tan, Yingshui and Wang, Weixun and Huang, Hui and Bu, Xingyuan and Guo, Hangyu and Hu, Chengwei and Zheng, Boren and others},
  journal={arXiv preprint arXiv:2411.07140},
  year={2024}
}

@inproceedings{rein2023gpqa,
      title={{GPQA}: A Graduate-Level Google-Proof Q\&A Benchmark},
      author={David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},
      booktitle={First Conference on Language Modeling},
      year={2024},
      url={https://openreview.net/forum?id=Ti67584b98}
}


@inproceedings{dua2019drop,
  title={{DROP}: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs},
  author={Dua, Dheeru and Wang, Yizhong and Dasigi, Pradeep and Stanovsky, Gabriel and Singh, Sameer and Gardner, Matt},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={2368--2378},
  year={2019}
}

@article{cobbe2021gsm8k,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@inproceedings{shen2024scaling,
  title={Scaling Laws for Linear Complexity Language Models},
  author={Shen, Xuyang and Li, Dong and Leng, Ruitao and Qin, Zhen and Sun, Weigao and Zhong, Yiran},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={16377--16426},
  year={2024}
}

@inproceedings{
hendrycks2021math,
title={Measuring Mathematical Problem Solving With the {MATH} Dataset},
author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
year={2021},
url={https://openreview.net/forum?id=7Bywt2mQsCe}
}

@inproceedings{hendrycks2measuring,
  title={Measuring Mathematical Problem Solving With the MATH Dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)}
}

@article{chen2021humaneval,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{austin2021mbpp,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{zhou2023ifeval,
  title={Instruction-following evaluation for large language models},
  author={Zhou, Jeffrey and Lu, Tianjian and Mishra, Swaroop and Brahma, Siddhartha and Basu, Sujoy and Luan, Yi and Zhou, Denny and Hou, Le},
  journal={arXiv preprint arXiv:2311.07911},
  year={2023}
}

@article{li2024arenahard,
  title={From Crowdsourced Data to High-Quality Benchmarks: Arena-{H}ard and {B}ench{B}uilder Pipeline},
  author={Li, Tianle and Chiang, Wei-Lin and Frick, Evan and Dunlap, Lisa and Wu, Tianhao and Zhu, Banghua and Gonzalez, Joseph E and Stoica, Ion},
  journal={arXiv preprint arXiv:2406.11939},
  year={2024}
}

@article{li2023cmmlu,
  title={{CMMLU}: Measuring massive multitask language understanding in chinese},
  author={Li, Haonan and Zhang, Yixuan and Koto, Fajri and Yang, Yifei and Zhao, Hai and Gong, Yeyun and Duan, Nan and Baldwin, Timothy},
  journal={arXiv preprint arXiv:2306.09212},
  year={2023}
}

@inproceedings{
tanzer2023mtob,
title={A Benchmark for Learning to Translate a New Language from One Grammar Book},
author={Garrett Tanzer and Mirac Suzgun and Eline Visser and Dan Jurafsky and Luke Melas-Kyriazi},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=tbVWug9f2h}
}


@misc{gkamradt2023needleinhaystack,
  author       = {G. Kamradt},
  title        = {LLMTest\_NeedleInAHaystack},
  year         = {2023},
  url= {https://github.com/gkamradt/LLMTest_NeedleInAHaystack},
}

@article{ruler2024,
  title={{RULER}: What's the Real Context Size of Your Long-Context Language Models?},
  author={Hsieh, Cheng-Ping and Simeng Sun and Samuel Kriman and Shantanu Acharya and Dima Rekesh and Fei Jia and Boris Ginsburg},
  journal={arXiv preprint arXiv:2404.06654},
  year={2024}
}

@article{vodrahalli2024mrcr,
  title={Michelangelo: Long context evaluations beyond haystacks via latent structure queries},
  author={Vodrahalli, Kiran and Ontanon, Santiago and Tripuraneni, Nilesh and Xu, Kelvin and Jain, Sanil and Shivanna, Rakesh and Hui, Jeffrey and Dikkala, Nishanth and Kazemi, Mehran and Fatemi, Bahare and others},
  journal={arXiv preprint arXiv:2409.12640},
  year={2024}
}

@inproceedings{zhang2024infinitebench,
  title={∞ Bench: Extending long context evaluation beyond 100k tokens},
  author={Zhang, Xinrong and Chen, Yingfa and Hu, Shengding and Xu, Zihang and Chen, Junhao and Hao, Moo and Han, Xu and Thai, Zhen and Wang, Shuo and Liu, Zhiyuan and others},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={15262--15277},
  year={2024}
}

@article{bai2023longbench,
  title={{LongBench}: A bilingual, multitask benchmark for long context understanding},
  author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and others},
  journal={arXiv preprint arXiv:2308.14508},
  year={2023}
}

@article{bai2024longbench2,
  title={{LongBench} v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks}, 
  author={Yushi Bai and Shangqing Tu and Jiajie Zhang and Hao Peng and Xiaozhi Wang and Xin Lv and Shulin Cao and Jiazheng Xu and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li},
  journal={arXiv preprint arXiv:2412.15204},
  year={2024}
}
 

@inproceedings{
agarwal2024manyshot_icl,
title={Many-shot In-Context Learning},
author={Rishabh Agarwal and Avi Singh and Lei M Zhang and Bernd Bohnet and Luis Rosias and Stephanie C.Y. Chan and Biao Zhang and Aleksandra Faust and Hugo Larochelle},
booktitle={ICML 2024 Workshop on In-Context Learning},
year={2024},
url={https://openreview.net/forum?id=goi7DFHlqS}
}


% <End of Evaluations>


@inproceedings{broder1997resemblance,
  title={On the resemblance and containment of documents},
  author={Broder, Andrei Z},
  booktitle={Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171)},
  pages={21--29},
  year={1997},
  organization={IEEE}
}


@article{sun2024linearattentionsequenceparallelism,
    author = {Weigao Sun and Zhen Qin and Dong Li and Xuyang Shen and Yu Qiao and Yiran Zhong},
    title = {Linear Attention Sequence Parallelism},
    journal = {arXiv preprint arXiv:2404.02882},
    year = {2024}
}

@misc{glm2024chatglm,
      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, 
      author={Team GLM  and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},
      year={2024},
      eprint={2406.12793},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@article{fedus2022switchtransformersscalingtrillion,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}

@article{chowdhery2022palmscalinglanguagemodeling,
  title={{PaLM}: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

@article{baevski2018adaptive,
  title={Adaptive input representations for neural language modeling},
  author={Baevski, Alexei and Auli, Michael},
  journal={arXiv preprint arXiv:1809.10853},
  year={2018}
}

@inproceedings{wang2019learning,
  title={Learning Deep Transformer Models for Machine Translation},
  author={Wang, Qiang and Li, Bei and Xiao, Tong and Zhu, Jingbo and Li, Changliang and Wong, Derek F and Chao, Lidia S},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={1810--1822},
  year={2019}
}

@article{chen2024mega,
  title={{MEGA-Bench}: Scaling multimodal evaluation to over 500 real-world tasks},
  author={Chen, Jiacheng and Liang, Tianhao and Siu, Sherman and Wang, Zhengqing and Wang, Kai and Wang, Yubo and Ni, Yuansheng and Zhu, Wang and Jiang, Ziyan and Lyu, Bohan and others},
  journal={arXiv preprint arXiv:2410.10563},
  year={2024}
}
@article{ma2024mmlongbench,
  title={{MMLongBench-Doc}: Benchmarking long-context document understanding with visualizations},
  author={Ma, Yubo and Zang, Yuhang and Chen, Liangyu and Chen, Meiqi and Jiao, Yizhu and Li, Xinze and Lu, Xinyuan and Liu, Ziyu and Ma, Yan and Dong, Xiaoyi and others},
  journal={arXiv preprint arXiv:2407.01523},
  year={2024}
}
@inproceedings{yue2024mmmu,
  title={{MMMU}: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi},
  author={Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9556--9567},
  year={2024}
}
@article{yue2024mmmupro,
  title={{MMMU}-{P}ro: A more robust multi-discipline multimodal understanding benchmark},
  author={Yue, Xiang and Zheng, Tianyu and Ni, Yuansheng and Wang, Yubo and Zhang, Kai and Tong, Shengbang and Sun, Yuxuan and Yu, Botao and Zhang, Ge and Sun, Huan and others},
  journal={arXiv preprint arXiv:2409.02813},
  year={2024}
}
@article{he2024olympiadbench,
  title={{OlympiadBench}: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems},
  author={He, Chaoqun and Luo, Renjie and Bai, Yuzhuo and Hu, Shengding and Thai, Zhen Leng and Shen, Junhao and Hu, Jinyi and Han, Xu and Huang, Yujie and Zhang, Yuxiang and others},
  journal={arXiv preprint arXiv:2402.14008},
  year={2024}
}
@article{lu2023mathvista,
  title={{MathVista}: Evaluating mathematical reasoning of foundation models in visual contexts},
  author={Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.02255},
  year={2023}
}
@inproceedings{kembhavi2016diagram,
  title={A diagram is worth a dozen images},
  author={Kembhavi, Aniruddha and Salvato, Mike and Kolve, Eric and Seo, Minjoon and Hajishirzi, Hannaneh and Farhadi, Ali},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part IV 14},
  pages={235--251},
  year={2016},
  organization={Springer}
}
@article{liu2024ocrbench,
  title={{OCRB}ench: On the hidden mystery of {OCR} in large multimodal models},
  author={Liu, Yuliang and Li, Zhang and Huang, Mingxin and Yang, Biao and Yu, Wenwen and Li, Chunyuan and Yin, Xu-Cheng and Liu, Cheng-Lin and Jin, Lianwen and Bai, Xiang},
  journal={Science China Information Sciences},
  volume={67},
  number={12},
  pages={220102},
  year={2024},
  publisher={Springer}
}
@inproceedings{mathew2021docvqa,
  title={Doc{VQA}: A dataset for {VQA} on document images},
  author={Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV},
  booktitle={Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages={2200--2209},
  year={2021}
}
@article{masry2022chartqa,
  title={Chart{QA}: A benchmark for question answering about charts with visual and logical reasoning},
  author={Masry, Ahmed and Long, Do Xuan and Tan, Jia Qing and Joty, Shafiq and Hoque, Enamul},
  journal={arXiv preprint arXiv:2203.10244},
  year={2022}
}

@misc{bai2025longbenchv2deeperunderstanding,
      title={LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks}, 
      author={Yushi Bai and Shangqing Tu and Jiajie Zhang and Hao Peng and Xiaozhi Wang and Xin Lv and Shulin Cao and Jiazheng Xu and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li},
      year={2025},
      eprint={2412.15204},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@article{hurst2024gpt,
  title={{GPT}-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@misc{claude35,
  title={Introducing Claude 3.5 Sonnet},
  author={Anthropic},
  url="https://www.anthropic.com/news/claude-3-5-sonnet",
  year={2024}
}

@article{grattafiori2024llama,
  title={The Llama 3 herd of models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv e-prints},
  pages={arXiv--2407},
  year={2024}
}
@article{team2024jamba,
  title={Jamba-1.5: Hybrid {T}ransformer-{M}amba models at scale},
  author={{Jamba Team} and Lenz, Barak and Arazi, Alan and Bergman, Amir and Manevich, Avshalom and Peleg, Barak and Aviram, Ben and Almagor, Chen and Fridman, Clara and Padnos, Dan and others},
  journal={arXiv preprint arXiv:2408.12570},
  year={2024}
}

@article{ren2024samba,
  title={Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling},
  author={Ren, Liliang and Liu, Yang and Lu, Yadong and Shen, Yelong and Liang, Chen and Chen, Weizhu},
  journal={arXiv preprint arXiv:2406.07522},
  year={2024}
}
@article{glorioso2024zamba,
  title={Zamba: A Compact 7B {SSM} Hybrid Model},
  author={Glorioso, Paolo and Anthony, Quentin and Tokpanov, Yury and Whittington, James and Pilault, Jonathan and Ibrahim, Adam and Millidge, Beren},
  journal={arXiv preprint arXiv:2405.16712},
  year={2024}
}
@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}
@misc{liu2025synlogic,
      title={SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond}, 
      author={Junteng Liu and Yuanxiang Fan and Zhuo Jiang and Han Ding and Yongyi Hu and Chi Zhang and Yiqi Shi and Shitong Weng and Aili Chen and Shiqi Chen and Yunan Huang and Mozhi Zhang and Pengyu Zhao and Junjie Yan and Junxian He},
      year={2025},
      eprint={2505.19641},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}

@article{minimax2025minimax01,
  title={Minimax-01: Scaling foundation models with lightning attention},
  author={MiniMax and Li, Aonian and Gong, Bangwei and Yang, Bo and Shan, Boji and Liu, Chang and Zhu, Cheng and Zhang, Chunhao and Guo, Congchao and Chen, Da and Li, Dong and others},
  journal={arXiv preprint arXiv:2501.08313},
  year={2025}
}

@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and others},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@misc{openai2025o3o4mini,
  title = {Introducing OpenAI o3 and o4-mini},
  author = {{OpenAI}},
  year = {2025},
  url = {https://openai.com/index/introducing-o3-and-o4-mini/}
}

@misc{openai2025research,
  title = {Introducing Deep Research},
  author = {{OpenAI}},
  year = {2025},
  url = {https://openai.com/index/introducing-deep-research/}
}

@inproceedings{xie2024osworld,
  title={Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments},
  author={Xie, Tianbao and Zhang, Danyang and Chen, Jixuan and Li, Xiaochuan and Zhao, Siheng and Cao, Ruisheng and Hua, Toh J and Cheng, Zhoujun and Shin, Dongchan and Lei, Fangyu and others},
  booktitle={Advances in Neural Information Processing Systems},
  year={2024}
}

@misc{starace2025paperbenchevaluatingaisability,
      title={PaperBench: Evaluating AI's Ability to Replicate AI Research}, 
      author={Giulio Starace and Oliver Jaffe and Dane Sherburn and James Aung and Jun Shern Chan and Leon Maksin and Rachel Dias and Evan Mays and Benjamin Kinsella and Wyatt Thompson and Johannes Heidecke and Amelia Glaese and Tejal Patwardhan},
      year={2025},
      eprint={2504.01848},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}

@misc{yu2025dapoopensourcellmreinforcement,
      title={DAPO: An Open-Source LLM Reinforcement Learning System at Scale}, 
      author={Qiying Yu and Zheng Zhang and Ruofei Zhu and Yufeng Yuan and Xiaochen Zuo and Yu Yue and Weinan Dai and Tiantian Fan and Gaohong Liu and Lingjun Liu and Xin Liu and Haibin Lin and Zhiqi Lin and Bole Ma and Guangming Sheng and Yuxuan Tong and Chi Zhang and Mofan Zhang and Wang Zhang and Hang Zhu and Jinhua Zhu and Jiaze Chen and Jiangjie Chen and Chengyi Wang and Hongli Yu and Yuxuan Song and Xiangpeng Wei and Hao Zhou and Jingjing Liu and Wei-Ying Ma and Ya-Qin Zhang and Lin Yan and Mu Qiao and Yonghui Wu and Mingxuan Wang},
      year={2025},
      eprint={2503.14476},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@misc{molybog2023theoryadaminstabilitylargescale,
      title={A Theory on Adam Instability in Large-Scale Machine Learning}, 
      author={Igor Molybog and Peter Albert and Moya Chen and Zachary DeVito and David Esiobu and Naman Goyal and Punit Singh Koura and Sharan Narang and Andrew Poulton and Ruan Silva and Binh Tang and Diana Liskovich and Puxin Xu and Yuchen Zhang and Melanie Kambadur and Stephen Roller and Susan Zhang},
      year={2023},
      eprint={2304.09871},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@misc{cui2025entropymechanismreinforcementlearning,
      title={The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models}, 
      author={Ganqu Cui and Yuchen Zhang and Jiacheng Chen and Lifan Yuan and Zhi Wang and Yuxin Zuo and Haozhan Li and Yuchen Fan and Huayu Chen and Weize Chen and Zhiyuan Liu and Hao Peng and Lei Bai and Wanli Ouyang and Yu Cheng and Bowen Zhou and Ning Ding},
      year={2025},
      eprint={2505.22617},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@misc{wang20258020rulehighentropyminority,
      title={Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning}, 
      author={Shenzhi Wang and Le Yu and Chang Gao and Chujie Zheng and Shixuan Liu and Rui Lu and Kai Dang and Xionghui Chen and Jianxin Yang and Zhenru Zhang and Yuqiong Liu and An Yang and Andrew Zhao and Yang Yue and Shiji Song and Bowen Yu and Gao Huang and Junyang Lin},
      year={2025},
      eprint={2506.01939},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@misc{liu2025understandingr1zeroliketrainingcritical,
      title={Understanding R1-Zero-Like Training: A Critical Perspective}, 
      author={Zichen Liu and Changyu Chen and Wenjun Li and Penghui Qi and Tianyu Pang and Chao Du and Wee Sun Lee and Min Lin},
      year={2025},
      eprint={2503.20783},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@misc{hu2025openreasonerzeroopensourceapproach,
      title={Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model}, 
      author={Jingcheng Hu and Yinmin Zhang and Qi Han and Daxin Jiang and Xiangyu Zhang and Heung-Yeung Shum},
      year={2025},
      eprint={2503.24290},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@misc{zeng2025simplerlzooinvestigatingtamingzero,
      title={SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild}, 
      author={Weihao Zeng and Yuzhen Huang and Qian Liu and Wei Liu and Keqing He and Zejun Ma and Junxian He},
      year={2025},
      eprint={2503.18892},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@article{seed2025seed1,
  title={Seed1. 5-thinking: Advancing superb reasoning models with reinforcement learning},
  author={Seed, ByteDance and Chen, Jiaze and Fan, Tiantian and Liu, Xin and Liu, Lingjun and Lin, Zhiqi and Wang, Mingxuan and Wang, Chengyi and Wei, Xiangpeng and Xu, Wenyuan and others},
  journal={arXiv preprint arXiv:2504.13914},
  year={2025}
}

@misc{anthropic2025claude37,
  author    = {Anthropic},
  title     = {Claude 3.7 Sonnet and Claude Code},
  howpublished = {\url{https://www.anthropic.com/news/claude-3-7-sonnet}},
  note      = {Blog post, February 24, 2025},
  year      = {2025}
}

@misc{deepmind2025geminipro,
  author    = {{Google DeepMind}},
  title     = {Gemini Pro},
  howpublished = {\url{https://deepmind.google/models/gemini/pro/}},
  note      = {Web page, accessed 2025},
  year      = {2025}
}

@article{team2025kimi,
  title={Kimi k1. 5: Scaling reinforcement learning with llms},
  author={{Kimi Team} and Du, Angang and Gao, Bofei and Xing, Bowei and Jiang, Changjiu and Chen, Cheng and Li, Cheng and Xiao, Chenjun and Du, Chenzhuang and Liao, Chonghua and others},
  journal={arXiv preprint arXiv:2501.12599},
  year={2025}
}

@misc{openai2024o1,
  author    = {OpenAI},
  title     = {Introducing OpenAI o1},
  howpublished = {\url{https://openai.com/o1/}},
  note      = {Web page, accessed 2024},
  year      = {2024}
}
@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}
@inproceedings{rein2024gpqa,
  title={Gpqa: A graduate-level google-proof q\&a benchmark},
  author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R},
  booktitle={First Conference on Language Modeling},
  year={2024}
}
@inproceedings{wang2024mmlu,
  title={Mmlu-pro: A more robust and challenging multi-task language understanding benchmark},
  author={Wang, Yubo and Ma, Xueguang and Zhang, Ge and Ni, Yuansheng and Chandra, Abhranil and Guo, Shiguang and Ren, Weiming and Arulraj, Aaran and He, Xuan and Jiang, Ziyan and others},
  booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2024}
}
@article{phan2025humanity,
  title={Humanity's last exam},
  author={Phan, Long and Gatti, Alice and Han, Ziwen and Li, Nathaniel and Hu, Josephina and Zhang, Hugh and Zhang, Chen Bo Calvin and Shaaban, Mohamed and Ling, John and Shi, Sean and others},
  journal={arXiv preprint arXiv:2501.14249},
  year={2025}
}
@article{lin2025zebralogic,
  title={ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning},
  author={Lin, Bill Yuchen and Bras, Ronan Le and Richardson, Kyle and Sabharwal, Ashish and Poovendran, Radha and Clark, Peter and Choi, Yejin},
  journal={arXiv preprint arXiv:2502.01100},
  year={2025}
}
@inproceedings{jainlivecodebench,
  title={LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code},
  author={Jain, Naman and Han, King and Gu, Alex and Li, Wen-Ding and Yan, Fanjia and Zhang, Tianjun and Wang, Sida and Solar-Lezama, Armando and Sen, Koushik and Stoica, Ion},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025}
}


@misc{liu2024fullstackbenchevaluatingllms,
      title={FullStack Bench: Evaluating LLMs as Full Stack Coders}, 
      author={Siyao Liu and He Zhu and Jerry Liu and Shulin Xin and Aoyan Li and Rui Long and Li Chen and Jack Yang and Jinxiang Xia and Z. Y. Peng and Shukai Liu and Zhaoxiang Zhang and Ge Zhang and Wenhao Huang and Kai Shen and Liang Xiang},
      year={2024},
      eprint={2412.00535},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}

@article{bai2024longbench,
  title={LongBench v2: Towards deeper understanding and reasoning on realistic long-context multitasks},
  author={Bai, Yushi and Tu, Shangqing and Zhang, Jiajie and Peng, Hao and Wang, Xiaozhi and Lv, Xin and Cao, Shulin and Xu, Jiazheng and Hou, Lei and Dong, Yuxiao and others},
  journal={arXiv preprint arXiv:2412.15204},
  year={2024}
}


@inproceedings{yaotau,
  title={$\tau$-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains}, 
  author={Yao, Shunyu and Shinn, Noah and Razavi, Pedram and Narasimhan, Karthik R},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025}
}


@article{wei2024measuring,
  title={Measuring short-form factuality in large language models},
  author={Wei, Jason and Karina, Nguyen and Chung, Hyung Won and Jiao, Yunxin Joy and Papay, Spencer and Glaese, Amelia and Schulman, John and Fedus, William},
  journal={arXiv preprint arXiv:2411.04368},
  year={2024}
}

@misc{hunyuan_t1,
  author       = {{Tencent AI Lab}},
  title        = {Hunyuan-T1: Reasoning Efficiency Redefined},
  howpublished = {\url{https://llm.hunyuan.tencent.com/#/Blog/hy-t1/}},
  year         = {2025},
  note         = {Accessed: 2025-06-15}
}

@article{sirdeshmukh2025multichallenge,
  title={MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark Challenging to Frontier LLMs},
  author={Sirdeshmukh, Ved and Deshpande, Kaustubh and Mols, Johannes and Jin, Lifeng and Cardona, Ed-Yeremai and Lee, Dean and Kritz, Jeremy and Primack, Willow and Yue, Summer and Xing, Chen},
  journal={arXiv preprint arXiv:2501.17399},
  year={2025}
}

@article{xia2024agentless,
  title={Agentless: Demystifying llm-based software engineering agents},
  author={Xia, Chunqiu Steven and Deng, Yinlin and Dunn, Soren and Zhang, Lingming},
  journal={arXiv preprint arXiv:2407.01489},
  year={2024}
}

@article{chen2025revisit,
  title={Revisit Self-Debugging with Self-Generated Tests for Code Generation},
  author={Chen, Xiancai and Tao, Zhengwei and Zhang, Kechi and Zhou, Changzhi and Gu, Wanli and He, Yuanpeng and Zhang, Mengdi and Cai, Xunliang and Zhao, Haiyan and Jin, Zhi},
  journal={arXiv preprint arXiv:2501.12793},
  year={2025}
}

@inproceedings{
gu2023how,
title={How to Train your {HIPPO}: State Space Models with Generalized Orthogonal Basis Projections},
author={Albert Gu and Isys Johnson and Aman Timalsina and Atri Rudra and Christopher Re},
booktitle={International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=klK17OQ3KB}
}
@article{gru,
  author       = {Junyoung Chung and
                  {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
                  KyungHyun Cho and
                  Yoshua Bengio},
  title        = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
                  Modeling},
  journal      = {CoRR},
  volume       = {abs/1412.3555},
  year         = {2014},
  eprinttype    = {arXiv},
  eprint       = {1412.3555},
  timestamp    = {Mon, 13 Aug 2018 16:47:38 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/ChungGCB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{du2025mom,
  title={Mom: Linear sequence modeling with mixture-of-memories},
  author={Du, Jusen and Sun, Weigao and Lan, Disen and Hu, Jiaxi and Cheng, Yu},
  journal={arXiv preprint arXiv:2502.13685},
  year={2025}
}
@article{zhang2024gated,
  title={Gated slot attention for efficient linear-time sequence modeling},
  author={Zhang, Yu and Yang, Songlin and Zhu, Rui-Jie and Zhang, Yue and Cui, Leyang and Wang, Yiqiao and Wang, Bolun and Shi, Freda and Wang, Bailin and Bi, Wei and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={116870--116898},
  year={2024}
}

@article{arora2024simple,
  title={Simple linear attention language models balance the recall-throughput tradeoff},
  author={Arora, Simran and Eyuboglu, Sabri and Zhang, Michael and Timalsina, Aman and Alberti, Silas and Zinsley, Dylan and Zou, James and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2402.18668},
  year={2024}
}
@article{sun2025linear,
  title={Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts},
  author={Sun, Weigao and Lan, Disen and Zhu, Tong and Qu, Xiaoye and Cheng, Yu},
  journal={arXiv preprint arXiv:2503.05447},
  year={2025}
}
@article{yang2024parallelizing,
  title={Parallelizing linear transformers with the delta rule over sequence length},
  author={Yang, Songlin and Wang, Bailin and Zhang, Yu and Shen, Yikang and Kim, Yoon},
  journal={arXiv preprint arXiv:2406.06484},
  year={2024}
}


@article{siems2025deltaproduct,
  title={Deltaproduct: Improving state-tracking in linear rnns via householder products},
  author={Siems, Julien and Carstensen, Timur and Zela, Arber and Hutter, Frank and Pontil, Massimiliano and Grazzi, Riccardo},
  journal={arXiv preprint arXiv:2502.10297},
  year={2025}
}
@article{chou2024metala,
  title={MetaLA: Unified optimal linear approximation to softmax attention map},
  author={Chou, Yuhong and Yao, Man and Wang, Kexin and Pan, Yuqi and Zhu, Rui-Jie and Wu, Jibin and Zhong, Yiran and Qiao, Yu and Xu, Bo and Li, Guoqi},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={71034--71067},
  year={2024}
}
@article{peng2025rwkv,
  title={Rwkv-7" goose" with expressive dynamic state evolution},
  author={Peng, Bo and Zhang, Ruichong and Goldstein, Daniel and Alcaide, Eric and Du, Xingjian and Hou, Haowen and Lin, Jiaju and Liu, Jiaxing and Lu, Janna and Merrill, William and others},
  journal={arXiv preprint arXiv:2503.14456},
  year={2025}
}
@article{peng2024eagle,
  title={Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence},
  author={Peng, Bo and Goldstein, Daniel and Anthony, Quentin and Albalak, Alon and Alcaide, Eric and Biderman, Stella and Cheah, Eugene and Ferdinan, Teddy and Hou, Haowen and Kazienko, Przemys{\l}aw and others},
  journal={arXiv preprint arXiv:2404.05892},
  volume={3},
  year={2024}
}
@article{yuan2025native,
  title={Native sparse attention: Hardware-aligned and natively trainable sparse attention},
  author={Yuan, Jingyang and Gao, Huazuo and Dai, Damai and Luo, Junyu and Zhao, Liang and Zhang, Zhengyan and Xie, Zhenda and Wei, YX and Wang, Lean and Xiao, Zhiping and others},
  journal={arXiv preprint arXiv:2502.11089},
  year={2025}
}
@article{lu2025moba,
  title={Moba: Mixture of block attention for long-context llms},
  author={Lu, Enzhe and Jiang, Zhejun and Liu, Jingyuan and Du, Yulun and Jiang, Tao and Hong, Chao and Liu, Shaowei and He, Weiran and Yuan, Enming and Wang, Yuzhi and others},
  journal={arXiv preprint arXiv:2502.13189},
  year={2025}
}
@article{he2024rodimus,
  title={Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions},
  author={He, Zhihao and Yu, Hang and Gong, Zi and Liu, Shizhan and Li, Jianguo and Lin, Weiyao},
  journal={arXiv preprint arXiv:2410.06577},
  year={2024}
}
@article{sun2024learning,
  title={Learning to (learn at test time): Rnns with expressive hidden states},
  author={Sun, Yu and Li, Xinhao and Dalal, Karan and Xu, Jiarui and Vikram, Arjun and Zhang, Genghan and Dubois, Yann and Chen, Xinlei and Wang, Xiaolong and Koyejo, Sanmi and others},
  journal={arXiv preprint arXiv:2407.04620},
  year={2024}
}
@article{behrouz2024titans,
  title={Titans: Learning to memorize at test time},
  author={Behrouz, Ali and Zhong, Peilin and Mirrokni, Vahab},
  journal={arXiv preprint arXiv:2501.00663},
  year={2024}
}
@article{von2025mesanet,
  title={MesaNet: Sequence Modeling by Locally Optimal Test-Time Training},
  author={von Oswald, Johannes and Scherrer, Nino and Kobayashi, Seijin and Versari, Luca and Yang, Songlin and Schlegel, Maximilian and Maile, Kaitlin and Schimpf, Yanick and Sieberling, Oliver and Meulemans, Alexander and others},
  journal={arXiv preprint arXiv:2506.05233},
  year={2025}
}

@article{sheng2024hybridflow,
  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},
  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2409.19256}
}