\section{Preparation for Scalable RL: Continual Pretraining and SFT}

In this work, we focus on scaling up reinforcement learning to enhance reasoning capabilities of Minimax-Text-01. To facilitate scalable RL training, we first carry out continual pretraining of our base model to strengthen its intrinsic reasoning abilities. Subsequently, we perform a cold-start supervised fine-tuning (SFT) stage to inject specific reasoning patterns to the model, thereby providing a stronger foundation for the subsequent RL phase.


\subsection{Continual Pre-Training: Foundation for RL Scaling}

To enhance the reasoning and long context capabilities of the foundation model while ensuring diversity, we continue training the MiniMax-Text-01 model with additional 7.5T tokens with optimized data quality and mixture.

\noindent\textbf{Training Data.}
We refine our pretraining Web and PDF parsing mechanisms and enhance our heuristic cleaning rules to ensure a high recall rate for mathematical and code-related data. We prioritize the extraction of natural Question-Answer (QA) pairs from a diverse range of sources, including webpages, forums, and textbooks, while strictly avoiding the use of synthetic data. Additionally, we conduct semantic deduplication on the QA data to maintain its diversity and uniqueness. Furthermore, we increase the proportion of STEM (Science, Technology, Engineering, and Mathematics), code, book, and reasoning-related data to 70\%. This significantly enhances the foundation model's ability to handle complex tasks without compromising its other general capabilities.

\noindent\textbf{Training Recipe.}
We decrease the coefficient of the MoE auxiliary loss and adjust the parallel training strategy to support a larger training micro batch size, which mitigates the detrimental effects of the auxiliary loss on overall model performance. Based on MiniMax-Text-01, we continue training with a constant learning rate of 8e-5 for 2.5T tokens, followed by a decay schedule over 5T tokens down to 8e-6.

\noindent\textbf{Long Context Extension.}
For a hybrid-lightning architecture model with higher convergence complexity, we have observed that excessively aggressive extensions of the training length can lead to a sudden gradient explosion that may occur during the training process. This makes the optimization process extremely challenging. We attribute this to the parameter optimization of the earlier layers not keeping up with the changes in the later layers -- For lightning attention, the earlier and later layers have different decay rates, which makes the earlier layers focus more on local information. We alleviate this issue by adapting a smoother extension of context length across four stages, starting from a 32K context window length and ultimately extending the training context to 1M tokens.  


\subsection{Supervised Fine-Tuning: Focused Alignment for Efficient RL}

After continual pretraining, we conduct Supervised Fine-Tuning (SFT) to instill desired behaviors like reflection-based Chain-of-Thought (CoT) reasoning using high-quality examples, creating a strong starting point for more efficient and stable RL in the next stage. Specifically, we curate data samples with long CoT responses. These data samples cover diverse domains such as math, coding, STEM, writing, QA, and multi-turn chat.  Math and coding samples account for around 60\% of all the data.


\section{Efficient RL Scaling: Algorithms and Lightning Attention} 
As shown in Figure~\ref{fig:perf-bars-flops} (Right), the M1 architecture demonstrates a clear efficiency advantage during inference. This naturally facilitates efficient RL scaling where increasingly longer responses are generated. However, as pioneers in scaling up RL with this hybrid architecture, we encounter unique challenges during the process, and the RL procedure can become unstable or even fail due to various issues.
To address these difficulties, we develop targeted solutions that enable us to successfully scale up RL training for M1. In addition, we propose a new RL algorithm that achieves greater RL efficiency compared to existing methods.
These dual contributions yield an efficient and scalable RL framework for training M1, where the complete training cycle requires 3 weeks on 512 H800 GPUs—equivalent to a rental cost of approximately \$0.53M USD.
In this section, we first provide general context on RL and present our novel RL algorithm, and then describe the specific challenges we face with the hybrid architecture, along with the solutions we devise to overcome them.


\subsection{Efficient RL Scaling with \method{}}
\label{sec:method}

\noindent\textbf{Background.}
For questions $q$ from a dataset $\mathcal{D}$, we denote $\pi$ as the policy model parameterized by $\theta$, and $o$ as the response generated by the policy.
PPO~\citep{schulman2017proximal} adopts the following objective to optimize the policy to maximize the expected return, and a clipping operation is applied to stabilize training: 
\begin{equation}
\begin{aligned}
\mathcal{J}_{\text{PPO}}(\theta) &= \mathbb{E}_{q \sim \mathcal{D}, o_i \sim \pi_{\theta_{\text{old}}}(\cdot|q)} \\
& \left[
     \frac{1}{|o_i|}\sum_{t=1}^{|o_i|} \min\left( r_{i,t}(\theta) \hat{A}_{i,t}, \text{clip}\big(r_{i,t}(\theta), 1 - \epsilon, 1 + \epsilon\big) \hat{A}_{i,t}\right) - \beta D_{KL}(\pi_{\theta} || \pi_{\text{ref}})
\right],
\end{aligned}
\label{eq:grpo_objective} 
\end{equation}
where $r_{i,t}(\theta) = \frac{\pi_\theta(o_{i,t} \mid q, o_{i,<t})}{\pi_{\theta_{\text{old}}}(o_{i,t} \mid q, o_{i,<t})}$ is the importance sampling (IS) weight, which is used to correct the distribution during off-policy updates, because we use $\pi_{\theta_{\text{old}}}$ to collect trajectories to update the policy via multiple steps in a minibatch manner. While PPO requires a separate value model to compute the advantage $\hat{A}_{i,t}$, GRPO~\citep{shao2024deepseekmath} eliminates the value model and defines the advantage as the output reward relative to other responses in the group:
\begin{equation}
\hat{A}_{i,t} = \frac{R_i - \text{mean}(\{R_j\}_{j=1}^G)}{\text{std}(\{R_j\}_{j=1}^G)}, 
\end{equation}
where $R_i$ is the reward of the response, and $G$ responses $\{o_i\}^G_{i=1}$ are sampled for each question. The reward is either from rule-based verifiers such as in mathematical problem solving, or from a reward model. 

\noindent\textbf{Issues of Token Clipping.}
In our initial experiments with the hybrid architecture under the zero-RL setting, we observed that the GRPO algorithm adversely affected training performance and failed to effectively promote the emergence of long CoT reasoning behaviors. Through a series of controlled ablation studies, we ultimately identified the undesirable clipping operation in the original PPO/GRPO loss as the primary factor contributing to degraded learning performance.
Specifically, we found that tokens associated with reflective behaviors (e.g., \texttt{However}, \texttt{Recheck}, \texttt{Wait}, \texttt{Aha}), which often serve as ``forks'' in reasoning paths, were typically rare and assigned low probabilities by our base model. During policy updates, these tokens were likely to exhibit high $r_{i,t}$ values. As a result, these tokens were clipped out after the first on-policy update, preventing them from contributing to subsequent off-policy gradient updates. This issue was particularly pronounced in our hybrid-architecture model and further hindered the scalability of reinforcement learning.
These low-probability tokens, however, are often crucial for stabilizing entropy~\citep{cui2025entropymechanismreinforcementlearning} and facilitating scalable RL~\citep{wang20258020rulehighentropyminority}. Although DAPO attempts to mitigate this issue by increasing the upper clipping bound~\citep{yu2025dapoopensourcellmreinforcement}, we found this approach to be less effective in our setup, which involved 16 rounds of off-policy updates per generation batch.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.5\textwidth]
    {figures/acc_compare_cropped.pdf}
\caption{Comparison of GRPO, DAPO, and our proposed \method{} on AIME 2024, based on Qwen2.5-32B-base. \method{} outperforms both GRPO and DAPO in terms of performance at the same number of training steps, and achieves comparable performance to DAPO using 50\% of the training steps.}
\label{fig:method-result}
\end{figure}

\noindent\textbf{The \method{} Algorithm.}
In response, we propose a new algorithm that explicitly avoids dropping tokens, even those associated with large updates, while inherently maintaining entropy within a reasonable range to ensure stable exploration. First, recall that the vanilla REINFORCE objective with corrected distribution for offline updates is:

\begin{equation}
    \begin{aligned}
    \mathcal{J}_{\text{REINFORCE}}(\theta) &= \mathbb{E}_{(q,a) \sim \mathcal{D}, o_i \sim \pi_{\theta_{\text{old}}}(\cdot|q)} \\
    & \left[
        \frac{1}{|o_i|} \sum_{t=1}^{|o_i|}
        \texttt{sg}(r_{i,t}(\theta))\hat{A}_{i,t}\log \pi_\theta(o_{i,t} \mid q, o_{i,<t})
    \right], 
    \end{aligned}
\label{eq:reinforce}
\end{equation}
where $\texttt{sg}(\cdot)$ denotes the stop-gradient operation.
Rather than clipping the token updates as in PPO/GRPO, we instead clip the importance sampling weight in Eq.~\ref{eq:reinforce} to stabilize training. 
We term our approach \method{} (\textbf{C}lipped \textbf{IS}-weight \textbf{P}olicy \textbf{O}ptimization). Adopting the group relative advantage from GRPO and the token-level loss~\citep{yu2025dapoopensourcellmreinforcement,liu2025understandingr1zeroliketrainingcritical}, \method{} optimizes the following objective:

\begin{equation}
    \begin{aligned}
    \mathcal{J}_{\text{\method{}}}(\theta) &= \mathbb{E}_{(q,a) \sim \mathcal{D}, \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(\cdot|q)} \\
    & \left[
        \frac{1}{\sum_{i=1}^G |o_i|} \sum_{i=1}^G \sum_{t=1}^{|o_i|}
        \texttt{sg}(\hat{r}_{i,t}(\theta))\hat{A}_{i,t}\log \pi_\theta(o_{i,t} \mid q, o_{i,<t})
    \right], 
    \end{aligned}
\label{eq:CISPO}
\end{equation}
where $\hat{r}_{i,t}(\theta)$ is the clipped IS weight:

\begin{equation}
    \hat{r}_{i,t}(\theta) = \text{clip}\left(r_{
    i,t}(\theta), 1-\epsilon^{IS}_{low}, 1+\epsilon^{IS}_{high}\right).
\end{equation}
We note that without weight clipping, $\mathcal{J}_{\text{\method{}}}$ reduces to the standard policy gradient objective. In our experiments, we did not impose a lower bound on the IS weight by setting $\epsilon^{IS}_{low}$ to a large value; instead, we only tuned $\epsilon^{IS}_{high}$.
Although the gradient of Eq.~\ref{eq:CISPO} is slightly biased due to weight clipping, this approach preserves gradient contributions from all tokens, especially in long responses. 
\method{} proves effective in our experiments, helping reduce variance and stabilizing RL training. 
In addition, we utilize the dynamic sampling and length penalty techniques from~\citet{yu2025dapoopensourcellmreinforcement}. There is no KL penalty term in \method{} similar to other recent works~\citep{yu2025dapoopensourcellmreinforcement,hu2025openreasonerzeroopensourceapproach}.
% \begin{equation}
% M_{i,t} = \lnot [ ( \hat{A}_{i,t} > 0 \;\wedge\; r_{i,t} > 1 + \epsilon_{high} ) \;\vee\; ( \hat{A}_{i,t} < 0 \;\wedge\; r_{i,t} < 1 - \epsilon_{low}) ]. \tag{5}
% \end{equation}


\noindent\textbf{A General Formulation.}
While we adopt \method{} in our experiments, here we further present a unified formulation by introducing a token-wise mask into the \method{} objective. This allows for hyperparameter tuning to control whether, and under what conditions, gradients from specific tokens should be dropped:

\begin{equation}
    \begin{aligned}
    \mathcal{J}_{\text{unify}}(\theta) &= \mathbb{E}_{(q,a) \sim \mathcal{D}, \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(\cdot|q)} \\
    & \left[
        \frac{1}{\sum_{i=1}^G |o_i|} \sum_{i=1}^G \sum_{t=1}^{|o_i|}
        \texttt{sg}(\hat{r}_{i,t}(\theta))\hat{A}_{i,t}\log \pi_\theta(o_{i,t} \mid q, o_{i,<t})M_{i,t}
    \right]. 
    \end{aligned}
\label{eq:unify}
\end{equation}
The mask $M_{i,t}$ is equivalent to the mask implicitly defined in the PPO trust region:
\begin{equation}
M_{i,t} = 
\begin{cases} 
0 & \text{if } \hat{A}_{i,t} > 0 \text{ and } r_{i,t}(\theta) > 1 + \epsilon_{\text{high}}, \\
0 & \text{if } \hat{A}_{i,t} < 0 \text{ and } r_{i,t}(\theta) < 1 - \epsilon_{\text{low}}, \\
1 & \text{otherwise}.
\end{cases} 
\end{equation}
% where $\texttt{sg}(\cdot)$ denotes the stop-gradient operation. 
This unified loss formulation can flexibly represent different clipping strategies under a common framework. 

\noindent\textbf{Empirical Validation of \method{}.}
To validate the effectiveness of \method{}, we empirically compare it with DAPO and GRPO in a zero-RL training setting. Specifically, we apply different RL algorithms to train the Qwen2.5-32B-base model on the mathematical reasoning dataset from~\citet{yu2025dapoopensourcellmreinforcement}, and report performance on the AIME 2024 benchmark. As shown in Figure~\ref{fig:method-result}, \method{} significantly outperforms both DAPO and GRPO with the same number of training steps. Notably, \method{} demonstrates superior training efficiency compared to other approaches; for example, it matches DAPO's performance with only 50\% of the training steps.




\subsection{Efficient RL Scaling with Lightning Attention -- Challenges and Recipes}
\label{sec:challenge}



\begin{figure}[!t]
\begin{subfigure}[t]{0.45\textwidth}  % 这里也用 [t]
    \centering
    \includegraphics[width=\textwidth]{figures/precision_comparison.pdf}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.45\textwidth}  % 注意这里用 [t]
    \centering
    \includegraphics[width=\textwidth]{figures/precision_comparison_v2.pdf}
\end{subfigure}
\caption{Probability of tokens in training-mode code vs. probability of tokens in inference-mode code. Each point in the figures represents an individual token. The Pearson correlation coefficient is indicated in the figures. Theoretically, the two probabilities should be identical, and all the tokens should be exactly on the diagonal line. 
{\bf Left:} Correlation of the M1 model before our fix; {\bf Right:} Correlation of the M1 model after applying our fix of using FP32 precision for the LM output head.}
\label{fig:mis-precision}
\end{figure}



As shown in Figure~\ref{fig:perf-bars-flops} (Right), we emphasize that our hybrid attention inherently enables more efficient RL scaling compared to traditional attention designs, since rollout computation and latency are often the primary bottlenecks in RL training. However, as pioneers in conducting large-scale RL experiments with this novel architecture, we encountered unique challenges and developed targeted solutions, as we describe below.

\noindent \textbf{Computational Precision Mismatch in Generation and Training.} 
RL training is highly sensitive to computational precision. 
During our RL training, we observed a significant discrepancy in the probabilities of rolled-out tokens between training-mode and inference-mode, as shown in Figure~\ref{fig:mis-precision} (Left). This discrepancy arose from a precision mismatch between the training and inference kernels. The issue was detrimental and prevented reward growth in our experiments.
Interestingly, this issue did not appear in smaller, dense models with softmax attention.
Through layer-by-layer analysis, we identified high-magnitude activations in the LM head at the output layer as the primary source of error. To address this, we increased the precision of the LM output head to FP32, thereby realigning the two theoretically identical probabilities, as demonstrated in Figure~\ref{fig:mis-precision} (Right). This adjustment improved the correlation between training and inference probabilities from approximately 0.9x to 0.99x. Notably, this correlation metric remained stable throughout training, enabling successful reward increase.

\noindent \textbf{Optimizer Hyperparameter Sensitivity.}
We employ the AdamW~\citep{loshchilovdecoupled} optimizer, and inappropriate configurations of $\beta_1$, $\beta_2$, and $\epsilon$ can lead to non-convergence during training. ~\cite{molybog2023theoryadaminstabilitylargescale}. For instance, using the default configuration from VeRL~\cite{sheng2024hybridflow}, where betas = (0.9, 0.999) and eps = 1e-8, can result in such issues.
We have observed that the gradient magnitudes in MiniMax-M1 training span a wide range, from 1e-18 to 1e-5, with the majority of the gradients being smaller than 1e-14. Furthermore, the correlation between the gradients of adjacent iterations is weak. Based on this, we set $\beta_1 = 0.9$, $\beta_2 = 0.95$, and eps=1e-15.

\noindent \textbf{Early Truncation via Repetition Detection.}
During RL training, we found that complex prompts could induce pathologically long and repetitive responses, whose large gradients threatened model stability. Our goal was to preemptively terminate these generation loops rather than penalize the already repetitive text. As simple string-matching is ineffective against varied repetition patterns, we developed a heuristic based on token probabilities. We observed that once a model enters a repetitive cycle, the probability for each token soars. Consequently, we implemented an early truncation rule: generation is halted if 3,000 consecutive tokens each have a probability above 0.99. This method successfully prevents model instability and improves generation throughput by eliminating these pathological, long-tail cases.




\section{Scaling Reinforcement Learning with Diverse Data}
\label{sec:data}
In this section, we describe the data and reward we adopted for our RL stage. We incorporate a diverse set of environments in our RL training pipeline, including tasks that can be verified by rules and general tasks that need to be verified through reward models.
All these environments are integrated into the RL stage using a carefully designed curriculum.

\subsection{Reasoning-Intensive Tasks with Rule-based Verification}
Below, we introduce our data that can be verified by deterministic rules. For all the following tasks, we employ rule-based final correctness as the correctness reward, complemented by a format reward.

\noindent \textbf{Mathematical Reasoning.}
Our initial mathematical dataset comprises hundreds of thousands of high-quality, competition-level problems, meticulously curated and organized from public sources and official mathematics competitions. These problems span a wide range of difficulty levels, each paired with a standard reference solution.
Our data cleaning pipeline begins with the removal of incomplete samples and those exhibiting formatting or typographical errors. We subsequently apply embedding-based deduplication across the RL data sources and enforce a strict separation from the SFT dataset to avoid any overlap, as leakage from the SFT phase into the RL stage hinders exploration and undermines training effectiveness. Additionally, we employ both n-gram and embedding-based methods to eliminate potential contamination from commonly used mathematical benchmark test sets, thereby ensuring the integrity and fairness of our evaluations.
We filter out samples containing multiple sub-problems, proof-based questions, and binary questions (e.g., true/false) that are susceptible to random guessing. Multiple-choice questions are reformulated into open-ended formats to better align with our reinforcement learning framework.
Next, we employ our internal model to extract the final answers from the reference solution, retaining only those samples whose extracted answers can be correctly parsed by our rule-based answer checker. Finally, we use a strong reasoning model to compute the pass@10 for each question and retain only those samples with a pass rate strictly between 0 and 0.9, resulting in a curated dataset of nearly 50K high-quality mathematical samples for our RL training.

\noindent \textbf{Logical Reasoning.}
For logical reasoning data, we carefully select 41 logical reasoning tasks requiring non-trivial reasoning ability such as cipher and Sudoku, then we implement a data synthesis framework to synthesize all the data. Concretely, we utilize our SynLogic framework~\citep{liu2025synlogic} to implement the data synthesis pipeline featuring task-specific data generators and rule-based task-specific verifiers, enabling automatic logical data generation. We meticulously configure the difficulty parameters during generation, ensuring the appropriate learning challenge of the generated data. Specifically, to prevent inclusion of overly difficult instances, we establish an upper difficulty bound based on the solvability limits of current strong reasoning models, requiring their pass@10 rates greater than zero. Similarly, we set a lower difficulty bound using the lowest difficulty parameters for which the MiniMax-Text-01 model achieves pass rates between 0 and 0.5. This approach ensures the data maintains a balance between difficulty and learnability. In addition, as the model capabilities improve during training, we increase the difficulty of the data in the later stages. Using this framework, we synthesize approximately 53K logical reasoning samples for RL training.

\noindent \textbf{Competitive Programming.}
For the competitive programming problems, we collect publicly available problems from online judge platforms and popular coding websites. For problems lacking test cases, we develop an LLM-based workflow and use the MiniMax-Text-01 model to generate comprehensive test suites. Similar to our approach with mathematical reasoning datasets, we filter problems based on quality and difficulty using pass rates from model sampling, retaining moderately challenging and high-quality algorithmic problems. Through this process, we generate 30K competitive programming data samples for RL training.

\noindent \textbf{Software Engineering.}
For the software engineering domain, inspired by SWE-bench~\citep{jimenez2024swebenchlanguagemodelsresolve}, we construct verifiable reinforcement learning environments by leveraging real-world data from public GitHub repositories. Our dataset primarily comprises issues and pull requests (PRs) that encapsulate common software development challenges, including bug localization, code repair, and test case synthesis.
To facilitate effective reinforcement learning, we develop a sophisticated containerized sandbox environment that simulates a realistic software development workflow. This environment enables the actual execution of code, providing direct and verifiable feedback on the correctness and efficacy of an agent's proposed interventions. The pass/fail status of pre-defined or newly generated test cases serves as the primary reward signal for our RL framework. A successful execution that passes all relevant test cases yields a positive reward, while compilation errors, runtime failures, or test case regressions result in a zero or negative reward, thus providing a clear signal for policy optimization.
Through this process, we curate several thousand high-quality data samples. Each sample includes a problem description (e.g., bug report from an issue), the initial faulty code, and a set of associated test cases. This setup allows our RL agent to learn to accurately pinpoint bugs, propose correct code fixes, and even synthesize new, effective test cases, with performance directly verifiable through the execution within our sandboxed environment.

\subsection{General Domain Tasks with Model-based Feedbacks}

In this section, we further extend the RL scope to a wider array of general domain tasks. As these tasks cannot be easily verified by rules, we utilize reward models to provide the feedback.

\subsubsection{Data and Reward Models}
Our general RL dataset consists of a total of 25K complex samples. These can be broadly categorized into two types: samples with ground-truth answers that are verifiable but difficult to validate using rules, and samples without ground-truth answers. 

\noindent\textbf{Tasks with Ground Truth.} This category primarily includes STEM and other factual problems where answers are objective but may have multiple valid expressions. Such diversity often renders rule-based answer checkers inaccurate. Our data cleaning process is similar to that used in mathematical reasoning, while we use our Generative Reward Model (GenRM) as a verifier, instead of relying on rule-based checkers.
To evaluate consistency between ground-truth answers and model responses, we adopt a five-grade reward scale to evaluate the two components. First, we construct a human-annotated reward model benchmark, which covers a range of objective tasks across diverse knowledge and task domains, especially the pairs of model response–ground truth that rule-based checkers fail to judge accurately. Second, we evaluate the GenRM's effectiveness by comparing the Best-of-N (BoN) responses selected by GenRM against the pass@N metrics across several benchmarks. GenRM performance is assessed using its accuracy on the human-annotated benchmark and the performance gap between BoN and pass@N. These metrics guide experiments to optimize both the data distribution and the prompt design used during the GenRM training.

\noindent\textbf{Tasks without Ground Truth.} This category encompasses a wider range of tasks, including instruction-following, creative writing, etc. 
Prompts are sampled from a large pool based on our internal tagging system, ensuring a balanced training distribution across fine-grained domains. 
Even though these queries are typically open-ended and do not have a ground-truth answer, we seek to pair a reference answer for each query, which serves as a reference for reward model judgment. To this end, we first generate responses by various internal and external models, and then these reference answers will undergo our internal quality evaluation.
During RL training, we adopt a pairwise comparison framework to evaluate model responses. Each comparison yields a score of -1, 0, or 1, indicating whether the model's output is worse than, similar to, or better than a reference answer. For instruction-following tasks with constraints particularly, we utilize both the rule-based reward to assess whether the response satisfies the constraint, and model-based reward to evaluate response's quality. As with the ground-truth setting, we first build a human-annotated benchmark, incorporating multiple blind preference judgments from reliable annotators. We then refine our scoring criteria and preference prompt to optimize accuracy as well as potential biases, which would be mentioned in \S\ref{sec:genrm-bias} below. To minimize the potential biases, training data are also optimized by several methods, such as multiple-blind consistent judgment, position-switched consistent judgment, etc. Once an optimal GenRM is trained, a Swiss Round scoring system is performed across the training dataset to determine the most suitable reference answer for RL training.


\subsubsection{Addressing Bias of Generative Reward Models for Long CoT}
\label{sec:genrm-bias}

Effective general RL for complex CoT reasoning tasks is critically dependent on accurate and unbiased reward models. Assessing such CoT responses turns out to be challenging, and we found that GenRMs preferred longer outputs over potentially superior concise alternatives, irrespective of actual reasoning quality. This {\bf length bias} is a significant issue as it may substantially misguide RL policy optimization, incentivizing verbosity without substance and inducing reward hacking.
Our initial efforts to improve GenRM fidelity include standard offline strategies: (1) Diversifying training data with a wide range of response lengths, sources, and quality tiers; (2) Incorporating adversarial examples to expose vulnerabilities; and (3) Refining model architectures. However, empirical analysis revealed that purely offline evaluation and preemptive mitigation of length bias in GenRMs frequently failed to prevent length bias during RL training.

Consequently, our core strategy incorporates continuous online monitoring of length bias during RL training. Specific metrics are established to detect whether the RL policy disproportionately extends output lengths to maximize GenRMs rewards without gains in task success or reasoning depth. Upon detecting such detrimental length-seeking behavior, indicative of exploiting GenRMs length bias, immediate GenRMs recalibration is triggered. This iterative adjustment is vital to preempt reward hacking related to output length, ensuring the policy prioritized substantive capability enhancement over superficial text inflation.
Complementing this adaptive approach, RL-side techniques including reward shaping, value clipping, and normalization are systematically employed.
These mechanisms desensitize reward signals to extreme values from superficial characteristics (e.g., length), thereby directing policy optimization toward substantive quality and correctness of its long CoT reasoning.

\subsection{Curriculum of Incorporating Diverse Data}

Given that our RL data spans a wide spectrum of categories, a core challenge is training a single policy capable of excelling on both reasoning-intensive tasks and general domain tasks. 
To address this, our approach entails a carefully managed curriculum and dynamic weighting strategy for reasoning and general-domain tasks during the RL training process with \method{}: we start with only the reasoning-intensive tasks with rule-based reward, and then gradually mix in the general domain tasks. This ensures that the model continues to refine its verifiable skills (e.g., in math and code) while progressively enhancing its performance on a diverse spectrum of general tasks, from complex instruction following to open-ended CoT reasoning.
This mixed RL training encourages the model to learn context-dependent application of its reasoning abilities—applying rigorous, step-by-step deduction for verifiable problems and more flexible, adaptive generation for general queries—all within a unified policy framework. It prevents catastrophic forgetting of specialized skills while fostering broader generalization.


\section{Extending RL Scaling to Longer Thinking}
\label{sec:long-context}
Our first RL training is performed with an output length limit of 40K tokens. Given that the hybrid architecture of M1 natively supports near-linear scaling for longer sequences, as demonstrated in Figure~\ref{fig:perf-bars-flops} (Right), we further extend the generation length during RL training to 80K tokens. This results in a new model, which we refer to as MiniMax-M1-80k.

\noindent\textbf{Data.} 
To efficiently train our RL model for an 80K output length, we utilize our previously trained 40K model to guide the data filtering process. First, we evaluate the pass rates on the curated dataset described in \S\ref{sec:data} and remove samples that are easily solved. We then adjust the data distribution to favor more challenging examples, such as difficult mathematical and coding problems. Additionally, we downsample synthetic reasoning data after observing that it destabilizes long-context RL training. Specifically, outputs generated from this data type often become repetitive and homogenous, and continued exposure to these patterns proves detrimental to the model's overall performance.

\noindent\textbf{Length Scaling Strategy.} To gradually increase the output length, we employ a staged window expansion RL strategy. We begin with an output length of 40K and incrementally expand it to 48K, 56K, 64K, 72K, and ultimately 80K. This staged approach ensures training stability at each step. The transition to a subsequent length is determined by a set of empirical indicators. These include the convergence of perplexity on the generated sequences and whether the 99th percentile of the output lengths is approaching the current context window limit. These signals offer valuable insights into the model's readiness for scaling, which allows us to maintain robust training throughout the process.

\noindent\textbf{Addressing Training Instability During Scaling.} During the scaling process, we encountered a critical issue in the later stages of training at each length window. Specifically, the model exhibited susceptibility to pattern collapse, where the latter portions of generated sequences degraded into incoherent or garbled text. This phenomenon consistently coincided with increased perplexity, indicating compromised generation quality and stability. We identify the root cause: during output length extension, negative samples increase in length substantially faster than positive samples, frequently reaching the context window limit earlier. Consequently, disproportionately large negative gradients accumulate in the latter segments of generation sequences. This imbalance originates from the inherently unequal nature of GRPO's advantage normalization and the token-level loss we adopt.
To address this, we implement three key solutions: (1) Detecting repetitive patterns (consecutive high-probability tokens) with early stopping to prevent excessive context window consumption by repetitive responses; (2) Adopting combined sample-level loss and token-level normalization to alleviate negative-positive sample imbalance and mitigate adverse effects; (3) Decreasing both the gradient clipping threshold and $\epsilon^{IS}_{high}$ to further stabilize generation.
